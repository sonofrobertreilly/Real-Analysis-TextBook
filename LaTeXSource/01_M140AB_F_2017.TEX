% M140AB_F.TeX  Notes for `Single-Variable Analysis': 

%
% Revised: 01/20/2017 Encoding: Western ASCII
%

                  \chapter{Advanced Topics in Limits and Continuity}
                  \label{ChaptF}


\StartSkip{
        \underline{Quotes for Chapter~\Ref{ChaptF}}: 

\V

\begin{quotation}
{\footnotesize
        (1) `... certainty and uniformity are gains not lightly to be sacrificed.'

        (From `The Paradoxes of Legal Science', by Benjamin Cardozo, Associate Justice of the U. S. Supreme Court) %% Bartlett p. 609 # 10

\V

        (2) `No author is a man of genius to his publisher'

        (Attributed to Heinrich Heine) %% Bartlett P. 420 # 14

\V

        (3) `I saw an angel in the marble and I carved until I set him free'.

        (Attributed to the Michelangelo, the great Italian Renaissance artist and sculptor.) %% Internet

\V

        (4) `What did the President know, and when did he know it?'

        (Question from Senator Howard Baker during the Senate Watergate Committee meetings)

}%EndFootNoteSize
\end{quotation}
}%\EndSkip

\V
\V

\StartSkip{POSSIBLE TOPICS

WEIRERSTRASS APROX THM

SPACE-FILLING CURVES (CANTOR FCN)


}%EndSkip



            \small{\bf Introduction}

\V

        In this chapter we return to the study of `Limits' and of `Continuous Functions', concepts which were introduced in Chapters~\Ref{ChaptC} and~\Ref{ChaptD}.
    Most of the topics discussed in the present chapter are {\em not} normally covered in elementary calculus courses.

\VV
%------------------
\StartSkip{

                \section{{\bf The Bolzano-Weierstrass Theorem for Bounded Sequences in ${\RR}$}}
                \label{SectF30}\IndB{ZZ Sections}{\Ref{SectF10} Bolzano-Weierstrass Theorem for Bounded Sequences in ${\RR}$}


        It turns out that one of the most powerful methods in analysis is a process of seeking convergent subsequences of a possibly-divergent sequence of real numbers.
    The next result gives the theoretical underpinnings for that method.
    It is one of the most important theorems in {\ThisText}.

\V

            \subsection{\small{\bf Theorem} (The Bolzano-Weierstrass Theorem for Bounded Sequences)}
            \label{ThmC30.10}

\V

        Every bounded sequence of real numbers has a convergent subsequence.

\V

        {\bf Proof} The proof given here is not standard. It is inspired by the proof given above of Theorem~\Ref{ThmC70.20}, the Cauchy Convergence Theorem.
    As in that proof, first assume that $0\,\,{\leq}\,\,x_{n}\,\,{\leq}\,\,1$ for every index~$n$.
    Also as in that proof, for each index $n$ choose a decimal representation for $x_{n}$, and fix that representations for the rest of this discussion.
    Then, as before, there exists at least one decimal digit which, for infinitely many values of~$n$, appears as the first digit of $x_{n}$.
    Let $d_{1}$ be the smallest such digit, and let $A_{1}$ be the (infinite) set of indices $n$ such that $d_{1}$ is the first digit of~$x_{n}$.
    By similar reasoning, there exists at least one digit which appears as the second digit of $x_{n}$ for infinitely many $n$ in the infinite set~$A_{1}$.
    Let $d_{2}$ be the smallest such digit, and let $A_{2}$ be the (infinite) set of $n$ in $A_{1}$ such that $d_{2}$ is the second digit of~$x_{n}$.
    Continue this process to get a base~$10$ sequence ${\sigma} \,=\, (d_{1}, d_{2}, \,{\ldots}\,d_{k},\,{\ldots}\,)$,
    and a corresponding sequence of infinite subsets $A_{1}$, $A_{2}$,\,{\ldots}\,of ${\NN}$, with the following properties:

\VA

        \h (i)\, $A_{j+1} \,{\subseteq}\, A_{j}$ for each $j$ in~${\NN}$;

        \h (ii) for each index $k$ one has $d_{1}$, $d_{2}$,\,{\ldots}\,$d_{k}$ are the corresponding digits of $x_{n}$ for each $n$ in $A_{k}$.

\VA

\noindent These digits form a base~$10$ sequence ${\sigma} \,=\, (d_{1}, d_{2},\,{\ldots}\,)$.
    Let $x \,=\, 0.d_{1}\,d_{2}\,\,{\ldots}\,d_{k}\,\,{\ldots}\,$ be the corresponding number in $[0,1]$.

        \underline{Claim} There is a subsequence ${\zeta} \,=\, (x_{n_{1}, x_{n_{2}},\,{\ldots}\,x_{n_{k},\,{\ldots}\,}})$
    of the original sequence ${\xi}$ such that $\lim_{k \,{\rightarrow}\, {\infty}} x_{n_{k}} \,=\, x$.

        \underline{Proof of Claim} Let $n_{1}$ be the smallest element of the set~$A_{1}$.
    Next let $n_{2}$ be the smallest element of the set $A_{2}$ such that $n_{2}\,>\,n_{1}$;
    such $n_{2}$ exists because the set $A_{2}$ is an infinite subset of~$A_{1}$.
    Continuing this way, suppose that $n_{1}$, $n_{2}$, \,{\ldots}\,$n_{k}$ have been chosen so that $n_{i}{\in}A_{i}$ for each $i \,=\, 1,2,\,{\ldots}\,k$
    and $n_{1}\,<\,n_{2}\,<\,\,{\ldots}\,\,<\,n_{k}$. Define $n_{k+1}$ to be the smallest element of $A_{k+1}$ such that $n_{k+1}\,>\,n_{k}$.
    Use these indices to form a subsequence ${\zeta}$ of~${\xi}$.
    By Property~(ii) above, the first $k$ decimal digits of $x_{n_{k}}$ equal the first $k$ digits of~$x$.
    Thus $|x_{n_{k}}-x|\,<\,1/10^{k}$ for each~$k$. It follows that the subsequence ${\zeta}$ converges to~$x$.

        All that remains is to extend the result from sequences which lie in $[0,1]$ to general bounded sequences.
    This can be done much as at the end of the proof of Theorem~\Ref{Thm70.20} above; the details are left as an exercise.

\VV

%%% 
\begin{quotation}
{\footnotesize \underline{\Note}\IndB{\notes}{on Cauchy and the Bolzano-Weierstrass theorem} (on Cauchy and the Bolzano-Weierstrass theorem):
        The history of the Bolzano-Weierstrass theorem is complicated. In particular,
    it is not obvious that Bolzano's name ought to be included with it; see, for example,
    a $2011$ paper by G.~H Moore in Volume~$20$ of `History and Philosophy of Logic'.
    However, assigning to results or concepts the names of mathematicians whose connections with them are minimal, or even nonexistent,
    occurs frequently in mathematics. Maybe Bolzano's name got joined with that of Weierstrass
    in compensation for the fact that it did {\em not} get associated with the concept of `Cauchy sequences', even though Bolzano stated it first.

        On the opposite side of the issue, perhaps Theorem~\Ref{ThmC30.10} be called the Cauchy-Weierstrass theorem instead.
    Indeed, later in {\ThisText} we study a theorem of Cauchy, the so-called `Root Test',
    in which Cauchy formulates an important application based on the set of limits of subsequences of a given sequence.
    Although Cauchy does not explicitly state that such a limit must exist, the applications don't make sense without that fact.
    Perhaps Cauchy simply thought that this result was `obvious' and not worth stating or proving;
    this would be consistent with much of his approach to infinite sequences (as opposed to `infinite series', i.e., infinite sums)
    in his {\em Cours d'analyse}. The proof given above, being based purely on properties of decimal representations,
    appears to be well within the mathematical capabilities of anyone who in the early nineteenth century had reason to formulate the result in the first place.
    Cauchy had such a reason: his `Root Test'.

        Perhaps the question to ask is, paraphrasing Chapter Quote~(4), is this:

        \h `What did Cauchy know, and when did he know it?'
}%EndFootnotesize
\end{quotation} 
%## 


\VV

        The hypothesis in the previous theorem, that the given sequence ${\xi}$ be bounded, cannot be omitted:
    an unbounded sequence certainly need not have a convergent subsequence. However,
    if one replaces `convergent subsequence' with `subsequence having a limit', then the boundedness can be dropped.

\V

            \subsection{\small{\bf Theorem} (The Bolzano-Weierstrass Theorem for Unbounded Sequences)}
            \label{ThmC30.10A}

\V

        Every sequence of real numbers, bounded or not, has a subsequence which has a limit.
    Furthermore, if the sequence is unbounded above, then a subsequence exists which is strictly monotonic up;
    likewise, if the sequence is unbounded below, then a subsequence exists which is strictly monotonic down.

\V

        \underline{Proof} Let ${\xi} \,=\, (x_{1}, x_{2},\,{\ldots}\,)$ be a sequence of real numbers.
    If ${\xi}$ is bounded, the previous theorem guarantees the existence of a convergent subsequence; of course such a subsequence has a limit.

       Next, suppose that ${\xi}$ is unbounded above. Define a subsequence $(x_{n_{1}}, x_{n_{2}},\,{\ldots}\,)$ as follows:

\VA

        Let $n_{1}$ be the first index such that $x_{n_{1}}\,>\,1$. Then let $n_{2}$
    be the smallest index such that $x_{n_{2}}\,>\,\max\{2, x_{1}, \,{\ldots}\,x_{n_{1}}\}$, and so on.

    It is clear by construction that $n_{1}\,>\,n_{2}\,>\,\,{\ldots}\,\,>\,n_{k}\,>\,\,{\ldots}\,$ for each~$k$,
    and thus $(x_{n_{1}}, x_{n_{2}},\,{\ldots}\,x_{n_{k}},\,{\ldots}\,)$ is a subsequence of~${\xi}$.
    Futhermore, $x_{n_{k}}\,>\,k$ for each $k$, so that $\lim_{k \,{\rightarrow}\, {\infty}} x_{k} \,=\, +{\infty}$.
    Finally, $x_{k+1}\,>\,\max\{k, x_{1}, x_{2},\,{\ldots}\,x_{k}\}$, so $x_{k+1}\,>\,x_{k}$ for each~$k$,
    so that this subsequence is strictly monotonic up, as required.

        A similar argument works if ${\xi}$ is unbounded below. Alternately, simply apply the result just obtained to the sequence~$-{\xi}$.

\V

            \subsection{\small{\bf Corollary}}
            \label{CorC30.10B}

\V

        Every sequence of real numbers has a monotonic subsequence.

\V

        {\bf Proof} Let ${\xi} \,=\, (x_{1}, x_{2},\,{\ldots}\,x_{n},\,{\ldots}\,)$ be a sequence of real  numbers.
    If ${\xi}$ is unbounded, the desired result follows from the preceding result.

        Next, suppose that ${\xi}$ is a bounded sequence. If there is a number $L$ such that $x_{n} \,=\, L$ for infinitely many values of~$n$,
    then clearly there is a constant subsequence all of whose terms equal~$L$. Clearly such a subsequence converges to~$L$,
    and it is monotonic; but of course it is not strictly monotonic.

        Finally, suppose that ${\xi}$ is bounded, and there is no number $L$ such that $x_{n} \,=\, L$ for infinitely many~$n$.
    By the Bolzano-Weierstrass Theorem above there exists a subsequence $(x_{n_{1}}, x_{n_{2}},\,{\ldots}\,x_{n_{k}},\,{\ldots}\,)$
    of ${\xi}$ which converges to some number~$L$. Set $z_{k} \,=\, x_{n_{k}}$.
    Since $x_{n} \,=\, L$ for only finitely many~$n$, it follows that if $k$ is sufficiently large,
    then $z_{k} \,\,{\neq}\,\, 0$; that is, $z_{k}\,>\,L$ or $z_{k}\,<\,L$. Thus either there are infinitely many values of $k$ such that $z_{k}\,>\,L$,
    or there are infinitely many values of $k$ such that $z_{k}\,<\,L$. To be definite,
    assume that the former situation holds; the latter situation can be handled similarly.
    Let $A$ be the set of indices $k$ such that $z_{k}\,>\,L$. Then the set $A$, being infinite,
    determines a subsequence $(z_{r_{1}}, z_{r_{2}},\,{\ldots}\,)$ which converges to $L$ and such that $z_{k_{r}}\,>\,L$.
    It follows easily that the sequence $(y_{1}, y_{2},\,{\ldots}\,)$ given by $y_{r} \,=\, 1(L-z_{k_{r}})$ is unbounded above.
    By Theorem~\Ref{ThmC30.10A} it follows that there is a subsequence $(y_{r_{1}}, y_{r_{2}},\,{\ldots}\,)$ which is strictly monotonic up.
    It follows that the corresponding sequence $(z_{r_{1}, z_{r_{2},\,{\ldots}\,}})$ is also strictly monotonic up.
    


        \underline{Proof}: Since, by hypothesis, the sequence ${\xi}$ is bounded,
    there exist real numbers $A$ and $B$, with $A\,<\,B$, such that $A\,\,{\leq}\,\,x_{k}\,\,{\leq}\,\,B$ for all indices $k$.
    That is, all the terms of the sequence ${\xi}$ lie in the closed interval $[A,B]$.

    Now construct a bisection sequence $[a_{1},b_{1}]$, $[a_{2},b_{2}]$,\,{\ldots}\,  (see Definition~\Ref{DefB30.05A}) as follows:

        \h (i) Set $a_{1} \,=\, A$ and $b_{1} \,=\, B$.

        \h (ii) Suppose that the interval $[a_{m},b_{m}]$ has been constructed so that there are infinitely many indices $k$ such that $x_{k}{\in}[a_{m},b_{m}]$.
    It is then clear that at least one of the two halves of this interval has the analogous property;
    namely, that there are infinitely many indices $k$ such that $x_{k}$ lies in that half of $[a_{m},b_{m}]$
    Let $c_{m} \,=\, (a_{m}+b_{m})/2$ be the midpoint of the interval $[a_{m},b_{m}]$.

        \underline{Case 1} Suppose that there there are infinitely many indices in the left half $[a_{m},c_{m}]$ of $[a_{m},b_{m}]$.
    Then set $a_{m+1} \,=\, a_{m}$ and $b_{m+1} \,=\, c_{m}$, so that $[a_{m+1},b_{m+1}]$ is the left half of $[a_{m},b_{m}]$.

        \underline{Case 2} Suppose instead that there are only finitely many $k$ such that $x_{k}$ is in the left half of $[a_{m},b_{m}]$.
    Then there must be infinitely many $k$ such that $x_{k}$ is in the right half;
    in this case, set $a_{m+1} \,=\, c_{m}$ and $b_{m+1} \,=\, b_{m}$, so that $[a_{m+1},b_{m+1}]$ is the right half of $[a_{m},b_{m}]$.

        In either case one sees that $[a_{m+1},b_{m+1}]$ is one of the halves of $[a_{m},b_{m}]$,
    and that there are infinitely many indices $k$ such that $x_{k}{\in}[a_{m+1},b_{m+1}]$.

        Next, define indices $k_{1}, k_{2},\,{\ldots}\,$ so that $x_{k_{1}}{\in}[a_{1},b_{1}]$, $k_{2}\,>\,k_{1}$ and $k_{2}{\in}[a_{2},b_{2}]$, and so on.
    More generally, once $k_{1}$,\,{\ldots}\,$k_{m}$ are chosen, let $k_{m+1}$ be an index such that $k_{m+1}\,>\,k_{m}$ and $x_{k_{m+1}}{\in}[a_{m+1},b_{m+1}]$.
    It is clear that the indices so constructed satisfy $1\,\,{\leq}\,\,k_{1}\,<\,k_{2}\,<\,\,{\ldots}\,$.
    Let ${\zeta} \,=\, (z_{1},z_{2},\,{\ldots}\,)$, be the corresponding subsequence of ${\xi}$,
    so that $z_{m} \,=\, x_{k_{m}}$ for each $m$ in ${\NN}$.
    Then $z_{m}{\in}[a_{m},b_{m}]$ for each $m$.
    Now let $L$ be the unique real number which lies in the intersection of the bisection sequence just constructed.
    Then the Squeeze Property for Sequences can be used, together with Corollary~\Ref{CorC20.50}, to show that the subsequence ${\zeta}$ converges to $L$.


\V

        \underline{Remark} In light of Theorem~\Ref{ThmC20.110} one could have stated the Bolzano-Weierstrass theorem given above in a slightly more precise form:

        \h `Every bounded sequence of real numbers has a {\em monotonic} convergent subsequence.'

\noindent This is normally not done. One reason is that in practice one normally doesn't use the fact that the subsequence can be chosen to be monotonic.
    A second reason (and quite possibly the main one) is that the form of the statement in Theorem~\Ref{ThmC30.10},
    i.e., without any reference to `monotonic', generalizes easily to the analogous theorem in more general spaces:
    simply replace the phrase `of real numbers' by the `of points in the more general space'.
    In contrast, the `monotonic' phrasing normally does not even make sense in the context of general spaces.



\V
\V

        The next result illustrates a simple application of the Bolzano-Weierstrass Theorem for Sequences.


            \subsection{\small{\bf Theorem}}
            \label{ThmC30.20}

        Let ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,)$ be a bounded sequence of real numbers, and let $L$ be a real number.

        (a) A necessary and sufficient condition for ${\xi}$ to \underline{not} converge to the number $L$ is that there exist a subsequence of ${\xi}$ which converges to some real number $L'$ not equal to $L$.

\V

        (b) A necessary and sufficient condition for ${\xi}$ to converge to $L$ is that every convergent subsequence of ${\xi}$ converge to $L$.

\V

        \underline{Proof}

        (a) \underline{The Condition is Sufficient} Indeed, suppose that there is a subsequence ${\zeta}$ of ${\xi}$ which converges to a number $L'$ such that $L'\,\,{\neq}\,\, L$.
    Then, by Part~(a) of Theorem~\Ref{ThmC20.10A}, the subsequence ${\zeta}$ cannot converge to $L$.
    Now Part~(b) of Theorem~\Ref{ThmC20.10A} implies that the original sequence ${\xi}$ also cannot converge to $L$.

        \underline{The Condition is Necessary} Suppose that ${\xi}$ does not converge to $L$.
    Then there exists ${\varepsilon}_{0}\,>\,0$ such that the inequality $|x_{k}-L|\,\,{\geq}\,\,{\varepsilon}_{0}$ holds for infinitely many values of the index $k$.
    Let $A$ be the set of all such indices, so that $A$ is an infinite subset of ${\NN}$.
    Then let ${\zeta} \,=\,(z_{1},z_{2},\,{\ldots}\,)$ be the subsequence of ${\xi}$ which corresponds to the subset $A$;
    that is, ${\zeta} \,=\, {\xi}{\circ}{\Psi}_{A}$.
    Note that (by the construction of the set $A$) each term $z_{j}$ of ${\zeta}$ satisfies $|z_{j}-L|\,\,{\geq}\,\,{\varepsilon}_{0}$.

        Since (by hypothesis) the sequence ${\xi}$ is bounded, it is clear that ${\zeta}$ is also bounded.
    Now apply the Bolzano-Weierstrass Theorem to the bounded sequence ${\zeta}$ to conclude that there is a subsequence ${\tau} \,=\, (t_{1},t_{2},\,{\ldots}\,)$ of ${\zeta}$ such that $\lim_{i \,{\rightarrow}\, {\infty}} t_{i} \,=\, L'$ for some number $L'$.
     One must then have  $|t_{i}-L'|\,<\,{\varepsilon}_{0}$ for all but a finite number of indices $i$.
    However, since each $t_{i}$ equals one of the numbers $z_{j}$, it follows that $|t_{i}-L|\,\,{\geq}\,\,{\varepsilon}_{0}$ for {\em all} indices $i$.
    Thus one must have $L' \,\,{\neq}\,\, L$.

\V

        (b) \underline{The Condition is Sufficient} Suppose that every convergent subsequence of ${\xi}$ converges to the given number $L$.
    Then, by the `Necessary' portion of Part~(a), it is {\em not} the case that ${\xi}$ does {\em not} converge to $L$.
    In other words, ${\xi}$ {\em does} converge to $L$.

\underline{The Condition is Necessary} This is simply the statement of Part~(b) of Theorem~\Ref{ThmC20.10A}.

\V

        \underline{Remark}: The hypothesis in the preceding result, that ${\xi}$ be a \underline{bounded} sequence, is not superfluous.
    For example, if $L$ is a real number, then certainly the sequence $(1,2,3,\,{\ldots}\,)$ fails to converge to $L$;
    but there is no subsequence which converges to some $L' \,\,{\neq}\,\, L$.
    Likewise, consider the sequence ${\xi} \,=\, (0,2,0,4,0,6,0,8,0,\,{\ldots}\,)$; that is, the sequence whose $k$-th term is $0$ if $k$ is odd, and is $k$ if $k$ is even.
    Clearly ${\xi}$ is not bounded, and thus is not convergent.
    Nevertheless, it is easy to show that this sequence does have the property that all of its convergent subsequences converge to the same limit;
    namely the limit value $L \,=\, 0$. The verification of this is left as an exercise.

\V
\V

        In the preceding theorem one is told to verify that {\em all} convergent subsequences of the bounded sequence ${\xi}$ converge to the same limit $L$.
    The next result says, in effect, that sometimes it suffices to carry out this verification for only a finite number of subsequences of ${\xi}$.
    In fact, this result provides a substantial generalization of the Odd/Even Convergence Theorem, Theorem~\Ref{ThmC20.90}.

\V

            \subsection{\small{\bf Theorem}}
            \label{ThmC30.30}

         Let ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,)$ be a bounded sequence of real numbers.
    Suppose that $A_{1}$, $A_{2}$,\,{\ldots}\,$A_{n}$ are infinite subsets of ${\NN}$ such that ${\NN} \,=\, A_{1}\,{\cup}\,A_{2}\,{\cup}\,\,{\ldots}\,\,{\cup}\,A_{n}$.
    For each $m \,=\, 1,2,\,{\ldots}\,n$ let ${\zeta}^{(m)}$ denote the subsequence of ${\xi}$ which corresponds to the subset $A_{m}$;
    that is, ${\zeta}^{(m)} \,=\, {\xi}{\circ}{\Psi}_{A_{m}}$; see Definition~\Ref{DefA40.40}.
    If each of the subsequences ${\zeta}^{(1)}$, ${\zeta}^{(2)}$,\,{\ldots}\,${\zeta}^{(n)}$ converges to the same number $L$,
    then the original sequence ${\xi}$ also converges to $L$.

\V

        \underline{Proof}: Let ${\tau} \,=\, (t_{1},t_{2},\,{\ldots}\,)$ be a convergent subsequence of ${\xi}$,
    and let $C$ be an infinite subset of ${\NN}$ such that ${\tau} \,=\, {\xi}{\circ}{\Psi}_{C}$.
    By the hypothesis that ${\NN}$ is the union of the sets $A_{1}$,\,{\ldots}\,$A_{n}$,
    it follows easily that $C$ is the union of the sets $C\,{\cap}\,A_{1}$,\,{\ldots}\,$C\,{\cap}\,A_{n}$.
    Since $C$ is an infinite subset of ${\NN}$, at least one of the intersections $C\,{\cap}\,A_{j}$, $1\,\,{\leq}\,\,j\,\,{\leq}\,\,m$, must also be an infinite set.
    Suppose that $D \,=\, C\,{\cap}\,A_{j_{0}}$ is infinite.
    Then ${\sigma} \,=\, {\xi}{\circ}{\Psi}_{D}$ is a subsequence of ${\xi}$ which is simultaneously a subsequence of ${\tau}$ and a subsequence of ${\zeta}^{(j_{0})}$.
    Since ${\sigma}$ is a subsequence of ${\zeta}^{(j_{0})}$, it also follows by Part~(b) of Theorem~\Ref{ThmC20.10A} that ${\sigma}$ converges to $L$.
   Similarly, since ${\sigma}$ is also a subsequence of the convergent sequence ${\tau}$, it follows that ${\sigma}$ and ${\tau}$ have the same limit, so that ${\tau}$ also converges to $L$.
    Now Part~(b) of Theorem~\Ref{ThmC30.20} implies that the original sequence ${\xi}$ also converges to $L$.
 
\V

            \subsection{\small{\bf Remarks}}
            \label{RemrkC30.40}

\hspace*{\parindent}(1) When $A_{1}$ is the set of all odd natural numbers and $A_{2}$ is the set of all even natural numbers,
    the previous theorem reduces to an earlier result, namely Theorem~\Ref{ThmC20.90}, the so-called `Odd/Even Convergence Theorem'. 

\V

        (2) The proof of Theorem~\Ref{ThmC30.30} makes strong use of the fact that only a finite number of subsets $A_{1}$, $A_{2}$ \,{\ldots}\, of ${\NN}$ are under consideration.
    However, it is conceivable that a different proof might be able to work for infinitely many sets.
    More precisely, the reader is invited to look at the following problem:

        Suppose that ${\xi}$ is a bounded infinite sequence of real numbers, and that ${\cal F} \,=\, (A_{1},A_{2},\,{\ldots}\,A_{n},\,{\ldots}\,)$ is a countably infinite family of infinite subsets $A_{n}$ of ${\NN}$ such that ${\NN} \,=\, {\bigcup}_{n=1}^{{\infty}} A_{n}$.

        \underline{Prove or Disprove}: If all of the subsequences of ${\xi}$ corresponding to the subsets $A_{n}$ in the family ${\cal F}$ converge to the same real number $L$, then the original sequence ${\xi}$ also converges to $L$.
}%EndSkip
%------------------


                \section{{\bf Limits and Suprema/Infima; ${\limsup}$} and ${\liminf}$}
                \label{SectC50}\IndB{ZZ Sections}{\Ref{SectC50} Limits and Suprema/Infima; ${\limsup}$ and ${\liminf}$}

%----------------
\StartSkip{
        The Monotonic-Sequence Principle (see Theorem~\Ref{ThmC20.10B} and Theorem~\Ref{ThmC40.30}) makes it possible to use the concepts of `supremum' and `infimum' to compute limits of certain sequences.
    The next result reverses the roles and shows how to use limits of sequences to compute suprema and infima.

\V


            \subsection{\small{\bf Theorem}}
            \label{ThmC50.10}

        Let $X$ be a nonempty set of real numbers. Then there exist monotonic sequences ${\alpha} \,=\, (a_{1},a_{2},\,{\ldots}\,)$ and ${\beta} \,=\, (b_{1},b_{2},\,{\ldots}\,)$ of elements in $X$ such that $\lim_{k \,{\rightarrow}\, {\infty}} a_{k} \,=\, {\inf}\,X$ and $\lim_{k \,{\rightarrow}\, {\infty}} b_{k} \,=\, {\sup}\,X$.

        More precisely, if ${\inf}\,X$ is an element of $X$ then one can take ${\alpha}$ to be the constant real sequence $({\inf}\,X,{\inf}\,X,\,{\ldots}\,)$;
    but if ${\inf}\,X$ is {\em not} in $X$, then ${\alpha}$ can be chosen to be a strictly decreasing sequence.

        Likewise, if ${\sup}\,X$ is an element of $X$ then one can take ${\beta}$ to be the constant real sequence $({\sup}\,X,{\sup}\,X,\,{\ldots}\,)$;
    but if ${\sup}\,X$ is {\em not} in $X$, then ${\beta}$ can be chosen to be a strictly increasing sequence.

\V

        \underline{Proof} Suppose that ${\inf}\,X{\in}X$. Then it is clear that the constant sequence $({\inf}\,X,{\inf}\,X,\,{\ldots}\,)$ converges to ${\inf}\,X$, and its terms are all elements of $X$, as claimed.

        Thus, suppose that ${\inf}\,X$ is not in $X$.
    There are two cases:

        \underline{Case 1} Suppose that ${\inf}\,X \,\,{\neq}\,\, -{\infty}$, so that ${\inf}\,X$ is a real number.
    Define a sequence ${\alpha} \,=\, (a_{1},a_{2},\,{\ldots}\,)$ by the rule:
        $a_{1}$ is an element of $X$ such that ${\inf}\,X\,<\,a_{1}\,<\,({\inf}\,X) + 1$.
    (The existence of such $a_{1}$ follows from the definition of `infimum'.)
    Likewise, if $a_{1}$, \,{\ldots}$\,a_{m}$ have been defined then let $a_{m+1}$ be any element of $X$ such that ${\inf}\,X\,<\,a_{m+1}\,<\,a_{m}$ and ${\displaystyle a_{m+1}\,<\,({\inf}\,X) + \frac{1}{m}}$.
    It is clear that ${\alpha} \,=\, (a_{1},a_{2},\,{\ldots}\,)$ is a strictly decreasing sequence of reals such that ${\inf}\,X\,<\,a_{m}\,<\,({\inf}\,X) + 1/m$ for each $m$ in ${\NN}$.
    Now apply the Squeeze Property to conclude that ${\inf}\,X \,=\, \lim_{k \,{\rightarrow}\, {\infty}} a_{k}$.

        \underline{Case 2} Suppose that ${\inf}\,X \,=\, -{\infty}$.
    Let $a_{1}$ be an element of $X$ such that $a_{1}\,<\,-1$.
    If $a_{1}$,\,{\ldots}\,$a_{m}$ have been defined, let $a_{m+1}$ be any element of $X$ such that $a_{m+1}\,<\,\min\,\{a_{m},-m\}$.
    Then it is clear that ${\alpha} \,=\, (a_{1},a_{2},\,{\ldots}\,)$ is strictly decreasing and $\lim_{k \,{\rightarrow}\, {\infty}} a_{k} \,=\, -{\infty} \,=\, {\inf}\,X$, as required.

        The analysis of ${\sup}\,X$ is similar, and is left to the reader.
}%\EndSkip
%-------------------------------
\V
\V


        The relations between suprema/infima and limits of monotonic sequences suggests that one might be able to describe `Completeness' in terms of monotonic sequences.
        Indeed, some texts use one of the following pair of equivalent statements as their choice of the Completeness Axiom for ${\RR}$:

        `A sequence which is monotonic up and bounded above must be convergent'

\noindent or

        `A sequence which is monotonic down and bounded below must be convergent'

\noindent Of course we have seen these statements as the `Monotonic-Sequence Principle' in Part~(b) of Theorem~\Ref{ThmC20.10B};
    thus they are results proved using our version of the Completeness Axiom, the Bisection Principle.
    In those other texts, however, one treats the Bisection Principle as a theorem to be proved using the Monotonic Sequence Principle.
    As usual, it is merely a matter of mathematical taste as to which statement one takes as the Completeness Axiom.

\V
\V

        There is a simple construction which associates a pair of monotonic sequences, one `up', the other `down', with any given bounded sequence of real numbers.

\V

            \subsection{\small{\bf Construction}}
            \label{ConstC50.30}

        Let ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,x_{n},\,{\ldots}\,)$ be a bounded sequence of real numbers.
    Thus, there exist real numbers $m$ and $M$ such that $m\,\,{\leq}\,\,x_{k}\,\,{\leq}\,\,M$ for all $k$ in ${\NN}$.
    From this one can easily construct examples of sequences ${\alpha} \,=\, (a_{1},a_{2},\,{\ldots}\,)$ and ${\beta} \,=\, (b_{1},b_{2},\,{\ldots}\,)$ with the following properties:

        \h (i) ${\alpha}$ is monotonic up, and $a_{k}\,\,{\leq}\,\,x_{k}$ for all $k$;

        \h (ii) ${\beta}$ is monotonic down, and $b_{k}\,\,{\geq}\,\,x_{k}$ for all $k$.

\noindent For instance, simply let $a_{k} \,=\, m$ and $b_{k} \,=\, M$ for all $k$ in ${\NN}$.

        One can do much better; indeed, one can construct a unique `best possible' pair of sequences associated with ${\xi}$ that satisfy (i) and (ii).

        First note that if a sequence ${\alpha}$ exists which satisfies (i), then clearly $a_{1}$ must satisfy $a_{1}\,\,{\leq}\,\,{\inf}\,\{x_{1},x_{2},\,{\ldots}\,\}$.
    For if $a_{1}$ did not satisfy this inequality, then one would have $a_{1}\,>\,{\inf}\,\{x_{1},x_{2},\,{\ldots}\,\}$.
    By the defining properties of the infimum of a set, this in turn would imply that there must exist an index $k$ such that $a_{1}\,>\,x_{k}$.
    From the requirement that the sequence ${\alpha}$ should be monotonic up one then sees that $a_{k}\,\,{\geq}\,\,a_{1}$ and thus $a_{k}\,>\,x_{k}$, contradicting~(i).
    This argument also shows that if such ${\alpha}$ exists, then one must have
        \begin{displaymath}
        a_{k}\,\,{\leq}\,\,{\inf}\,\{x_{k},x_{k+1},\,{\ldots}\,\} \mbox{ for each $k$ in ${\NN}$}.
        \end{displaymath}
   A similar argument shows that if a sequence ${\beta}$ which satisfies~(ii) exists, then ${\beta}$ must satisfy
        \begin{displaymath}
        b_{k}\,\,{\geq}\,\,{\sup}\,\{x_{k},x_{k+1},\,{\ldots}\,\} \mbox{ for each $k$ in ${\NN}$}.
        \end{displaymath}

        The preceding discussion motivates the following.


\V

            \subsection{\small{\bf Definition}}
            \label{DefC50.40}

        (a) Let ${\xi} \,=\, (x_{1},\,{\ldots}\,x_{k},\,{\ldots}\,)$ be a sequence of real numbers which is bounded above.
    The {\bf upper envelope associated with ${\xi}$}, denoted ${\xi}^{+}$, is the sequence whose $k$-th term $M_{k}({\xi})$ is given by the rule
        \begin{displaymath}
        M_{k}({\xi}) \,=\, {\sup}\,\{x_{k},x_{k+1},\,{\ldots}\,\} \mbox{ for all $k$ in ${\NN}$}.
        \end{displaymath}
    (The fact that the suprema $M_{k}({\xi})$ exist, and are numbers, follows from the Supremum Principle.)

\V


         (b) Similarly, if the sequence ${\xi}$ is bounded below then the {\bf lower envelope associated with ${\xi}$},
    denoted ${\xi}^{-}$, has $k$-th term $m_{k}({\xi})$ given by
        \begin{displaymath}
        m_{k}({\xi}) \,=\, {\inf}\,\{x_{k},x_{k+1},\,{\ldots}\,\} \mbox{ for all $k$ in ${\NN}$}.
        \end{displaymath}
    (The fact that these infima exist, and are real numbers, follows from the Infimum Principle.)


    \underline{Note} If the context makes clear which sequence ${\xi}$ is under consideration, one may abbreviate the notations $M_{k}({\xi})$ and $m_{k}({\xi})$ to $M_{k}$ and $m_{k}$, respectively.

\V

            \subsection{\small{\bf Remark}}
            \label{RemrkC50.50}

        There is an obvious extension of the concept of `upper envelope' to the case in which ${\xi}$ is unbounded above.
    Indeed, in this case it is clear that ${\sup}\,\{x_{k},x_{k+1},\,{\ldots}\,\} \,=\, +{\infty}$ for {\em all} indices $k$, so the natural definition would be ${\xi}^{+} \,=\, (+{\infty},+{\infty},\,{\ldots}\,)$.
    Likewise, there is an obvious way to extend the notion of `lower envelope' to allow sequences which are unbounded below.
    However, it does not appear to be worth the effort to introduce these extensions, so we don't.

\V
\V

        The next result shows how questions about convergence of arbitrary sequences can be reduced to the theory for monotonic sequences.
    To simplify the statement of the theorem, the hypothesis of `boundedness' is included.
    This is a reasonable restriction, however, since unbounded sequences cannot be convergent.

\V

            \subsection{\small{\bf Theorem} (The Upper/Lower-Envelopes Theorem)}
            \label{ThmC50.60}

        Let ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,)$ be a bounded sequence of real numbers,
    and let ${\xi}^{+} \,=\, (M_{1},M_{2},\,{\ldots}\,)$ and ${\xi}^{-} \,=\, (m_{1},m_{2},\,{\ldots}\,)$
    be the corresponding upper and lower envelopes associated with ${\xi}$, as described in Definition~\Ref{DefC50.40}. Then:

        (a) The upper envelope ${\xi}^{+} \,=\, (M_{1},M_{2},\,{\ldots}\,)$ is monotonic down,
    and the lower envelope ${\xi}^{-} \,=\, (m_{1},m_{2},\,{\ldots}\,)$ is monotonic up.
    Furthermore, one has
        \begin{displaymath}
        m_{k}\,\,{\leq}\,\,x_{k}\,\,{\leq}\,\,M_{k} \mbox{ for each $k$ in ${\NN}$}.
        \end{displaymath}

\V

        (b) The monotonic sequences ${\xi}^{+}$ and ${\xi}^{-}$ are bounded, and thus are convergent.

\V

        (c) The original sequence ${\xi}$ is convergent if, and only if, $\lim_{k \,{\rightarrow}\, {\infty}} (M_{k}-m_{k}) \,=\, 0$.
    When this occurs, one has $\lim_{k \,{\rightarrow}\, {\infty}} x_{k} \,=\, \lim_{k \,{\rightarrow}\, {\infty}} m_{k} \,=\, \lim_{k \,{\rightarrow}\, {\infty}} M_{k}$.

\V

         \underline{Proof}:

\V

        (a) For convenience let $A_{k}$ denote the set $\{x_{k},x_{k+1},\,{\ldots}\,\}$,
    so that $m_{k} \,=\, {\inf}\,A_{k}$, and $M_{k} \,=\, {\sup}\,A_{k}$.
    It is clear that $A_{k+1} \,{\subseteq}\, A_{k}$ for each $k$ in ${\NN}$, so that by Part~(b) of Theorem~\Ref{ThmB30.150} one has, for each index $k$,
        \begin{displaymath}
        m_{k+1} \,=\, {\inf}\,A_{k+1}\,\,{\geq}\,\,{\inf}\,A_{k} \,=\, m_{k},
    \mbox{ and } 
        M_{k+1} \,=\, {\sup}\,A_{k+1}\,\,{\leq}\,\,{\sup}\,A_{k} \,=\, M_{k}.
        \end{displaymath}
    That is, the claimed monotonicity holds.

\V

        (b) The monotonicity properties of the sequences ${\xi}^{+}$ and ${\xi}^{-}$ imply that $m_{k}\,\,{\geq}\,\,m_{1}$ and $M_{k}\,\,{\leq}\,\,M_{1}$ for all indices $k$.
    Combining this with the hypothesis $m_{k}\,\,{\leq}\,\,x_{k}\,\,{\leq}\,\,M_{k}$ (and, of course, using Transitivity of Order in ${\RR}$), one then obtains
        \begin{displaymath}
        m_{1}\,\,{\leq}\,\,m_{k}\,\,{\leq}\,\,x_{k}\,\,{\leq}\,\,M_{k}\,\,{\leq}\,\,M_{1} \mbox{ for all indices $k$}.
        \end{displaymath}
    In particular, both of the sequences ${\xi}^{+}$ and ${\xi}^{-}$ are bounded below by $m_{1}$ and bounded above by $M_{1}$.
    The fact that these sequences are convergent then follows from the Monotonic-Sequence Principle (Part~(b) of Theorem~\Ref{ThmC20.10B})

\V

        (c) Let $A \,=\, \lim_{k \,{\rightarrow}\, {\infty}} m_{k}$ and $B \,=\, \lim_{k \,{\rightarrow}\, {\infty}} M_{k}$ be the (real) limits whose existence is proved in Part~(b).

    \h (i)\, Assume that $\lim_{k \,{\rightarrow}\, {\infty}} (M_{k}-m_{k}) \,=\, 0$, so that $A \,=\, B$. Since, by Part~(a), one also has $m_{k}\,\,{\leq}\,\,x_{k}\,\,{\leq}\,\,M_{k}$, 
    the Squeeze Property for Sequences (Part~(c) of Theorem~\Ref{ThmC20.10B}) can be used to conclude that $\lim_{k \,{\rightarrow}\, {\infty}} x_{k}$ exists and equals the common limit of ${\xi}^{-}$ and ${\xi}^{+}$.

        Conversely, suppose that the sequence ${\xi}$ is convergent, and let $L \,=\, \lim_{k \,{\rightarrow}\, {\infty}} x_{k}$.
    Let $y$ and $z$ be real numbers such that $y\,<\,L\,<\,z$.
    Then there exists a number $B$ such that if $k\,\,{\geq}\,\,B$ then $y\,<\,x_{k}\,<\,z$.
    From this it is clear that for $k\,\,{\geq}\,\,B$ the number $y$ is a lower bound for the set $A_{k}$ and $z$ is an upper bound for $A_{k}$.
    Thus, by the basic properties of `supremum' and `infimum', if $k\,\,{\geq}\,\,B$ then 
    $y\,\,{\leq}\,\,m_{k}\,\,{\leq}\,\,M_{k}\,\,{\leq}\,\,z$.
    Thus, by Theorem~\Ref{ThmC20.100} it follows that the sequences ${\xi}^{-}$ and ${\xi}^{+}$ converge to $L$.

\V
\V

        To illustrate the preceding result, let us use it to give a second proof of Part~(b) of Theorem~\Ref{ThmC20.10A}.
    That is, suppose ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,)$ is a convergent sequence of real numbers, with $\lim_{k \,{\rightarrow}\, {\infty}} x_{k} \,=\, L$,
    and that ${\zeta} \,=\, (z_{1},z_{2},\,{\ldots}\,)$ is a subsequence of ${\xi}$.
     We want to show that ${\zeta}$ also converges to $L$ by using the preceding theorem.

\V

        \underline{Proof Using Theorem~\Ref{ThmC50.60}}: First, recall from Theorem~\Ref{ThmA30.60} that to each infinite subset $A$ of ${\NN}$ there is a (unique) strictly increasing bijection ${\Psi}_{A}:{\NN} \,{\rightarrow}\, A$ of ${\NN}$ with $A$, given as follows:
        \begin{displaymath}
        {\Psi}_{A}(1) \,=\, \min\,A; {\Psi}_{A}(j+1) \,=\, \min\,A{\setminus}
    \{ {\Psi}_{A}(1),\,{\ldots}\,{\Psi}_{A}(j)\} \mbox{ for each $j$ in ${\NN}$}.
        \end{displaymath}
    Also recall (from the same theorem) that the subsequence ${\zeta}$ of the given sequence ${\xi}$ can be expressed in the form ${\zeta} \,=\, {\xi}{\circ}{\Psi}_{A}$ for at least one infinite subset $A$ of ${\NN}$.
    (In terms of the notation used in Proof~(A) above, $A \,=\, \{k_{1},k_{2},\,{\ldots}\,\}$, and ${\Psi}_{A}(j) \,=\, k_{j}$ for each $j$.)
    Since $k_{j} \,=\, {\Psi}_{A}(j)\,\,{\geq}\,\,j$ for each $j$ in ${\NN}$, and ${\Psi}_{A}$ is strictly increasing,
    it follows that the set $\{z_{j},z_{j+1},\,{\ldots}\,\}$ is a subset of the set $\{x_{j},x_{j+1},\,{\ldots}\,\}$.
Now apply Theorem~\Ref{ThmB30.150} to conclude that
        \begin{displaymath}
        {\inf}\,\{x_{j},x_{j+1},\,{\ldots}\,\} \,\,{\geq}\,\, 
        {\inf}\,\{z_{j},z_{j+1},\,{\ldots}\,\} \mbox{ and }
        {\sup}\,\{z_{j},z_{j+1},\,{\ldots}\,\} \,\,{\leq}\,\, 
        {\sup}\,\{x_{j},x_{j+1},\,{\ldots}\,\}
        \end{displaymath}
    Using the notation of Definition~\Ref{DefC50.40}, one can then say
        \begin{equation} 
        \label{IneqC.50}
        m_{j}({\xi})\,\,{\leq}\,\,m_{j}({\zeta})
    \,\,{\leq}\,\,
        M_{j}({\zeta})\,\,{\leq}\,\,M_{j}({\xi}) \mbox{ for each index $j$}.
        \end{equation}
    By the Upper/Lower-Envelopes Theorem (Theorem~\Ref{ThmC50.60}), combined with the hypothesis that $\lim_{k \,{\rightarrow}\, {\infty}} x_{k} \,=\, L$, one knows that the sequences ${\xi}^{+} \,=\, (M_{1}({\xi}),M_{2}({\xi}),\,{\ldots}\,)$ and ${\xi}^{-} \,=\, (m_{1}({\xi}),m_{2}({\xi}),\,{\ldots}\,)$ both converge to~$L$.
    Now apply Inequality~\Ref{IneqC.50} and the Squeeze Property for Sequences (Part~(c) of Theorem~\Ref{ThmC20.10B}) to conclude that the sequences ${\zeta}^{+} \,=\, (M_{1}({\zeta}),M_{2}({\zeta}),\,{\ldots}\,)$ and ${\zeta}^{-} \,=\, (m_{1}({\zeta}),m_{2}({\zeta}),\,{\ldots}\,)$ also both converge to~$L$.
    Finally, apply the Upper/Lower-Envelopes Theorem again to conclude that the subsequence ${\zeta}$ converges to~$L$, as claimed.


\V
\V

        We have seen repeatedly that it is important to analyse the convergence properties of subsequences of a given sequence.
    The next definition provides terminology to aid in that analysis.

            \subsection{\small{\bf Definition}}
            \label{DefC50.80}

        Let ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,)$ be a real sequence. A quantity $L$ is said to be a {\bf subsequential limit of ${\xi}$} if there exists a subsequence $(x_{k_{1}},x_{k_{2}},\,{\ldots}\,)$ of ${\xi}$ such that $L \,=\, \lim_{j \,{\rightarrow}\,{\infty}} x_{k_{j}} \,=\, L$.
    Note that the quantity $L$ can be a real number or one of the infinities $+{\infty}$, $-{\infty}$.

        The set of all subsequential limits of ${\xi}$ is denoted by ${\cal L}[{\xi}]$.

\V

            \subsection{\small{\bf Remarks}}
            \label{RemrkC50.90}


        (1) It follows from Theorem~\Ref{ThmC40.60}, (i.e., the Extended Bolzano-Weierstrass Theorem), that the set ${\cal L}[{\xi}]$ is nonempty.

\V

        (2) It follows from Part~(g) of Theorem~\Ref{ThmC40.30} that ${\xi}$ has a limit if, and only if the set ${\cal L}[{\xi}]$ has precisely one element.
    Also, when this condition occurs, that element equals $\lim_{k \,{\rightarrow}\, {\infty}} x_{k}$.

\V

        There is a simple characterization of the set of subsequential limits of a sequence.

\V


            \subsection{\small{\bf Theorem}}
            \label{ThmC50.100}

        Let ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,)$ be a sequence of real numbers.

\V

        (a) Let $L$ be a real number. Then $L$ is an element of ${\cal L}[{\xi}]$ if, and only if, for every pair of numbers $y$ and $z$ such that $y\,<\,L\,<\,z$, there are infinitely many indices $k$ such that $y\,<\,x_{k}\,<\,z$.

        \underline{Alternate Phrasing}: $L$ is an element of ${\cal L}[{\xi}]$ if, and only if, for every ${\varepsilon}\,>\,0$ there are infinitely many indices $k$ such that $|L-x_{k}|\,<\,{\varepsilon}$.

\V

        (b) The quantity $+{\infty}$ is an element of ${\cal L}[{\xi}]$ if, and only if, the sequence ${\xi}$ is unbounded above.


\V
        (c) Likewise, the quantity $-{\infty} is in {\cal L}[{\xi}]$ if, and only if, ${\xi}$ is unbounded below.


\V

        \underline{Proof} (a) Suppose that $L{\in}{\cal L}[{\xi}]$, and let $(x_{k_{1}},x_{k_{2}},\,{\ldots}\,)$ be a subsequence of ${\xi}$ which converges to $L$.
    Let $y$ and $z$ be numbers such that $y\,<\,L\,<\,z$. Then by Part~(a) of Theorem~\Ref{ThmC20.10A} there is a number $B$ such that if $j\,\,{\geq}\,\,B$ then $y\,<\,x_{k_{j}}\,<\,z$.
    Since the indices $k_{1}$, $k_{2}$,\,{\ldots}\,form a strictly increasing sequence of natural numbers, it follows that there are infinitely many different indices $k_{j}$ with $j\,\,{\geq}\,\,B$,
    Thus there are infinitely many indices $k$ such that $y\,<\,x_{k}\,<\,z$; for instance, the $k$'s of the form $k_{j}$ with $j\,\,{\geq}\,\,B$.

        Conversely, suppose that for each $y$ and $z$ in ${\RR}$ such that $y\,<\,L\,<\,z$ there are infinitely many indices $k$ such that $y\,<\,x_{k}\,<\,z$.
    Choose an infinite strictly increasing sequence of indices $k_{1}\,<\,k_{2}\,<\,\,{\ldots}\,$ as follows:

        (i) $x_{k_{1}}$ satisfies $L-1\,<\,x_{k_{1}}\,<\,L+1$.

        (ii) Suppose that indices $k_{1}\,<\,k_{2}\,<\,\,{\ldots}\,k_{m}$ have been chosen. By hypothesis, there are infinitely many indices $k$ such that
        \begin{displaymath}
        L-\frac{1}{m+1}\,<\,x_{k}\,<\,L+\frac{1}{m+1}. \h ({\ast})
        \end{displaymath}
    From among these, choose $k_{m+1}$ so that $k_{m+1}\,>\,k_{m}$.
    Then it is clear that the subsequence $(x_{k_{1}},x_{k_{2}},\,{\ldots}\,)$ has the property that
        \begin{displaymath}
        |L-x_{k_{j_{m}}}|\,<\,\frac{1}{m} \mbox{ for each $m$ in ${\NN}$}.
        \end{displaymath}
    Since $\lim_{m \,{\rightarrow}\, {\infty}} 1/m \,=\, 0$, the Squeeze Property implies that this subsequence converges to $L$.

        The proof that the alternate phrasing also works is left to the reader;
    see the proof of Part~(a) of Theorem~\Ref{ThmC20.10A}.

\V

         (b) Suppose that $+{\infty}{\in}{\cal L}[{\xi}]$.
 Then (by the definition of  the set ${\cal L}[{\xi}]$) there is a subsequence of ${\xi}$ which has $+{\infty}$ as limit.
    Clearly that subsequence is unbounded above, and thus ${\xi}$ itself is unbounded above.

        Conversely, suppose that ${\xi}$ is unbounded above.
    Then, by Case~(ii) of Theorem~\Ref{ThmC40.60}, there exists a subsequence of ${\xi}$ which has $+{\infty}$ as its limit; thus, $+{\infty}{\in}{\cal L}[{\xi}]$.

\V

        (c) Apply the conclusions of Part~(b) to the sequence $(-x_{1},-x_{2},\,{\ldots}\,)$.
    The details are left to the reader.

\V
\V

        The preceding result allows one to easily show that the limit properties of a sequence of real numbers do not depend on the order in which one writes down the terms of the sequence.

\V
\V

            \subsection{\small{\bf Theorem}}
            \label{ThmC50.110}

        Let ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,)$ be a sequence of real numbers, and suppose that ${\sigma} \,=\, (s_{1},s_{2},\,{\ldots}\,)$ is a sequence obtained by permuting the terms of the sequence ${\xi}$.
    That is, suppose that ${\sigma}$ can be expressed in the form ${\sigma} \,=\, {\xi}{\circ}F$, where $F:{\NN} \,{\rightarrow}\, {\NN}$ is a bijection of ${\NN}$ onto ${\NN}$.
    Then ${\cal L}[{\sigma}] \,=\, {\cal L}[{\xi}]$.

\V

        \underline{Proof} First, suppose that $L$ is a real number in the set ${\cal L}[{\sigma}]$,
    and let $y$ and $z$ be numbers such that $y\,<\,L\,<\,z$. By Theorem~\Ref{ThmC50.100} there are infinitely many indices $j$ such that $y\,<\,s_{j}\,<\,z$.
    Let $A$ be the set of such indices, and let $B \,=\, F[A]$. Since $F$ is one-to-one, the set $B$ is also an infinite subset of ${\NN}$.
    Suppose that $k{\in}B$, so that $k \,=\, F(j)$ for a unique $j$ in $A$.
    Then, since $s_{j} \,=\, x_{F(j)} \,=\, x_{k}$, it follows that $x_{k}$ also satisfies $y\,<\,x_{k}\,<\,z$ for all $k$ in the infinite set~$B$.
    Thus, by Theorem~\Ref{ThmC50.100} again, one sees that $L{\in}{\cal L}[{\xi}]$ as well.
    A similar argument shows that if one of the infinities is in ${\cal L}[{\sigma}]$ then it is also in ${\cal L}[{\xi}]$.
    Combining these results leads to the conclusion that ${\cal L}[{\sigma}] \,{\subseteq}\, {\cal L}[{\xi}]$.

        By reversing the roles of ${\sigma}$ and ${\xi}$ in the preceding argument,
    which one can do because $F$ is invertible and thus one can write ${\xi} \,=\, {\sigma}{\circ}F^{-1}$,
    one sees that ${\cal L}[{\xi}] \,{\subseteq}\, {\cal L}[{\sigma}]$ as well.
    Thus, ${\cal L}[{\sigma}] \,=\, {\cal L}[{\xi}]$, as claimed.

\V
            \subsection{\small{\bf Corollary}}
            \label{CorC50.120}

        Suppose that ${\xi}$ and ${\sigma}$ are real sequences which differ only by a permutation of their indices;
    that is, there exists a bijection $F:{\NN} \,{\rightarrow}\, {\NN}$ of ${\NN}$ onto itself such that ${\sigma} \,=\, {\xi}{\circ}F$.
    Then ${\xi}$ has a limit if, and only if, ${\sigma}$ has a limit; and when this occurs, their limits are equal.

\V

        \underline{Proof} Combine the results of the preceding theorem with the result stated in Remark~\Ref{RemrkC50.90}~(2) above.

\V
\V


            \subsection{\small{\bf Examples}}
            \label{ExampC50.130}

        \hspace*{\parindent}(1) Let ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,)$ be the sequence given by $x_{k} \,=\, (-1)^{k-1}$ for each $k$ in ${\NN}$;
    that is, ${\xi} \,=\, (1,-1,1,-1,\,{\ldots}\,)$.
    This sequence is bounded (note that $|x_{k}| \,=\, 1$ for all $k$), so neither $+{\infty}$ nor $-{\infty}$ is in ${\cal L}[{\xi}]$.
    Likewise, if $L$ is a real number such that $L \,\,{\neq}\,\, 1$ and $L \,\,{\neq}\,\, -1$,
    then there exists ${\varepsilon}\,>\,0$ such that $|L-x_{k}|\,\,{\geq}\,\,{\varepsilon}$ for all indices $k$.
    Indeed, let ${\varepsilon} \,=\, \min\,\{|L-1|,|L+1|\}$.

        In contrast, it is clear that $1$ and $-1$ are both in ${\cal L}[{\xi}]$.
    For instance, no matter which ${\varepsilon}\,>\,0$ is chosen, there are infinitely many indices $k$ for which $|1-x_{k}|\,<\,{\varepsilon}$;
    indeed, one has $1-x_{k} \,=\, 0$ whenever $k$ is odd.
    Likewise, $|-1-x_{k}| \,=\, 0\,<\,{\varepsilon}$ if $k$ is even.

        Thus one has ${\cal L}[{\xi}] \,=\, \{-1,1\}$.

\V

        (2) Recall that the set ${\QQ}$ of all rational numbers is countable (see Corollary~\Ref{CorA20.80}).
    Thus, there exists a bijection ${\alpha}:{\NN} \,{\rightarrow}\, {\QQ}$ which maps ${\NN}$ one-to-one onto ${\QQ}$.
    As such, the map ${\alpha}$ is an infinite sequence $(a_{1},a_{2},\,{\ldots}\,)$ of rational numbers in which each rational number appears exactly once.

        \underline{Claim}: ${\cal L}[{\alpha}] \,=\, {\RR}\,{\cup}\,\{-{\infty},+{\infty}\}$.

        \underline{Proof of Claim} First, note that the set ${\QQ}$ is unbounded above and below, so the same is true for the sequence ${\alpha}$.
    Thus $-{\infty}$ and $+{\infty}$ are elements of the set ${\cal L}[{\alpha}]$.

        Next, notice that if $L$ is a real number and if $y$ and $z$ are numbers such that $y\,<\,L\,<\,z$,
    then there are infinitely many rational numbers in the open interval $(y,z)$.
    Since each rational number corresponds to exactly one index $k$, there exist infinitely many indices $k$ such that $y\,<\,a_{k}\,<\,z$.

        The claim now follows by applying Part~(a) of Theorem~\Ref{ThmC50.100}.

\V
\V

            \subsection{\small{\bf Theorem}}
            \label{ThmC50.140}

        Let ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,)$ be a sequence of real numbers, and let ${\cal L}[{\xi}]$ be the corresponding set of subsequential limits of ${\xi}$.
    Then the set ${\cal L}[{\xi}]$ has both a maximum element and a minimum element.
    That is, there are quantities $L_{1}$ and $L_{2}$ such that

       \h (i) $L_{1}$ and $L_{2}$ are elements of ${\cal L}[{\xi}]$, and

       \h (ii) $L_{1}\,\,{\geq}\,\,L\,\,{\geq}\,\,L_{2}$ for all $L$ in ${\cal L}[{\xi}]$.

\noindent (Of course, we allow the possibility that $L_{1}$ or $L_{2}$ could be an infinity.)


\V

        \underline{Proof}: Let us first show that ${\cal L}[{\xi}]$ has a maximum element.
    There are several cases to consider.

\V

        \underline{Case 1} Suppose that ${\cal L}[{\xi}]$ is a singleton set $\{L\}$.
    (By Part~(g) of Theorem~\Ref{ThmC40.30} this corresponds to the situation in which $\lim_{k \,{\rightarrow}\, {\infty}} x_{k} \,=\, L$.)
    In this case one sees that the choice $L_{1} \,=\, L$ is the desired maximum element.
    (Of course it is also the desired minimum element, but we are not yet ready to discuss the minimum element in general.)

\V

        \underline{Case 2} Suppose that $+{\infty}{\in}{\cal L}[{\xi}]$.
    Then clearly $L_{1} \,=\, +{\infty}$ is the desired maximum.

\V

        \underline{Case 3} Suppose that ${\cal L}[{\xi}]$ is not a singleton set, and assume also that $+{\infty}$ is not in ${\cal L}[{\xi}]$.
    Then, by Part~(a) of Theorem~\Ref{ThmC50.100}, ${\xi}$ is bounded above.
    Let $M$ in ${\RR}$ be an upper bound for ${\xi}$, so that $M\,\,{\geq}\,\,x_{k}$ for all $k$.
    Let ${\zeta} \,=\, (z_{1},z_{2},\,{\ldots}\,)$ be any subsequence of ${\xi}$ which has a limit.
    Since ${\zeta}$ is a subsequence of ${\xi}$, it follows that $M\,\,{\geq}\,\,z_{j}$ for all $j$ in ${\NN}$, and thus $M\,\,{\geq}\,\,\lim_{j \,{\rightarrow}\, {\infty}} z_{j}$.
    Now let $X \,=\, {\cal L}[{\xi}]{\setminus}\{-{\infty}\}$.
    Then $X$ is a nonempty set of real numbers which is bounded above by the number $M$.
    Let $L_{1} \,=\, {\sup}\,X$, so that $L_{1}$ is a real number and $M\,\,{\geq}\,\,L_{1}\,\,{\geq}\,\,L$ for all $L$ in $X$.
    Of course $L_{1}\,>\,-{\infty}$, so certainly $L_{1}\,\,{\geq}\,\,L$ for all $L$ in ${\cal L}[{\xi}]$; that is, $L_{1}$ satisfies Condition~(ii) above.

    Next, note that because of Theorem~\Ref{ThmC50.10} one knows that there exists a monotonic-up sequence of numbers $b_{1}$, $b_{2}$,\,{\ldots}\, in $X$
    (hence in ${\cal L}[{\xi}]$) such that $L_{1} \,=\, \lim_{j \,{\rightarrow}\, {\infty}} b_{j}$.
    Since $b_{j}{\in}{\cal L}[{\xi}]$, it follows that there exists a sequence of subsequences ${\tau}_{1} \,=\, (t_{11},t_{12},\,{\ldots}\,)$, ${\tau}_{2} \,=\, (t_{21},t_{22},\,{\ldots}\,)$, \,{\ldots}\, of ${\xi}$ such that $b_{j} \,=\, \lim_{m \,{\rightarrow}\, {\infty}} t_{jm}$ for each $j \,=\, 1,2,\,{\ldots}\,$.

        Now let ${\varepsilon}\,>\,0$ be given. By the Alternate Phrasing of Part~(a) of Theorem~\Ref{ThmC50.100} there exist infinitely many $k$ such that $|L_{1}-b_{k}|\,<\,{\varepsilon}/2$.
    Let $p$ be one such index, and consider the corresponding subsequence ${\tau}_{p} \,=\, (t_{p1},t_{p2},\,{\ldots}\,)$, so that $b_{p} \,=\, \lim_{j \,{\rightarrow}\, {\infty}} t_{pj}$.
    By the definition of convergent sequence, there exist infinitely many indices $j$ such that $|b_{p}-t_{pj}|\,<\,{\varepsilon}/2$.
    By the Triangle Inequality one then has
        \begin{displaymath}
        |L_{1}-t_{pj}| \,=\, |L_{1}-b_{p}| + |b_{p}-t_{pj}|\,<\,\frac{{\varepsilon}}{2} + \frac{{\varepsilon}}{2} \,=\, {\varepsilon}
        \end{displaymath}
    for infinitely many indices $j$.
    Since each number $t_{pj}$ is of the form $x_{k_{j}}$ for some index $k_{j}$, and since the indices $k_{j}$, $j \,=\, 1,2,\,{\ldots}\,$ form a strictly increasing sequence, it follows that $|L_{1}-x_{k}|\,\,{\leq}\,\,{\varepsilon}$ for infinitely many indices $j$.
    Thus, by Theorem~\Ref{ThmC50.100} again, it follows that $L_{1}{\in}{\cal L}[{\xi}]$, as claimed. 

        
        The proof that ${\cal L}[{\xi}]$ has a minimum element can be carried out in a similar manner.
    Or, better yet, one can simply apply to the sequence $-{\xi}$ the result just obtained about the maximium element; the details are left to the reader.
        
\V
\V


            \subsection{\small{\bf Definition}}
            \label{DefC50.150}


        Let ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,)$ be a sequence of real numbers, and (as usual) let ${\cal L}[{\xi}]$ denote the corresponding set of subsequential limits of ${\xi}$.
    The maximum element of ${\cal L}[{\xi}]$ is called the {\bf limit superior of ${\xi}$}.
    It is denoted by an expression such as ${\displaystyle \limsup_{k \,{\rightarrow}\, {\infty}} x_{k}}$, or, on occasion,by $\limsup\,{\xi}$.
    (The symbol $\limsup$ is pronounced `lim~soup'.)
    This quantity is also called the {\bf upper limit of the sequence ${\xi}$}. As such, it is sometimes denoted by an expression such as $\overline{\mbox{lim}}_{k \,{\rightarrow}\, {\infty}} x_{k}$;
    but some authors combine the phrase `upper limit' with the `$\limsup$' notation.

        Likewise, the minimum element of the set ${\cal L}[{\xi}]$ is called the {\bf limit inferior}, or the {\bf lower limit}, of the sequence ${\xi}$,
    and it is denoted by expressions such as ${\displaystyle \liminf_{k \,{\rightarrow}\, {\infty}} x_{k}}$ or $\underline{\mbox{lim}}_{k \,{\rightarrow}\, {\infty}} x_{k}$.

\V

            \subsection{\small{\bf Examples}}
            \label{ExamC50.160}

\V

\hspace*{\parindent}(1) Let ${\xi} \,=\, (x_{1}, x_{2},\,{\ldots}\,)$ be the sequence given by $x_{k} \,=\, (-1)^{k-1}$ for each $k$ in ${\NN}$.
    From the results obtained in Example~\Ref{ExampC50.130}~(1) one sees that $\limsup\,{\xi} \,=\, +1$, $\limsup\,{\xi} \,=\, -1$.

\V

        (2) Let ${\alpha} \,=\, (a_{1}, a_{2},\,{\ldots}\,)$ be a sequence of real numbers.
    Let $X$ be the set of all real numbers $R\,\,{\geq}\,\,0$ such that the sequence ${\rho} \,=\, (a_{1}R, a_{2}R^{2}, a_{3}R^{3}, \,{\ldots}\,)$
    is a bounded sequence.
    It is clear that the set $X$ is nonempty; for example, $R \,=\, 0$ is obviously in $X$.
    Then one can show that ${\sup}\,X \,=\, 1/\limsup {\xi}$, where ${\xi}\,=\, (|a_{1}|, \sqrt[2]{|a_{2}|}, \sqrt[3]{|a_{3}|}, \,{\ldots}\,\sqrt[k]{|a_{k}|} \,{\ldots}\,)$.

        \underline{Note} It appears that the concept -- although not the terminology -- of `limit superior' is due Cauchy in his proof of a version of the preceding result.



\V


            \subsection{\small{\bf Theorem}}
            \label{ThmC50.170}

        Let ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,)$ be a sequence of real numbers.

\V

        (a) A necessary and sufficient condition for $\limsup {\xi} \,=\, +{\infty}$ is that the sequence ${\xi}$ be unbounded above.
    
    Likewise, a necessary and sufficient condition for $\liminf {\xi} \,=\, -{\infty}$ is that the sequence ${\xi}$ be unbounded below.


\V

        (b) Suppose that ${\xi}$ is bounded above, and let $L$ be a real number.
    Then a necessary and sufficient condition for $L$ to equal $\limsup\,_{k \,{\rightarrow}\, {\infty}} x_{k}$ is that for every number ${\varepsilon}\,>\,0$ one has

        \h (i) $x_{k}\,>\,L+{\varepsilon}$ for only finitely many indices $k$; and

        \h (ii) $x_{k}\,>\,L-{\varepsilon}$ for infinitely many indices $k$.

        \V

        (c) Suppose that ${\xi}$ is bounded below, and let $L$ be a real number.
    Then a necessary and sufficient condition for $L$ to equal $\liminf\,_{k \,{\rightarrow}\, {\infty}} x_{k}$ is that for every number ${\varepsilon}\,>\,0$ one has

        \h (i) $x_{k}\,<\,L-{\varepsilon}$ for only finitely many indices $k$; and

        \h (ii) $x_{k}\,<\,L+{\varepsilon}$ for infinitely many indices $k$.

\V

\V

        (d) Suppose that ${\xi}$ is bounded above, and let ${\xi}^{+} \,=\, (M_{1}({\xi}),M_{2}({\xi}),\,{\ldots}\,)$ denote the upper envelope associated with ${\xi}$ (see Definition~\Ref{DefC50.40}). Then
        \begin{equation}
        \label{EqnC.55a}
        \limsup_{k \,{\rightarrow}\, {\infty}} x_{k} \,=\, \lim_{k \,{\rightarrow}\, {\infty}} M_{k}({\xi}). 
        \end{equation}
    Likewise, suppose that ${\xi}$ is bounded below, and let ${\xi}^{-} \,=\, (m_{1}({\xi}),m_{2}({\xi}),\,{\ldots}\,)$ denote the lower envelope associated with ${\xi}$. Then
        \begin{equation}
        \label{EqnC.55b}
        \liminf_{k \,{\rightarrow}\, {\infty}} x_{k} \,=\, \lim_{k \,{\rightarrow}\, {\infty}} m_{k}({\xi}). 
        \end{equation}


        The simple proof is left as an exercise.

\V

            \subsection{\small{\bf Corollary}}
            \label{CorC50.180}

        Let ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,)$ be a sequence of real numbers.
    A necessary and sufficient condition for the sequence ${\xi}$ to have a limit $L$ is that
        \begin{displaymath}
        {\limsup}_{k \,{\rightarrow}\, {\infty}}\,x_{k} \,=\, {\liminf}_{k \,{\rightarrow}\, {\infty}}\, x_{k} \,=\, L.
        \end{displaymath}


        \underline{Proof} This follows from Parts~(a) and~(d) of the preceding theorem when combined with the results of Theorem~\Ref{ThmC50.60}.


\V

        NOTE: Some mathematics texts use Equation~\Ref{EqnC.55a} as the {\em definition} of the limit superior of a real sequence which is bounded above.
    Likewise, they use Equation~\Ref{EqnC.55b} as the {\em definition} of the limit inferior of a real sequence which is bounded below.
    Normally, however, such texts would write these equations in the form
        \begin{displaymath}
        \limsup_{k \,{\rightarrow}\, {\infty}} x_{k} \,=\, \lim_{k \,{\rightarrow}\, {\infty}} \left({\sup}\,\{x_{k},x_{k+1},\,{\ldots}\,\}\right)
        \end{displaymath}
    and
        \begin{displaymath}
        \liminf_{k \,{\rightarrow}\, {\infty}} x_{k} \,=\, \lim_{k \,{\rightarrow}\, {\infty}} \left({\inf}\,\{x_{k},x_{k+1},\,{\ldots}\,\}\right). 
        \end{displaymath}
    That is, they probably would not introduce the auxiliary notion of `enveloping sequence'.

        In contrast, some texts define the concepts of ${\limsup\, {\xi}}$ and $\liminf\, {\xi}$ in terms of the conditions stated in Parts~(b) and~(c) of Theorem~\Ref{ThmC50.170}.

\V
\V

\begin{quotation}
{\footnotesize \underline{Remark on the `sup' and `inf' Terminology}

        Many students get confused when trying to sort out the differences between the word `supremum' and the phrase `limit superior'.
    One obvious source of this confusion is that mathematicians have elected to use the same abbreviation, namely `sup', for both `supremum' and `superior'.
    A similar confusion holds between `infimum' and `limit inferior'.
    Thus, it may be useful to briefly consider the linguistic backgrounds of these words and phrases.

       It has already been stated that the words `supremum' and `infimum' are of Latin origin;
    indeed, one often uses the Latin version of their plurals, (`suprema' and  `infima' respectively).
    In any event, these words are \underline{nouns} which mean (roughly) `highest one' and `lowest one', respectively.

        In contrast, the phrases `limit superior' and `limit inferior' are word-for-word translations into English of the Latin phrases `limes superior' and `limes inferior'.
    Unfortunately, word-for-word translations between languages with different structures often produce awkward phrasings.
    Indeed,  in Latin the words `superior' and `inferior' are \underline{adjectives}; and as such they are placed after the noun they modify, `limes', because that's proper Latin grammar.
    English, in contrast, is a language in which an attributive adjective is normally placed {\em before} the noun it modifies;
    thus  better translations would have been `superior limit' and `inferior limit'.
    Of course, these last phrases have virtually the same meanings in English as the corresponding `upper limit' and `lower limit' mentioned in Definition~\Ref{DefC50.150}.

        The situation gets even less clear if a textbook introduces the notations $\limsup$ and $\liminf$,
    but fails to tell its readers what phrases they correspond to.
    Lacking any guidance, the reader of such a textbook is then likely to conclude -- incorrectly -- that these symbols should be pronounced `limit supremum' and `limit infimum'.
}%EndFootNoteSize
\end{quotation}

%}%EndSkip
%------------------------


%----------
        Special cases of the first two of these topics, `Uniform Convergence' and `Uniform Continuity'
    have already made appearances in Chapter~\Ref{ChaptE}, but without being mentioned explicitly by name.
    This reflects the normal situation in the development of mathematical concepts:
    formal definitions normally arise {\em after} one knows that the idea is useful and thus worth naming, not at the beginning of the theory.

    A good way to prepare for the introduction of the new concepts of `Uniform Convergence' and `Uniform Continuity'
    would be to carefully reread the proof of Theorem~\Ref{ThmE45.125B}.
    (That theorem states that if $f$ is a $C^{1}$ function on an open interval $I$, then $f$ has an antiderivative on $I$.)
    Indeed, we use some of the details of that proof to motivate the definitions of the new concepts below.
    While rereading that earlier proof, ask yourself `What is the real heart of the proof? What features makes it work?'

\V
\V

                \section{{\bf Uniform Convergence of a Sequence of Functions}}
                \label{SectF05}

\V

RE-DO LIGHT OF CHANGES IN CHAPTER E

        In the proof of Theorem~\Ref{ThmE45.125B} we use the fact that, for every index $n$, one has
        \begin{displaymath}
        |f(x)-g_{n}(x)|\,\,{\leq}\,\,\frac{M(b-a)}{n} \mbox{ for each $x$ in $[a,b]$}.
        \end{displaymath}
This fact implies that for each $x$ in $[a,b]$ the sequence ${\gamma}(x) \,=\, (g_{1}(x), g_{2}(x),\,{\ldots}\,)$ converges to $f(x)$.
    However, what that inequality tells us is, in a subtle way, much stronger than that;
    this extra strength is needed to complete of the proof of Theorem~\Ref{ThmE45.125B}.
    The next definition clarifies the subtlety involved.

\V

            \subsection{\small{\bf Definition} (Pointwise Convergence; Uniform Convergence)}
            \label{DefF05.20}

\V

        Let ${\gamma} \,=\, (g_{1}, g_{2},\,{\ldots}\,)$ be a sequence of real-valued functions which are all defined on a nonempty set $X \,{\subseteq}\, {\RR}$.


        (1) One says that the sequence ${\gamma}$ {\bf converges pointwise on $X$ to a function $f:X \,{\rightarrow}\, {\RR}$}
    provided that for each $x$ in $X$ the numerical sequence ${\gamma}(x) \,=\, (g_{1}(x), g_{2}(x),\,{\ldots}\,)$ converges to the number~$f(x)$.
    That is, provided the following holds:
        \begin{displaymath}
        \mbox{for every $x$ in $X$ if ${\varepsilon}\,>\,0$ then there exists $B$ such that $k\,\,{\geq}\,\,B$ implies that $|f(x)-g_{k}(x)|\,<\,{\varepsilon}$} \h \h (I)
        \end{displaymath}

\V

        (2) One says that the sequence ${\gamma}$ {\bf converges uniformly on $X$ to a function $f:X \,{\rightarrow}\, {\RR}$}
    provided the following holds:
        \begin{displaymath}
        \mbox{if ${\varepsilon}\,>\,0$ then there exists $B$ such that $k\,\,{\geq}\,\,B$ implies that $|f(x)-g_{k}(x)|\,<\,{\varepsilon}$ for every $x$ in $X$} \h (II)
        \end{displaymath}

\V
\V

        On the surface, Statements (I) and (II) appear to be nearly the same; indeed, the only real difference is the placement of the phrase `for every $x$ in $X$'.
    But the location of that phrase affects the meaning of the statement.
    Indeed, in Statement~(I) by placing the phrase `for every $x$ in $X$' first, one is saying that if you first chose $x$ in $X$ and ${\varepsilon}\,>\,0$,
    then one can find a number $B$ for which $|f(x)-g_{k}(x)|\,<\,{\varepsilon}$ when $k\,\,{\geq}\,\,B$;
    in particular, the value of $B$ may depend in an essential way on the choice of both $x$ and ${\varepsilon}$.
    In Statement~(II), however, the choice of $B$ can be made given only ${\varepsilon}$;
    in particular, $B$ can be chosen so that $|f(x)-g_{n}(x)|\,<\,{\varepsilon}$ when $k\,\,{\geq}\,\,B$, and that this same $B$ works {\em simultaneously} for all $x$ in $X$.

\V

            \subsection{\small{\bf Examples}}
            \label{ExampF05.30}

\V

\hspace*{\parindent}(1) The sequence ${\gamma} \,=\, (g_{1},g_{2},\,{\ldots}\,)$ which appears in the proof of Theorem~\Ref{ThmE45.125B} converges uniformly on $[a,b]$ to the function~$f$ in that theorem.
    Indeed, suppose that ${\varepsilon}\,>\,0$ is given, and let $B \,=\, M(b-a)/{\varepsilon}$.
    Then it follows from the inequality $|f(x)-g_{n}(x)|\,\,{\leq}\,\,\frac{M(b-a)}{n} \mbox{ for each $x$ in $[a,b]$}$ which appears in the proof of that theorem that if $n\,>\,B$ then $|f(x)-g_{n}(x)|\,<\,{\varepsilon}$ for all $x$ in $[a,b]$.

\V

        (2) Let ${\Gamma} \,=\, (G_{1},G_{2},\,{\ldots}\,)$ and $F$ be as in the proof of the same theorem.
    It is easy to check that the sequence ${\Gamma}$ converges uniformly on $[a,b]$ to $F$.

\V

        (3) It is clear that if a sequence ${\gamma} \,=\, (g_{1},g_{2},\,{\ldots}\,)$ converges uniformly on $X$ to a function $f$,
    then it certainly converges pointwise on $X$ to $f$.

\V

        (4) One does not have to consider exotic situations to find a sequence of functions which converges pointwise, but not uniformly, to a function on a set.
    For instance, let $X \,=\, {\RR}$ and let $g_{k}(x) \,=\, {\displaystyle \left(x+\frac{1}{k}\right)^{2}}$ for all $x$ in ${\RR}$.
    It is clear that for each $x$ in ${\RR}$ one has $\lim_{k \,{\rightarrow}\, {\infty}} g_{k}(x) \,=\, x^{2}$;
    that is, the sequence ${\gamma}$ converges pointwise on ${\RR}$ to the function $f:{\RR} \,{\rightarrow}\, {\RR}$ given by $f(x) \,=\, x^{2}$.
    However, the sequence ${\gamma}$ does not converge uniformly on ${\RR}$ to $f$.
    Indeed, suppose that ${\varepsilon}\,>\,0$ is given. Note that
        \begin{displaymath}
        |f(x) - g_{k}(x)| \,=\, \left|x^{2} - \left(x+\frac{1}{k}\right)^{2}\right| \,=\, \left|\frac{2x}{k} + \frac{1}{k^{2}}\right|\,\,{\geq}\,\,\left|\frac{2|x|k-1}{k^{2}}\right|,
        \end{displaymath}
    where the final inequality comes from using the Modified Triangle Inequality.
    It is easy to see that if $x\,>\,(1+{\varepsilon}k^{2})/(2k)$ then $|f(x)-g_{k}(x)|\,>\,{\varepsilon}$.

\V

        (5) In the preceding example the fact that the domain ${\RR}$ is unbounded is crucial to the proof that the sequence ${\gamma}$ fails to be uniformly convergent.
    Indeed, it is a simple exercise to show that the same sequence ${\gamma}$ does converge uniformly to the same function $f$ on any closed bounded interval~$[a,b]$.

\V

        (6) For each $k$ in ${\NN}$ let ${\displaystyle g_{k} \,=\, \hat{B}_{[\frac{1}{k+1},\frac{1}{k}]}^{[1]}}$
    be the normalized $C^{1}$ bump function on the interval ${\displaystyle \left[\frac{1}{k+1},\frac{1}{k}\right]}$; see Definition~\Ref{DefE35.110}.
    It is an easy exercise to show that the sequence ${\gamma} \,=\, (g_{1},g_{2},\,{\ldots}\,)$ converges pointwise to the zero function on ${\RR}$,
    but that it fails to converge uniformly on any interval of the form $[0,b]$ with $b\,>\,0$.
    In contrast, the sequence {\em does} converge uniformly on each interval $[a,b]$ with $a\,>\,0$.

\V

        (7) Let $g_{k}(x) \,=\, x^{k}$ for all $x$ in $[0,1]$. It is easy to see that the sequence ${\gamma} \,=\, (g_{1},g_{2},\,{\ldots}\,)$ converges pointwise on $[0,1]$ to the function $f:[0,1] \,{\rightarrow}\, {\RR}$ given by the rule
        \begin{displaymath}
        f(x) \,=\, \left\{
        \begin{array}{ll}
        0 & \mbox{if $0\,\,{\leq}\,\,x\,<\,1$} \\
        1 & \mbox{if $x \,=\, 1$}
        \end{array}
                            \right.
        \end{displaymath}
    It is an easy exercise to show that the sequence ${\gamma}$ fails to be uniformly convergent on $[0,1]$.

\V

        (8) Let $p_{k}$ denote the Taylor polynomial of order $k$ for the exponential function ${\exp}$ about the center point~$c \,=\, 0$;
    that is,
        \begin{displaymath}
        p_{k}(x) \,=\, 1+ x+ \frac{x^{2}}{2} + \,{\ldots}\,+\frac{x^{k}}{k!}
        \end{displaymath}
    In Theorem~\Ref{ThmE60.85} it is proved that
        \begin{displaymath}
        |{\exp}\, x - p_{k}(x)| \,=\, \frac{{\exp}(r_{k})}{(k+1)!}|x|^{k+1}\,\,{\leq}\,\,{\exp}\,(|x|)\frac{|x|^{k+1}}{(k+1)!}
        \end{displaymath}
    for every $x$ in ${\RR}$.
    In particular, if $R\,>\,0$ is given, then for each $x$ in the interval $[-R,R]$ one has
        \begin{displaymath}
       {\exp}\,(|x|)\frac{|x|^{k}}{k!}\,\,{\leq}\,\,e^{R}\frac{R^{k}}{k!}.
        \end{displaymath}
    It follows that the sequence of these Taylor polynomials converges uniformly to ${\exp}$ on $[-R,R]$.

\V
\V

            \subsection{\small{\bf Theorem}}
            \label{ThmF05.35}

\V

        Let ${\gamma} \,=\, (g_{1},g_{2},\,{\ldots}\,)$ be a sequence of real-valued functions defined on a nonempty set $X \,{\subseteq}\, {\RR}$.

\V

        (a) If ${\gamma}$ converges uniformly on $X$ to a function $f:X \,{\rightarrow}\, {\RR}$, then it converges uniformly to $f$ on every nonempty subset of $X$.

\V

        (b) If
    $X \,=\, A_{1}\,{\cup}\,A_{2}\,{\cup}\,\,{\ldots}\,\,{\cup}\,A_{m}$,
where for each $j \,=\, 1,2,\,{\ldots}\,m$ $A_{j}$ is a nonempty subset of $X$,
    then ${\gamma}$ converges uniformly on $X$ to a function $f:X \,{\rightarrow}\, {\RR}$ if, and only if, it converges uniformly to $f$ on each subset $A_{j}$.

\V

        (c) Suppose that $f$ is a function which is defined on $X$. For each $k$ let $M_{k} \,=\, {\sup}\,\{|f(x) - g_{k}(x)|: x{\in}X\}$.
    Then a necessary and sufficient condition for the sequence ${\gamma}$ to converge uniformly on $X$ to $f$ is that $\lim_{k \,{\rightarrow}\, {\infty}} M_{k} \,=\, 0$.

        Note: It is possible that some of the quantities $M_{k}$ might equal $+{\infty}$.
    The notation $\lim_{k \,{\rightarrow}\, {\infty}} M_{k}$ here then tacitly assumes that this can occur only for finitely many indices~$k$;
    compare with Remark~\Ref{RemrkC10.30} and Definition~\Ref{DefC10.40}.

\V

        The simple proof is left as an exercise.

\V
\V


        Cauchy claimed to prove the following result (or at least a result that is easily seen to be equivalent)):
    
        \h `{\em If a sequence of continuous functions converges pointwise to a function $f$ on an interval $I$, then the limit function $f$ is also continuous on $I$}.'

\noindent Unfortunately, this statement in not correct, as Example~(7) above illustrates.
    The next result, which appears to be due to Weierstrasse in its current form,
    shows that Cauchy's conclusion can be obtained if one replaces the hypothesis of `pointwise convergence' with `uniform convergence'.
 

\V

            \subsection{\small{\bf Theorem} (The `Uniform-Convergence-Preserves-Continuity' Theorem)}
            \label{ThmF05.40}

\V

        Let $X$ be a nonempty subset of ${\RR}$.

\V

        (a) Let ${\gamma} \,=\, (g_{1},g_{2},\,{\ldots}\,)$
    be a sequence of real-valued functions with domain $X$. Assume that the sequence ${\gamma}$ converges uniformly on $X$ to a function $f:X \,{\rightarrow}\, {\RR}$.
    If each function $g_{k}$, $k \,=\, 1,2,\,{\ldots}\,$, is continuous at a certain point $c$ of $X$,
    then the function $f$ is also continuous at $c$.
    Likewise, if each function $g_{k}$ is continuous on $X$, then $f$ is continuous on $X$.


\V

        (b) Let ${\gamma} \,=\, (g_{1},g_{2},\,{\ldots}\,)$ be a sequence of real-valued functions which are defined and continuous on~$X$.
    Suppose that ${\gamma}$ converges pointwise on $X$ to a function $f:X \,{\rightarrow}\, {\RR}$,
    and that for each interval $[a,b]$ for which $X\,{\cap}\,[a,b] \,\,{\neq}\,\, {\emptyset}$ the convergence to $f$ is uniform.
    Then $f$ is continuous on $X$.

\V

        {\bf Proof} (based on the `${\varepsilon}{\delta}$' characterization of continuity)

\V

        (a) Note that for each $x$ in $X$ and each $k$ in ${\NN}$ one has
        \begin{displaymath}
        |f(x) - f(c)| \,=\, |(f(x) - g_{k}(x)) + (g_{k}(x) - g_{k}(c)) + (g_{k}(c) - f(c))|
    \,\,{\leq}\,\
        |f(x) - g_{k}(x)| + |g_{k}(x) - g_{k}(c)| + |g_{k}(c) - f(c)| \h ({\ast})
        \end{displaymath}
    Now let ${\varepsilon}\,>\,0$ be given, and let $B$ be large enough so that if $k\,\,{\geq}\,\,B$ then $|f(z)-g_{k}(z)|\,\,{\leq}\,\,{\varepsilon}/3$ for all $z$ in $X$.
    Let $k$ be such an index, and let ${\delta}\,>\,0$ be small enough that if $|x-c|\,<\,{\delta}$ then $|g_{k}(x)-g_{k}(c)|\,<\,{\varepsilon}/3$.
    (Such ${\delta}$ exists because of the hypothesis that $g_{k}$ is continuous at $c$.)
    With this choice of $k$, Inequality~$({\ast})$ then implies
        \begin{displaymath}
        |f(x) - f(c)|\,\,{\leq}\,\
        |f(x) - g_{k}(x)| + |g_{k}(x) - g_{k}(c)| + |g_{k}(c) - f(c)|\,<\,
    \frac{{\varepsilon}}{3} + \frac{{\varepsilon}}{3} + \frac{{\varepsilon}}{3} \,=\, {\varepsilon}
        \end{displaymath}
        for all $x$ in $X$ such that $|x-c|\,<\,{\delta}$.
    The continuity of $f$ at $c$ now follows.
    Of course if each $g_{k}$ is continuous at each point of $X$, then $c$ can be any element of $X$, hence $f$ is continuous on $X$.

\V

        (b) This result follows easily by applying Part~(a) to the sets of the form $X\,{\cap}\,[a,b]$; the details are left as an exercise.

\V
\V

            \subsection{\small{\bf Examples}}
            \label{ExampF05.50}

\V

    
\hspace*{\parindent}(1) The preceding theorem provides an explanation for the fact that the sequence in Example~\Ref{ExampF05.30}~(7) fails to be uniformly convergent on $[0,1]$.
    Indeed, the functions $g_{k}(x) \,=\, x^{k}$ in that example are continuous on $[0,1]$, but the limit function $f$ fails to be continuous at~$1$.

\V

        (2) The hypothesis, in the preceding theorem, that the continuous functions $g_{k}$ converge {\em uniformly} to $f$ on $X$,
    is sufficient to guarantee the continuity of the limit function $f$,
    but is certainly not necessary; see Example~\Ref{ExampF05.30}~(6).
    Indeed, even the hypothesis that the functions $g_{k}$ be continuous is not necessary to get $f$ continuous;
    the reader is invited to find a suitable example.

\V
\V

        The `${\varepsilon}/3$' proof given above is the standard one found in most texts.
    The classic text {\em Principles of Mathematical Analysis} by W.~Rudin bases the proof on the `sequential' characterization of continuity, using the following result.


\V

            \subsection{\small{\bf Lemma}}
            \label{LemmaF05.55}

        Suppose that ${\gamma} \,=\, (g_{1},g_{2},\,{\ldots}\,)$ is a sequence of functions defined on a nonempty subset $X$ of ${\RR}$,
    and assume that ${\gamma}$ converges uniformly on $X$ to a function~$f:X \,{\rightarrow}\, {\RR}$.
    Let ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,):{\NN} \,{\rightarrow}\, {\RR}$ be a Cauchy sequence in $X$.
    Suppose that for each $n$ in ${\NN}$ the corresponding sequence $g_{n}{\circ}{\xi} \,=\, (g_{n}(x_{1}), g_{2}(x_{2}),\,{\ldots}\,)$ of values is also Cauchy.
    Then the sequence $f{\circ}{\xi} \,=\, (f(x_{1},f(x_{2}),\,{\ldots}\,))$ is Cauchy.
    Furthermore, for each $n$ one let $A_{n} \,=\, \lim\,g_{n}{\circ}{\xi}$, and likewise let $A \,=\, \lim\,f{\circ}{\xi}$.
    Then the sequence $(A_{1},A_{2},\,{\ldots}\,)$ is convergent, and one has $\lim_{n \,{\rightarrow}\, {\infty}} A_{n} \,=\, A$.

    Note: This last equation is sometimes written
        \begin{displaymath}
        \lim_{n \,{\rightarrow}\, {\infty}} \lim_{k \,{\rightarrow}\, {\infty}} g_{n}(x_{k}) \,=\, 
    \lim_{k \,{\rightarrow}\, {\infty}} \lim_{n \,{\rightarrow}\, {\infty}} g_{n}(x_{k}).
        \end{displaymath}

\V

        {\bf Proof} Note that for each $m$, $k$ and $n$ in ${\NN}$ one has
        \begin{displaymath}
        |f(x_{m+k}) - f(x_{m})|\,\,{\leq}\,\,|f(x_{m+k}) - g_{n}(x_{m+k})|
    + |g_{n}(x_{m+k}) - g_{n}(x_{m})| + |g_{n}(x_{m}) - f(x_{m})| \h ({\ast})
        \end{displaymath}
    Let ${\varepsilon}\,>\,0$ be given, and let $B_{1}$ in ${\NN}$ be large enough that if $n\,\,{\geq}\,\,B_{1}$ then $|f(y) - g_{n}(y)|$ for all $y$ in $X$.
    (Such $B_{1}$ exists because of the hypothesis that the sequence ${\gamma}$ converges uniformly to $f$ on $X$.)
    In particular, for such $n$ one has $|f(x_{m+k}) - g_{n}(x_{m+k})|\,<\,{\varepsilon}/3$ and $|g_{n}(x_{m}) - f(x_{m})|\,<\,{\varepsilon}/3$
    for each $m$ and $k$ in ${\NN}$, since the points of the sequence ${\xi}$ are in~$X$.
    Now fix $n\,\,{\geq}\,\,B_{1}$, and for that $n$ let $B_{2}$ be large enough that if $m\,\,{\geq}\,\,B_{2}$ then $|g_{n}(x_{m+k}) - g_{n}(x_{m})|\,<\,{\varepsilon}/3$.
    (Such $B_{2}$ exists because of the hypothesis that the sequence $(g_{n}(x_{1}),g_{n}(x_{2}),\,{\ldots}\,)$ is Cauchy.)
    Then from $({\ast})$ one gets
        \begin{displaymath}
        |f(x_{m+k}) - f(x_{m})|\,<\,\frac{{\varepsilon}}{3} + \frac{{\varepsilon}}{3} + \frac{{\varepsilon}}{3} \,=\, {\varepsilon} \mbox{ for all $m\,\,{\geq}\,\,B_{2}$ and all $k$}.
        \end{displaymath}
    It follows that the sequence $(f(x_{1}),f(x_{2}),\,{\ldots}\,)$ is Cauchy, as claimed.
    Note that the limits $A$ and $A_{n}$ which appear in the statement of the result exist and are finite because Cauchy sequences are convergent.

    By doing a similar analysis on the inequality
        \begin{displaymath}
        |A-A_{n}|\,\,{\leq}\,\,|A-f(x_{k})| + |f(x_{k}) - g_{n}(x_{k})| + |g_{n}(x_{k}) - A_{n}|,
        \end{displaymath}
    one also sees that $A \,=\, \lim_{n \,{\rightarrow}\, {\infty}} A_{n}$, as required.

\V

        {\bf Remarks}

\V

        (1) It is easy to give an alternate proof of Theorem~\Ref{ThmF05.40} based on the results of the precedeing lemma;
    the details are left as an exercise.

\V

        (2) Notice that the preceding lemma does not assume any continuity of the functions $g_{n}$ or $f$;
    nor does it assume that the limit of the Cauchy sequence ${\xi}$ lies in $X$.

\V
\V

        As in the theory of limits of sequences of numbers, there are useful concepts of `Cauchy sequences' in the context of pointwise and uniform convergence.

\V

            \subsection{\small{\bf Definition}}
            \label{DefF05.60}

\V

        Let $X$ be a nonempty subset of ${\RR}$, and let ${\gamma} \,=\, (g_{1},g_{2},\,{\ldots}\,)$ be a sequence of real-valued functions with domain $X$.

\V

        (1) The sequence ${\gamma}$ is said to be {\bf pointwise Cauchy on $X$} provided for each $x$ in $X$ the sequence ${\gamma}(x) \,=\, (g_{1}(x),g_{2}(x),\,{\ldots}\,)$ is a Cauchy sequence.
    That is: for every $x$ in $X$ and every ${\varepsilon}\,>\,0$, there exists $B$ such that  if $n$ in ${\NN}$ satisfies $n\,\,{\geq}\,\,B$ then $|g_{n+k}(x)-g_{n}(x)|\,<\,{\varepsilon}$ for all $k$ in ${\NN}$.

\V


        (2) The sequence ${\gamma}$ is said to be {\bf uniformly Cauchy on $X$} provided that
        for every ${\varepsilon}\,>\,0$, there exists $B$ such that if $n$ in ${\NN}$ satisfies $n\,\,{\geq}\,\,B$ then $|g_{n+k}(x)-g_{n}(x)|\,<\,{\varepsilon}$ for all $k$ in ${\NN}$ and all $x$ in $X$.

\V

        {\bf Example} In the proof of Part~(a) of Theorem~\Ref{ThmE45.125B}, the sequence ${\Gamma} \,=\, (G_{1},G_{2},\,{\ldots}\,)$ introduced there is shown to satisfy the inequality
        \begin{displaymath}
        \left|G_{n+k}(x)-G_{n}(x)\right|\,\,{\leq}\,\,\frac{2M(b-a)^{2}}{n}
        \end{displaymath}
    for all $x$ in $[a,b]$.
    Thus the sequence ${\Gamma}$ is uniformly Cauchy on $[a,b]$.

            \subsection{\small{\bf Theorem}}
            \label{ThmF05.70}

\V

        Let $X$ be a nonempty subset of ${\RR}$, and let ${\gamma} \,=\, (g_{1},g_{2},\,{\ldots}\,)$ be a sequence of real-valued functions with domain~$X$.

\V

        (a) A necessary and sufficient condition for ${\gamma}$ to converge pointwise on $X$ to some function is that ${\gamma}$ be pointwise Cauchy on $X$.

\V

        (b) A necessary and sufficient condition for ${\gamma}$ to converge uniformly on $X$ to some function is that ${\gamma}$ be uniformly Cauchy on $X$.

\V

        The simple proof is left as an exercise.

\V
\V

        Uniform convergence works well at `preserving continuity', but is not nearly so useful in dealing with differentiability.

\V

            \subsection{\small{\bf Examples}}
            \label{ExampF05.80}

\V

\hspace*{\parindent}(1) For each $k$ in ${\NN}$ let $g_{k}:{\RR} \,{\rightarrow}\, {\RR}$ be given by the formula
        \begin{displaymath}
        g_{k}(x) \,=\, \sqrt{x^{2} + \frac{1}{k}}.
        \end{displaymath}
    Each of these functions is differentiable on ${\RR}$. In addition, it is easy to show that the sequence ${\gamma} \,=\, (g_{1},g_{2},\,{\ldots}\,)$ converges uniformly on ${\RR}$ to the absolute-value function $\mbox{abs}$.
    The latter function fails to be differentiable at $0$. Thus, it is {\em not} the case that the uniform limit of functions which are differentiable on an interval needs to be differentiable on that interval.

\V

        (2) For each $k$ in ${\NN}$ let $g_{k}:{\RR} \,{\rightarrow}\, {\RR}$ be given by $g_{k}(x) \,=\, ({\sin}\,kx)/\sqrt{k}$.
    Then it is obvious that the sequence ${\gamma} \,=\, (g_{1},g_{2},\,{\ldots}\,)$ converges uniformly on ${\RR}$ to the zero function.
    However the corresponding sequence of derivatives $g_{k}'(x) \,=\, \sqrt{k}{\cos}\,kx$ does not converge to a differentiable function;
    for example, it fails to even remain bounded at $x \,=\, 0$.

\V
\V

        In contrast, uniform convergence works quite well with {\em anti}derivatives.

\V

            \subsection{\small{\bf Theorem} (The `Uniform-Converence-Preserves-Antidifferentiability' Theorem)}
            \label{ThmF05.90}

\V

    Let ${\gamma} \,=\, (g_{1},g_{2},\,{\ldots}\,)$ be a sequence of real-valued functions defined on an open interval $I$ in ${\RR}$.
     Assume that ${\gamma}$ converges pointwise on $I$ to a function $f:I \,{\rightarrow}\, {\RR}$, and that on each closed bounded subinterval $[a,b]$ of $I$ the convergence is uniform.
    If each of the functions $g_{k}$ has an antiderivative on $I$, then $f$ has an antiderivative on $I$.
    More precisely, fix a point $c$ in $I$, and set $G_{k} \,=\, D^{-1}_{c} g_{k}$.
    Then the sequence ${\Gamma} \,=\, (G_{1}, G_{2},\,{\ldots}\,)$ converges pointwise on $I$ to a function $F:I \,{\rightarrow}\, {\RR}$ such that $F'(x) \,=\, f(x)$ for all $x$ in $I$, and $F(c) \,=\, 0$.
    The sequence ${\Gamma}$ converges uniformly to $F$ on each closed bounded subinterval $[a,b]$ of $I$.

\V

        {\bf Proof} Let $a$ and $b$ be numbers in $I$ such that $a\,<\,c\,<\,b$.

        {\bf Claim 1} The sequence ${\Gamma}$ is uniformly Cauchy on $[a,b]$.

\V

        {\bf Proof of Claim 1} For each $n$ and $k$ in ${\NN}$ and each $x$ in $[a,b]$ one has
        \begin{displaymath}
        \left|(G_{n+k} - G_{n})(x)\right| \,=\, \left|(G_{n+k} - G_{n})(x) - (G_{n+k} - G_{n})(c)\right| \,=\, 
        \left|(G_{n+k} - G_{n})'(\hat{x})(x-c)\right| \,=\, 
        |(g_{n+k}(\hat{x}) - g_{n}(\hat{x}))(x-c)|
        \end{displaymath}
    for some number $\hat{x}$ in $\mbox{Seg}\,[x,c] \,{\subseteq}\, [a,b]$. Since, by hypothesis,
    the sequence ${\gamma}$ is uniformly convergent on $[a,b]$, it follows that the sequence ${\Gamma}$ is also uniformly Cauchy on $[a,b]$.
    Indeed, let ${\varepsilon}\,>\,0$ be given, and let $B$ in ${\NN}$ be large enough that
    if $n\,\,{\geq}\,\,B$ then $|g_{n+k}(z)-g_{n}(z)|\,<\,{\varepsilon}/|b-a|$ for all $z$ in $[a,b]$.
    In particular this inequality holds for $z \,=\, \hat{x}$, so one gets
        \begin{displaymath}
        \left|(G_{n+k} - G_{n})(x)\right|\,\,{\leq}\,\,|(g_{n+k}(\hat{x}) - g_{n}(\hat{x}))(x-c)|\,<\,\left(\frac{{\varepsilon}}{|b-a|}\right)|x-c|\,\,{\leq}\,\,{\varepsilon}
        \end{displaymath}
    for all $n$ and $k$ in ${\NN}$ such that $n\,\,{\geq}\,\,B$ and all $x$ in $[a,b]$.
    Claim~1 follows.

        Now define $F:{\RR} \,{\rightarrow}\, {\RR}$ by the rule $F(x) \,=\, \lim_{n \,{\rightarrow}\, {\infty}} G_{n}(x)$ for each $x$ in $I$.
    Thus, the sequence ${\Gamma}$ converges uniformly to $F$ on each closed bounded subinterval $[a,b]$ of $I$.
    In particular, one also has $F(c) \,=\, \lim_{n \,{\rightarrow}\, {\infty}} G_{k}(c) \,=\, 0$, since by construction one has $G_{n}(c) \,=\, 0$ for each~$n$.

        {\bf Claim 2} For each $x$ in $I$ one has $F'(x) \,=\, f(x)$.

        {\bf Proof of Claim 2} Fix $x$ in $I$, and let $a$ and $b$ in $I$ be chosen so that $x,c{\in}(a,b)$.
    For each $t$ in $[a,b]$ and each $n$ and $k$ in ${\NN}$ one has
        \begin{displaymath}
        \left|\left(G_{n+k} - G_{n}\right)(t) - \left(G_{n+k} - G_{n}\right)(x)\right| \,=\, 
        \left|\left(G_{n+k} - G_{n}\right)'(\hat{x})(t-x)\right| \,=\, 
        \left|g_{n+k}(\hat{x}) - g_{n}(\hat{x})\right||t-x|
        \end{displaymath}
    for some $\hat{x}$ in $\mbox{Seg}\,[x,t]$. Now let $Y \,=\, [a,b]{\setminus}\{x\}$.
    Then for $t$ in $Y$ the preceding equation can be written
        \begin{displaymath}
        \left|\left(\frac{G_{n+k}(t) - G_{n+k}(x)}{t-x}\right) -
    \left(\frac{G_{n}(t) - G_{n}(x)}{t-x}\right)\right| \,=\, \left|g_{n+k}(\hat{x}) - g_{n}(\hat{x})\right| \h ({\ast})
        \end{displaymath}
    For convenience, define ${\varphi}_{m}:Y \,{\rightarrow}\, {\RR}$ by the rule
        \begin{displaymath}
        {\varphi}_{m}(t) \,=\, \frac{G_{m}(t) - G_{m}(x)}{t-x}
        \end{displaymath}
    Then $({\ast})$ can be written
        \begin{displaymath}
        \left|{\varphi}_{n+k}(t) - {\varphi}_{n}(t)\right|\,\,{\leq}\,\,|g_{n+k}(\hat{x})-g_{n}(\hat{x})|.
        \end{displaymath}
    Let ${\varepsilon}\,>\,0$ be given, and let $B$ in ${\NN}$ be large enough that if $n\,\,{\geq}\,\,B$ then $|g_{n+k}(z)-g_{n}(z)|\,<\,{\varepsilon}$ for all $z$ in $[a,b]$.
    Then one has
        \begin{displaymath}
        \left|{\varphi}_{n+k}(t) - {\varphi}_{n}(t)\right|\,\,{\leq}\,\,{\varepsilon} \mbox{ for all $n\,\,{\geq}\,\,B$ and all $k$ in ${\NN}$ and for all $t$ in $Y$}.
        \end{displaymath}
    That is, the sequence ${\Phi} \,=\, ({\varphi}_{1},{\varphi}_{2},\,{\ldots}\,)$ is uniformly Cauchy on $Y$.
    Define ${\psi}:Y \,{\rightarrow}\, {\RR}$ by
        \begin{displaymath}
        {\psi}(t) \,=\, \frac{F(t) - F(x)}{t-x}
        \end{displaymath}
    Then it follows from the fact that $\lim_{k \,{\rightarrow}\, {\infty}} G_{k}(t) \,=\, F(t)$ that the sequence ${\Phi}$ converges uniformly on $Y$ to ${\psi}$.

        We are now ready to apply Lemma~\Ref{LemmaF05.55}, with the roles of $X$, $g_{n}$ and $f$ in the lemma being played here by $Y$, ${\varphi}_{n}$ and ${\psi}$, respectively.
    Indeed, let ${\tau} \,=\, (t_{1},t_{2},\,{\ldots}\,)$ be a sequence of points in $Y$ converging to $x$; note that ${\tau}$ is a Cauchy sequence in $Y$, but its limit, $x$, is not in $Y$.
    Since, by hypothesis, $G_{n}$ is differentiable at $x$, one sees that for each $n$ in ${\NN}$, one has $\lim_{k \,{\rightarrow}\, {\infty}} {\varphi}_{n}(t_{k}) \,=\, A_{n}$, where $A_{n} \,=\, G'_{n}(x) \,=\, g_{n}(x)$.
    Lemma~\Ref{LemmaF05.55} then implies the sequence $(A_{1},A_{2},\,{\ldots}\,\,{\ldots}\,)$ converges to some number $A$.
    However, one has $A_{n} \,=\, g_{n}(x)$, and by hypothesis $\lim_{n \,{\rightarrow}\, {\infty}} g_{n}(x) \,=\, f(x)$.
    Thus, $A \,=\, f(x)$, and this holds for every choice of the sequence ${\tau}$.
    In addition, it also follows from that same lemma that the sequence $({\psi}(t_{1}),{\psi}(t_{2}),\,{\ldots}\,)$ also converges to~$A$, that is, to $f(x)$, independently of the choice of ${\tau}$.
    That is, one has
        \begin{displaymath}
        \lim_{k \,{\rightarrow}\, {\infty}} \frac{F(t_{k})-F(x)}{t_{k}-x} \,=\, f(x).
        \end{displaymath}
    Since this is independent of the choice of sequence ${\tau}$, it follows that ${\displaystyle \lim_{t \,{\rightarrow}\, x} \frac{F(t)-F(x)}{t-x}} \,=\, f(x)$.
    This implies that $F'(x)$ exists and equals $f(x)$. Since this argument works for every $x$ in $I$, it follows that $F'(x) \,=\, f(x)$ for all $x$ in $I$, as claimed.

\V

        The preceding result is often phrased as follows, in terms of derivatives instead of antiderivatives.

\V

            \subsection{\small{\bf Corollary}}
            \label{CorF05.100}

\V

        Suppose that ${\Gamma} \,=\, (G_{1},G_{2},\,{\ldots}\,)$ is a sequence of real-valued functions which are differentiable on an open interval $I$,
    and assume that there is a point $c$ in $I$ such that the numerical sequence ${\Gamma}(c)$ is convergent.
    If the corresponding sequence ${\gamma} \,=\, (g_{1},g_{2},\,{\ldots}\,)$ of derivatives $g_{k} \,=\, G_{k}'$ converges uniformly on $I$ to some function $f$,
    then the sequence ${\Gamma}$ converges uniformly on $I$ to a function $F$ such that $F' \,=\, f$ on $I$.

\V

        The reader is encouraged to see why this corollary is equivalent to the preceding theorem.
    (The only feature that requires any thought is to see where the hypothesis, that the sequence ${\Gamma}(c) \,=\, (G_{1}(c),G_{2}(c),\,{\ldots}\,)$ is convergent, fits in.)


\V
\V
\V


        One of the iconic results of classical analysis is the Weierstrass Approximation Theorem.
    Weierstrass was $70$~years of age when he published it.

\V
\V

             \subsection{\small{\bf Theorem} (The Weierstrass Approximation Theorem)}
            \label{ThmF05.90A}

\V

        Suppose that $f:[a,b] \,{\rightarrow}\, {\RR}$ is continuous on the closed bounded interval $[a,b]$.
    Then there is an infinite sequence ${\varphi} \,=\, (p_{1},p_{2},\,{\ldots}\,)$ of polynomials which converges uniformly to $f$ on $[a,b]$.

\V

        {\bf Proof} The proof given here is essentially that of Serge Bernstein. It is ultimately based on ideas from probability theory.
    We follow the treatment in the text `Advanced Calculus' by Patrick Fitzgerald.

\V

        \underline{Background -- Coin Tosses} Suppose that one has a two-sided coin which is `weighted' so that the probability of `Heads' is $p$ and thus the probability of `Tails' is $q \,=\, 1-p$.
    Now do an experiment in which the coin is tossed $n$ times. Then it is known that the probability of getting exactly $j$ heads out of $n$ tosses is $C(n,j)p^{j}(1-p)^{j}$,
    where $C(n,j)$ denotes the {\bf binary coefficient} ${\displaystyle \frac{n!}{j!(n-j)!}}$.
    In Bernstein's proof one can think of what is going on in probabilistic terms, but that is not required here.

\V

        Note that, for all $x$ in $[0,1]$, we have the following identities:
        \begin{displaymath}
        \sum_{j=0}^{n} C(n,j)x^{j}(1-x)^{n-j} \,=\, 1 \h (I)
        \end{displaymath}
    This follows from the Binomial Theorem: $(a+b)^{n} \,=\, \sum_{j=0}^{n} C(n,j)a^{j}b^{n-j}$, with $a \,=\, x$, $b \,=\, 1-x$.
        \begin{displaymath}
        \sum_{j=0}^{n} \left(\frac{j}{n}\right)C(n,j)x^{j}(1-x)^{n-j} \,=\, x \h (II)
        \end{displaymath}
    This follows from the Binomial Theorem applied to $x(x+(1-x))^{n-1}$.

        In Equations ($I$) and ($II$) $n$ can be in any natural number. In the next equation one needs $n\,\,{\geq}\,\,2$:
        \begin{displaymath}
        \sum_{j=0}^{n} \left(\frac{j(j-1)}{n(n-1)}\right)C(n,j)x^{j}(1-x)^{n-j} \,=\, x^{2} \h (III)
        \end{displaymath}
    This follows from the Binomial Theorem applied to $x^{2}(x+(1-x))^{n-2}$.

        \underline{Claim} For each $n$ in ${\NN}$ and each $x$ in $[0,1]$ one has
        \begin{displaymath}
        \sum_{j=0}^{n} \left(x - \frac{j}{n}\right)^{2}C(n,j)x^{j}(1-x)^{n-j} \,=\, \frac{x(1-x)}{n} \h (IV)
        \end{displaymath}

        \underline{Proof of Claim} The result is trivially true if $n \,=\, 1$, so assume that $n\,\,{\geq}\,\,2$.
    Let $L_{n}$ denote the left side of Equation~$(IV)$. Then one can write
        \begin{displaymath}
        L_{n} \,=\, \sum_{j=0}^{n} \left(x^{2} - \frac{2xj}{n} + \frac{j^{2}}{n^{2}}\right)C(n,j)x^{j}(1-x)^{n-j} \,=\, A_{n} + B_{n} + C_{n},
        \end{displaymath}
    where
        \begin{displaymath}
        A_{n} \,=\, x^{2}\sum_{j=0}^{n} C(n,j)x^{j}(1-x)^{n-j}, \h
        B_{n} \,=\, (-2x)\sum_{j=0}^{n} \frac{j}{n}C(n,j)x^{j}(1-x)^{n-j}, \h
        C_{n} \,=\, \sum_{j=0}^{n} \frac{j^{2}}{n^{2}}C(n,j)x^{j}(1-x)^{n-j}.
        \end{displaymath}
    From $(I)$ and $(II)$ above one sees that $A_{n} \,=\, x^{2}$ and $B_{n} \,=\, -2x^{2}$.
    Also, one easily computes that
        \begin{displaymath}
        \frac{j^{2}}{n^{2}} \,=\, \left(\frac{n-1}{n}\right)\left(\frac{j^{2}}{n(n-1)}\right) \,=\, 
    \left(\frac{n-1}{n}\right)\left(\frac{j(j-1)}{n(n-1)} + \frac{j}{n(n-1)}\right).
        \end{displaymath}
    Now Equations $(II)$ and $(III)$ can be applied to get
        \begin{displaymath}
        C_{n} \,=\, \left(\frac{n-1}{n}\right)\left(x^{2} + \frac{x}{n-1}\right).
        \end{displaymath}
    Thus,
        \begin{displaymath}
        A_{n} + B_{n} + C_{n} \,=\, x^{2} - 2x^{2} + \left(\frac{n-1}{n}\right)\left(x^{2} + \frac{x}{n-1}\right) \,=\, \frac{x(1-x)}{n},
        \end{displaymath}
    as required.

\V

        It is easy to see that if one can prove the Approximation Theorem for continuous functions on the interval $[0,1]$,
    then the theorem is true for the general interval $[a,b]$, so we restrict our attention to the case $[a,b] \,=\, [0,1]$.

        Thus, consider a function $f:[0,1] \,{\rightarrow}\, {\RR}$ which is continuous on $[0,1]$.
    For each $n$ in ${\NN}$, Bernstein uses as the approximating $n$-th degree polynomial ${\displaystyle p_{n}(x) \,=\, \sum_{j=0}^{n} f\left(\frac{j}{n}\right)B_{n,j}(x)}$.
    Notice that, by Equation~$(I)$ above, one has
        \begin{displaymath}
        |f(x)-p_{n}(x)|\,\,{\leq}\,\,\sum_{j=0}^{n} \left|f(x) - \left(\frac{j}{n}\right)\right|C(n,j)x^{j}(1-x)^{n-j} \h ({\ast})
        \end{displaymath}
    Now let ${\varepsilon}\,>\,0$ be given. Since $f$ is uniformly continuous on $[0,1]$,
    there exists ${\delta}\,>\,0$ so that $|f(y)-f(x)|\,<\,{\varepsilon}/2$ for all $x,y$ in $[0,1]$ such that $|y-x|\,\,{\leq}\,\,{\delta}$.
    Let $M$ be the maximum value of $f$ on $[0,1]$. Then for every $x$ in $[0,1]$ and every $j \,=\, 0,1,2,\,{\ldots}\,n$ one of the following must hold:
        \begin{displaymath}
        \left|x-\frac{j}{n}\right|\,<\,{\delta}, \mbox{ hence }
        \left|f(x) - f\left(\frac{j}{n}\right)\right|\,<\,\frac{{\varepsilon}}{2};
        \end{displaymath}
    or
        \begin{displaymath}
        \left|x-\frac{j}{n}\right|\,\,{\geq}\,\,{\delta}, \mbox{ hence }
   \left|f(x) - f\left(\frac{j}{n}\right)\right|\,\,{\leq}\,\,2M\,\,{\leq}\,\,
    \frac{2M}{{\delta}^{2}}\left(x- \frac{j}{n}\right)^{2}
        \end{displaymath}
    Thus one has
        \begin{displaymath}
        \left|f(x) - f\left(\frac{j}{n}\right)\right|\,<\,\frac{{\varepsilon}}{2} + \frac{2M}{{\delta}^{2}}\left(x- \frac{j}{n}\right)^{2}.
        \end{displaymath}
    for all $x$ in $[0,1]$ and all $j \,=\, 0,1,2,\,{\ldots}\,n$.
    Multiply both sides of this last inequality by $C(n,j)x^{j}(1-x)^{n-j}$ and sum over $j$ to get
        \begin{displaymath}
        |f(x) - p_{n}(x)|\,\,{\leq}\,\,
    \frac{{\varepsilon}}{2}\sum_{j=0}^{n} C(n,j)x^{j}(1-x)^{n-j} + \frac{2M}{{\delta}^{2}}\sum_{j=0}^{n} \left(x- \left(\frac{j}{n}\right)\right)^{2}C(n,j)x^{j}(1-x)^{n-j}.
        \end{displaymath}
    In light of Equations~$(I)$ and~$(IV)$, one then has
        \begin{displaymath}
        |f(x) - p_{n}(x)|\,\,{\leq}\,\,\frac{{\varepsilon}}{2} + \frac{2M}{{\delta}^{2}}\frac{x(1-x)}{n}\,\,{\leq}\,\,\frac{{\varepsilon}}{2} + \frac{2M}{n{\delta}^{2}}.
        \end{displaymath}
    Clearly if $n\,>\,{\displaystyle \frac{4M}{{{\varepsilon}\delta}^{2}}}$, then one gets
        \begin{displaymath}
        |f(x) - p_{n}(x)|\,<\,\frac{{\varepsilon}}{2} + \frac{{\varepsilon}}{2} \,=\, {\varepsilon}.
        \end{displaymath}
    It follows that the sequence $(p_{1},p_{2},\,{\ldots}\,)$ converges uniformly to $f$ on $[0,1]$, sa required.



\V
\V


                \section{{\bf Extending Continuous Functions}}
                \label{SectF20}

\V

        In Example~\Ref{ExampD20.80}~(1) it is pointed out that if $f:X \,{\rightarrow}\, {\RR}$ is a function which is continuous on a nonempty set $X \,{\subseteq}\, {\RR}$,
    and if $S$ is a nonempty subset of $X$, then the restriction $f|_{S}$ of $f$ to $S$ is also continuous on $S$.

        It is natural to ask to what extent the converse statement holds; that is,
    to ask whether a real-valued function which is continuous on a set has a continuous extension on a larger set.
    (See Definition~\Ref{DefA30.10} to review the meanings of `restriction' and `extension'.)
    The next result gives an important partial answer.

\V 

            \subsection{\small{\bf Theorem} (The Tietze Extension Theorem in ${\RR}$)}
            \label{ThmF20.20}

\V

        Suppose that $g:S \,{\rightarrow}\, {\RR}$ is a function whose domain is a nonempty subset $S$ of ${\RR}$.
    Assume that $S$ is a closed subset of ${\RR}$, and that $g$ is continuous at each point of its domain $S$.
    Let $T$ be a subset of ${\RR}$ such that $S \,{\subseteq}\, T$.
    Then there is an extension $f:T \,{\rightarrow}\, {\RR}$ of $g$ to $T$ which is continuous on $T$.

\V

        \underline{Proof} Assume first that $T \,=\, {\RR}$.
    If $S \,=\, {\RR}$ there is nothing to prove -- simply let $f \,=\, g$ -- so assume that $S$ is a proper subset of ${\RR}$.
    Let $U \,=\, {\RR}{\setminus}S$.
    Then, by Theorem~\Ref{ThmB30.180}, $U$ is the union of a countable family ${\cal F}$ of mutually disjoint open intervals.

        To simplify the exposition, assume for the moment that $S$ is a {\em bounded} closed subset of ${\RR}$; that is, a compact subset.
    Then the open intervals in ${\cal F}$ are of three types: $I_{1} \,=\, (-{\infty},m)$, where $m$ is the minimum element of $S$;
    $I_{2} \,=\, (M,+{\infty})$, where $M$ is the maximum element of $S$;
    and $(a,b)$, where $a$ and $b$ are certain elements of $S$ with $a\,<\,b$.
    (The case $(-{\infty},+{\infty})$ cannot occur, since $S \,\,{\neq}\,\, {\emptyset}$ implies $U \,\,{\neq}\,\, {\RR}$.)

        In reality, there  are infinitely many ways to extend $g$ to a function which is continuous on all of ${\RR}$, but perhaps the simplest one is constructed as follows:

        \h (i)\,\, On the interval $I_{1} \,=\, (-{\infty},m)$ the function $f$ must be chosen to be continuous and to approach the value $g(m)$ as $x{\nearrow}m$.
    The simplest choice is to make $f(x) \,=\, g(m)$ for all $x\,<\,m$, so $f$ is constant on the open interval $I_{1}$.

        \h (ii)\, Likewise, the simplest choice that can do the job on $I_{2} \,=\, (M,+{\infty})$ is $f(x) \,=\, g(M)$ for all $x\,>\,M$.

      \h (iii) On intervals of the form $(a,b)$, one must choose $f(x)$ so that $f(x) \,{\rightarrow}\, g(a)$ as $x{\searrow}a$, while $f(x) \,{\rightarrow}\, g(b)$ as $x{\nearrow}b$.
    The obvious simplest way to do that in a continuous manner is to use the process of `linear interpolation'; see Example~\Ref{ExampD20.58B}.

    That is, define $f:{\RR} \,{\rightarrow}\, {\RR}$ by the rule
        \begin{equation}
        \label{EqnF.20}
        f(x) \,=\, \left\{
        \begin{array}{cl}
              g(x)      & \mbox{if $x{\in}S$} \\
              g(b)      & \mbox{if $x$ is in an interval in ${\cal F}$ of the form $(-{\infty},b)$} \\
              g(a)      & \mbox{if $x$ is in an interval in ${\cal F}$ of the form $(a,+{\infty})$} \\
        (1-t)g(a)+tg(b) & \mbox{if $x$ is in an interval in ${\cal F}$ of the form $(a,b)$} \\
                          &  \mbox{and $t$ in $(0,1)$ is such that $x \,=\, (1-t)a+tb$}
        \end{array}
        \right.
        \end{equation}
    It is obvious from the first line of Equation~\Ref{EqnF.20} that $f$ is an extension of $g$.
    Now let $c$ be any real number. In light of Theorem~\Ref{ThmD20.55}, in order to show that $f$ is continuous at $c$ it suffices to verify that $\lim_{k \,{\rightarrow}\, {\infty}} f(x_{k}) \,=\, f(c)$ for every {\em monotonic} sequence ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,)$ in ${\RR}$ which converges to $c$.


        \underline{Case 1} Suppose that $c$ is {\em not} in $S$.
    Then $c$ must lie in exactly one of the intervals that form the family ${\cal F}$; call that interval $I_{c}$.
    By Theorem~\Ref{ThmC80.75} one must have $x_{k}$ in $I_{c}$ for $k$ sufficiently large.

        If $I_{c}$ is of the form $(-{\infty},m)$ then $f(c) \,=\, f(x_{k}) \,=\, g(m)$ for all sufficiently large $k$;
in particular, $\lim_{k \,{\rightarrow}\, {\infty}} f(x_{k}) \,=\, f(c)$.
    A similar argument shows that if $I_{c} \,=\, (M,+{\infty})$ then $\lim_{k \,{\rightarrow}\, {\infty}} f(x_{k}) \,=\, f(c)$.

        If $I_{c}$ is of the form $(a,b)$ for certain elements $a$ and $b$ in $S$, the analysis is slightly harder.
    In this situation one can write $c \,=\, (1-t)a+tb$ for a unique $t$ in $(0,1)$.
    Likewise, when $k$ is large enough that $x_{k}{\in}I_{c}$ one has $x_{k} \,=\, (1-t_{k})a+t_{k}b$ for certain $t_{k}$ in $(0,1)$.
    It is clear that the condition $\lim_{k \,{\rightarrow}\, {\infty}} x_{k} \,=\, c$ implies $\lim_{k \,{\rightarrow}\, {\infty}} t_{k} \,=\, t$.
    Thus from the equations
        \begin{displaymath}
        f(x_{k}) \,=\, f((1-t_{k})a + t_{k}b) \,=\, (1-t_{k})g(a) + t_{k}g(b),
        \end{displaymath}
    valid for sufficiently large $k$, and
        \begin{displaymath}
        f(c) \,=\, f((1-t)a+tb) \,=\, (1-t)g(a)+tg(b),
        \end{displaymath}
    it follows that
        \begin{displaymath}
        \lim_{k \,{\rightarrow}\, {\infty}} f(x_{k}) \,=\, \lim_{k \,{\rightarrow}\, {\infty}} ((1-t_{k})g(a) + t_{k}g(b)) \,=\, (1-t)g(a)+tg(b) \,=\, f(c).
        \end{displaymath}

        Thus, if $c$ is not in $S$ then $f$ is certainly continuous at $c$.

\V

        \underline{Case 2} Suppose that $c{\in}S$, and suppose, to be definite, that the sequence ${\xi}$ is monotonic {\em up}, so that $x_{k}\,\,{\leq}\,\,c$ for each $k$.
    Let ${\varepsilon}\,>\,0$ be given, and let ${\delta}\,>\,0$ be small enough that if $x{\in}S$ and $|x-c|\,<\,{\delta}$ then $|g(x)-g(c)|\,<\,{\varepsilon}$.
    (That such ${\delta}\,>\,0$ exists follows from the hypothesis that $g$ is continuous at each point of $S$.)

        Suppose that there exists a point $u$ in $S$ such that $c-{\delta}\,<\,u\,<\,c$.
    Then the assumption, that the sequence ${\xi}$ is monotonic up and converges to $c$, implies that $u\,<\,x_{k}\,\,{\leq}\,\,c$ for all sufficiently large $k$.
    For such $k$ either $x_{k}{\in}S$ or $x_{k} \not \in S$.
    In the former situation one has $|f(x_{k})-f(c)| \,=\, |g(x_{k})-g(c)|\,<\,{\varepsilon}$, by definition of ${\delta}$ and the fact that $f$ is an extension of $g$.
    In the latter situation there must exist $a,b$ in $S$, with $u\,\,{\leq}\,\,a \,<\,b\,\,{\leq}\,\,c$, such that $(a,b){\in}{\cal F}$ and $x_{k}{\in}(a,b)$.
    Then $f(x_{k}) \,=\, (1-t)g(a)+tg(b)$ for some $t$ in $(0,1)$. Also, $|c-a|\,<\,{\delta}$ and $|c-b|\,<\,{\delta}$, so one gets
        \begin{displaymath}
        |f(c)-f(x_{k})| \,=\, |g(c)-((1-t)g(a)+tg(b))| \,=\, |(1-t)(g(c)-g(a)) + t(g(c)-g(b))|\,\,{\leq}\,\,
        \end{displaymath}
        \begin{displaymath}
    (1-t)|g(c)-g(a)| + t|g(c)-g(b)|\,<\,(1-t){\varepsilon}+t{\varepsilon} \,=\, {\varepsilon}.
        \end{displaymath}
    In any event, if such $u$ exists, then $|f(c)-f(x_{k})|\,<\,{\varepsilon}$ for all sufficiently large $k$.

        Suppose, instead, that no such $u$ exists. Then either $c \,=\, m$
    or there exists $a$ in $S$, with $a\,\,{\leq}\,\,c-{\delta}$, such that the open interval $(a,c)$ is in the family ${\cal F}$.
    In the former situation one has $x_{k}\,\,{\leq}\,\,m$ for all $k$, hence $f(x_{k}) \,=\, g(c)$ by definition.
    In the latter case one has $x_{k}$ in $(a,c)$ for all sufficiently large $k$ and $f(x_{k}) \,=\, (1-t_{k})g(a)+t_{k}g(c)$ for some $t_{k}$ in $(0,1)$, by definition of $f$ on points outside $S$.
    As in Case~(1) above, it is clear that $\lim_{k \,{\rightarrow}\, {\infty}} t_{k} \,=\, 1$ since $\lim_{k \,{\rightarrow}\, {\infty}} x_{k} \,=\, c$.
    This implies
        \begin{displaymath}
        \lim_{k \,{\rightarrow}\, {\infty}} f(x_{k}) \,=\, \lim_{k \,{\rightarrow}\, {\infty}} (1-t_{k})g(a) + tg(c) \,=\, 0{\cdot}g(a)+1{\cdot}g(c) \,=\, g(c).
        \end{displaymath}
    In particular, if $k$ is sufficiently large then $|f(x_{k})-f(c)|\,<\,{\varepsilon}$.

        It follows that, in all circumstances, $\lim_{k \,{\rightarrow}\, {\infty}} f(x_{k}) \,=\, f(c)$.
    A similar argument works for sequences ${\zeta} \,=\, (z_{1},z_{2},\,{\ldots}\,)$ which are monotonic down and converge to $c$.

\V

        The conclusion then is that $f$ is continuous at all $c$ in ${\RR}$, so that $f$ is an extension of $g$ to ${\RR}$ which is continuous on ${\RR}$, as required.

\V

        Suppose now that the set $T$ referred to in the statement of the theorem is a proper subset of~${\RR}$.
    Let $f:{\RR} \,{\rightarrow}\, {\RR}$ be any continuous extension of $g$ to ${\RR}$.
    Then clearly $f|_{T}:T \,{\rightarrow}\, {\RR}$ is a continuous extension of $g$ to $T$.

\V

        \underline{Remark} The `Tietze' referred to in the title of the preceding theorem is the Austrian mathematician Heinrich Tietze (1880-1964), and it is part of his work in the field of General Topology.
    His general extension theorem, which is well outside the scope of these {\em Notes}, applies to spaces much more complicated than the set ${\RR}$.

\V

            \subsection{\small{\bf Definition}}
            \label{DefF20.30}

\V

        Suppose that $g:S \,{\rightarrow}\, {\RR}$ is a continuous function whose domain is a nonempty closed subset $S$ of ${\RR}$.
    The continuous extension $f:{\RR} \,{\rightarrow}\, {\RR}$ constructed from $g$ by the `linear interpolation' technique given above is called the {\bf piecewise-linear extension of $g$ to ${\RR}$}.
    Likewise, if $T$ is a subset of ${\RR}$ such that $S \,{\subseteq}\, T$, 
    then the restriction to $T$ of the function $f$ just described is called the {\bf piecewise-linear extension of $g$ to $T$}.

\V
\V

            \subsection{\small{\bf Examples}}
            \label{ExampF20.40}

\V

\hspace*{\parindent}(1) Let $C \,{\subseteq}\, [0,1]$ denote the Cantor Ternary Set; see Definition~\Ref{DefA20.100}.
    The set $C$ is closed; see Example~\Ref{ExampC20.80}~(3)).
    There is a natural function $g:C \,{\rightarrow}\, {\RR}$ given by the following rule:

        If $x{\in}C$, express $x$ as a one-free ternary decimal $0 \stackrel{(3)}{.}d_{1}d_{2},\,{\ldots}\,d_{k}\,{\ldots}\,$ with all the ternary digits $d_{k}$ either $0$ or $2$.
    For each $k$ let $c_{k} \,=\, d_{k}/2$, so that for each $k$, $c_{k}$ is either $0$ or $1$.
    Then $g(x)$ is the number with {\em binary} representation $0 \stackrel{(2)}{.}c_{1}c_{2}\,{\ldots}\,c_{k}\,{\ldots}\,$.

        The function $g$ is continuous on the closed set $C$. Indeed, let ${\varepsilon}\,>\,0$ be given, and let ${\delta}$ be a number of the form $1/3^{m}$, where $m$ in ${\NN}$ is large enough that $1/2^{m}\,<\,{\varepsilon}$.
    Let $x$ and $x'$ be numbers in $C$ whose $1$-free ternary representations are $x \,=\,  \stackrel{(3)}{{\cdot}}d_{1}d_{2}\,{\ldots}\,$ and $x' \,=\,  \stackrel{(3)}{{\cdot}}d'_{1}d'_{2}\,{\ldots}\,$.
    If $|x-x'|\,<\,{\delta}$ then one has $d_{j} \,=\, d'_{j}$ for $j \,=\, 1,2,\,{\ldots}\,m$;
    see Theorem~\Ref{ThmB30.65B}.
    Next, for each $j$ let $c_{j} \,=\, d_{j}/2$ and $c'_{j} \,=\, d_{j}/2$ as above.
    Then $c_{j} \,=\, c_{j}'$ for each $j \,=\, 1,2,\,{\ldots}\,m$, hence the numbers $g(x)$ and $g(x')$ have binary representations which agree for the first $m$ binary digits.
    It is clear from this that $|g(x)-g(x')|\,\,{\leq}\,\,1/2^{m}\,<\,{\varepsilon}$.
    The continuity of $g$ on $C$ follows.

        {\bf Definition} The piecewise-linear extension of $g$ to the closed interval $[0,1]$ is called the {\bf Cantor Function}.
    It is a simple exercise to prove that the Cantor function is monotonic up on $[0,1]$,
    and is constant on each of the `middle-thirds' open intervals which are removed from $[0,1]$ to form the set $C$; see Remark~\Ref{RemrkA20.125}.

\V

        (2) Let $S \,=\, \{x_{0},x_{1},x_{2},\,{\ldots}\,x_{m-1},x_{m}\}$ be a finite (nonempty) set of points in ${\RR}$, labelled in `strictly increasing order';
    that is, so that
        \begin{displaymath}
        x_{0}\,<\,x_{1}\,<\,x_{2}\,<\,\,{\ldots}\,\,<\,x_{m-1}\,<\,x_{m}.
        \end{displaymath}
    Let $g:S \,{\rightarrow}\, {\RR}$ be a function defined on $S$.
    Since $S$ is finite, it is automatically closed in ${\RR}$, and the function $g$ is automatically continuous on $S$.

        Let $h:[x_{0},x_{m}] \,{\rightarrow}\, {\RR}$ denote the piecewise-linear extension of $g$ to the closed interval $T \,=\, [x_{0},x_{m}]$.
    It is easy to characterize the function $h$ geometrically.
    Indeed, the graph of $h$ is the $m$-sided polygonal line in ${\RR}^{2}$ whose sides are the line segments connecting the points $(x_{j-1},g(x_{j-1}))$ to $(x_{j},g(x_{j}))$ for each $j \,=\, 1,2,\,{\ldots}\,m$.

        \underline{Note} In light of the terminology introduced in Definition~\Ref{DefD20.58A}, one refers to the function $h$ constructed here as the {\bf piecewise linear function associated with $g$}.


\V
\V

        Now consider the analogous `extension problem' for a continuous function $g:S \,{\rightarrow}\, {\RR}$ whose domain $S$ is an {\em arbitrary} nonempty subset $S$ of ${\RR}$.
    In light of the previous theorem, it is clear that the real issue is whether $g$ can be extended continuously to the closure $\overline{S}$ of $S$.
    Indeed, if it can be so extended, then the preceding theorem guarantees that $g$  can be extended continuously to {\em every} set $T$ such that $S \,{\subseteq}\, T \,{\subseteq}\, {\RR}$.
    The next examples show that analysis when $S$ is {\em not} closed is far from obvious.

\V

            \subsection{\small{\bf Examples}}
            \label{ExampF20.50}

\V

\hspace*{\parindent}(1) Let $S$ be the half-open interval $(0,1]$, and define $g:S \,{\rightarrow}\, {\RR}$ by the rule $g(x) \,=\, 1/x$ for all $x$ such that $0\,<\,x\,\,{\leq}\,\,1$.
    It is clear that $g$ is continuous at each point of $S$. Nevertheless, $g$ does {\em not} have a continuous extension to the closure $\overline{S} \,=\, [0,1]$ of $S$.
    Indeed, if such a continuous extension $f:[0,1] \,{\rightarrow}\, {\RR}$ were to exist,
    it would have to be bounded (by the Extreme-Value Theorem), and thus any restriction of $f$ -- including $g$ -- would also have to be bounded.
    However, $g$ is clearly unbounded, since $\lim_{x{\searrow}0} 1/x \,=\, +{\infty}$.

\V


        (2) Consider the function $g:(0,1] \,{\rightarrow}\, {\RR}$ defined as follows:

        \h\, (i) If $x \,=\, 1/(2m)$ for some $m$ in ${\NN}$ then $g(x) \,=\, 0$;
    and if $x \,=\, 1/(2m-1)$ for some $m$ in ${\NN}$ then $g(x) \,=\, 1$.

        \h (ii) Suppose that $x$ satisfies the inequalities ${\displaystyle \frac{1}{2m}\,<\,x\,<\, \frac{1}{2m-1}}$ for some $m$ in ${\NN}$.
    Express $x$ in the form $(1-t)/(2m)+t/(2m-1)$ for a unique $t$ in $(0,1)$, and then set $g(x) \,=\, t$.

\noindent It is easy to check that $g$ is continuous on $(0,1]$. Even more, the function $g$ is bounded on $(0,1]$. Nevertheless, $g$ does not have a continuous extension to $[0,1]$.
    To see this, note that if $f$ is such an extension, then one must have, in particular,
        \begin{displaymath}
        f(0) \,=\, \lim_{k \,{\rightarrow}\, {\infty}} f\left(\frac{1}{k}\right).
        \end{displaymath}
    However $f(1/k) \,=\, g(1/k)$ for each $k$ in ${\NN}$, and half of the terms $g(1/k)$ equal $1$ (namely, when $k$ is odd), while half equal $0$ (namely, when $k$ is even).
    Thus, any such extension must have the impossible property that $f(0) \,=\, 0$ and $f(0) \,=\, 1$.
    In other words, no such extension exists.

\V

        (3) Suppose that $h: \,{\rightarrow}\, {\RR}$ is a function which is continuous at each point of an open interval $(a,b)$ in ${\RR}$, and let $c$ be a point of $(a,b)$.
    Let $S \,=\, (a,b){\setminus}\{c\} \,=\, (a,c)\,{\cup}\,(c,b)$, and define $g:S \,{\rightarrow}\, {\RR}$ by the rule
        \begin{displaymath}
        g(x) \,=\, \frac{h(x)-h(c)}{x-c} \mbox{ for all $x$ in $S$}.
        \end{displaymath}
    Notice that the fraction on the right side of this equation does not make sense when $x \,=\, c$,
    so it is natural to ask whether $g$ has a continuous extension from $S$ to $(a,b)$.
    The analysis of this question is left as an exercise; but the main conclusion is that such an extension $f:(a,b) \,{\rightarrow}\, {\RR}$ exists if, and only if, $h$ is differentiable at $c$.
    Moreover, in this case one has
        \begin{displaymath}
        f(x) \,=\, \left\{
        \begin{array}{cl}
        {\displaystyle \frac{h(x)-h(c)}{x-c}} & \mbox{if $x{\in}S$}; \\
                                              &                      \\
                        h'(c)                 & \mbox{if $x \,=\, c$}.
        \end{array}
            \right.
        \end{displaymath}

        \underline{Remark} Some authors reverse the thinking used in this example to give an alternate approach to the derivative. This approach is called the {\bf Caratheodory Definition of the Derivative} in honor of the mathematician who introduced it.


\V
\V

        It is easy to state a general criterion for when a continuous function on a set extends continuously to the closure of that set.
    In order to simplify the discussion, it helps to introduce some terminology.

\V

            \subsection{\small{\bf Definition}}
            \label{DefF20.60}

        Let $g:S \,{\rightarrow}\, {\RR}$ be a real-valued function whose domain is a nonempty subset $S$ of ${\RR}$.
    One says that {\bf the function $g$ preserves the Cauchy-Sequence Property on $S$}
    provided that if ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,)$ is a Cauchy sequence of points in $S$,
    then $g{\circ}{\xi} \,=\, (g(x_{1}),g(x_{2}),\,{\ldots}\,)$ is also a Cauchy sequence in ${\RR}$.

        \underline{Note}: One often paraphrases the statement `$g$ preserves the Cauchy-Sequence Property on $S$' as {\bf `$g$ preserves Cauchy sequences on $S$'}  or {\bf `$g$ maps Cauchy sequences in $S$ to Cauchy sequences in ${\RR}$}'.

\V

            \subsection{\small{\bf Examples}}
            \label{ExampF20.70}

\V

\hspace*{\parindent}(1) In Examples~\Ref{ExampF20.50}~(1) and~(2) it is easy to show that the sequence $g(1), g(1/2), \,{\ldots}\,g(1/k),\,{\ldots}\,$ is not Cauchy.
    Thus it is not the case that $g$ preserves Cauchy sequences on $S$.

\V

        (2) Suppose that $g:S \,{\rightarrow}\, {\RR}$ is continuous on a \underline{closed} nonempty subset $S$  of ${\RR}$.
    Then $g$ preserves Cauchy sequences on $S$.

        Indeed, suppose that ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,)$ is a Cauchy sequence in $S$.
    By Theorem~\Ref{ThmC70.20} (`The Cauchy Criterion'), the sequence ${\xi}$ converges to some number $c$ in ${\RR}$.
    By the hypothesis that the set $S$ is closed, the limit of the sequence ${\xi}$ must be in the set $S$.
    By the hypothesis that $g$ is continuous at each point of $S$ it follows that $\lim_{k \,{\rightarrow}\, {\infty}} g(x_{k}) \,=\, g(c)$.
    By the Cauchy Criterion (again) it follows that the sequence $(g(x_{1}),g(x_{2}),\,{\ldots}\,)$ is also a Cauchy sequence, as required.

\V

        (3) Suppose that $g:I \,{\rightarrow}\, {\RR}$ is differentiable at each point of an open interval $I$.
    Assume further that $g'$ is bounded on $I$; more precisely, assume there exists $M\,>\,0$ in ${\RR}$ such that $|g'(x)|\,\,{\leq}\,\,M$ for all $x$ in $I$.
    Then $g$ preserves Cauchy sequences on $I$.

        To see this, let ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,)$ be a Cauchy sequence in $I$.
    If ${\varepsilon}\,>\,0$ is given, let $B$ be large enough that if $j,k\,\,{\geq}\,\,B$ then $|x_{k}-x_{j}|\,<\,{\varepsilon}/M$.
    It follows from the Lagrange Mean-Value Theorem that if $j,k\,\,{\geq}\,\,B$ then there exists $z_{jk}$ in $I$ such that
        \begin{displaymath}
        |g(x_{k})-g(x_{j})| \,=\, |g'(x_{jk})|{\cdot}|x_{k}-x_{j}|\,\,{\leq}\,\,
    M{\cdot}\left(\frac{{\varepsilon}}{M}\right) \,=\, {\varepsilon}.
        \end{displaymath}
    Thus, the sequence $(g(x_{1}),g(x_{2}),\,{\ldots}\,)$ is also Cauchy, as required.

\V

        The following is the analog, for the preceding definition, of Theorem~\Ref{ThmD20.55}.

\V

            \subsection{\small{\bf Theorem}}
            \label{ThmF20.80}

        Let $g:S \,{\rightarrow}\, {\RR}$ be a function whose domain is a nonempty subset $S$ of ${\RR}$.
    A necessary and sufficient condition for $g$ to preserve the Cauchy-Sequence Property on $S$ is that if ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,)$ is a bounded monotonic sequence in $S$ then $g{\circ}{\xi} \,=\, (g(x_{1}),g(x_{2}),\,{\ldots}\,)$ is a Cauchy sequence in ${\RR}$.

\V

        One can prove this result using an argument similar to that used in the proof of Theorem~\Ref{ThmD20.55} above.
    The details are left as an exercise.

\V
\V

        Notice that in the previous theorem, as in Definition~\Ref{DefF20.60}, we do {\em not} assume that the function $g$ is continuous on $S$.
    The next result shows that this continuity occurs automatically.

\V

            \subsection{\small{\bf Theorem}}
            \label{ThmF20.85}

        Let $S$ be a nonempty suubset of ${\RR}$, and suppose that $g:S \,{\rightarrow}\, {\RR}$ preserves the Cauchy-Sequence Property on $S$,
    in the sense of Definition~\Ref{DefF20.60} above. Then $g$ is continuous on $S$.

\V

        \underline{Proof} Let $c$ be a point of $S$, and let ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,)$ be any sequence in $S$ converging to $c$.
    Let ${\tau} \,=\, (x_{1},c,x_{2},c,\,{\ldots}\,)$ be the sequence formed by inserting a $c$ between each term of the sequence ${\xi}$.
    It follows from Theorem~\Ref{ThmC20.90}, the `Odd/Even Convergence Theorem',
that the sequence ${\tau}$ also converges to $c$, and thus it must be a Cauchy sequence in $S$.
    By the hypothesis that $g$ preserves Cauchy sequences on $S$ it then follows that the sequence $g{\circ}{\tau} \,=\, (g(x_{1}),g(c),g(x_{2}),g(c),\,{\ldots}\,)$ is a Cauchy sequence in ${\RR}$, and thus must be convergent to some $L$ in ${\RR}$.
    It then follows that the subsequence of even-order terms, namely $(g(c),g(c),\,{\ldots}\,)$ converges to $L$, from which one gets $L \,=\, g(c)$.
    But it also follows from the convergence of $g{\circ}{\tau}$ to $L$ that the subsequence of odd-order terms also converges to $g(c)$.
    That is,
        \begin{displaymath}
        \lim_{j \,{\rightarrow}\, {\infty}} g(x_{j}) \,=\, L \,=\, g(c).
        \end{displaymath}
    Thus $g$ is continuous at $c$, as claimed.

\V
\V

            \subsection{\small{\bf Theorem} (Continuus Extensions and Cauchy Sequences)}
            \label{ThmF20.90}

\V

        Suppose that $g:S \,{\rightarrow}\, {\RR}$ is a function whose domain is a nonempty set $S \,{\subseteq}\, {\RR}$.
    Note that we do {\em not} assume that $g$ is continuous on $S$.

\V

        (a) If the function $g$ has a continuous extension to the closure $\overline{S}$ of $S$, then this extension is unique.

\V


        (b) A necessary and sufficient condition for $g$ to have a continuous extension to the closure $\overline{S}$ of $S$ is that $g$ preserve the Cauchy-Sequence Property on $S$, in the sense of Definition~\Ref{DefF20.60} above.

\V

        (c) More generally, a necessary and sufficient condition for $g$ to have a continuous extension to the closure $\overline{S}$ of $S$ is that $g$ preserve the Cauchy-Sequence Property on every {\em bounded} nonempty subset of $S$.

\V

        \underline{Proof}

        (a) Suppose that $f:\overline{S} \,{\rightarrow}\, {\RR}$ is a continuous extension of $g$ to $\overline{S}$.
    Let $c$ be a point of $\overline{S}$, and let ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,)$ be a sequence of points in $S$ such that $c \,=\, \lim_{k \,{\rightarrow}\, {\infty}} x_{k}$.
    (Such a sequence must exist because of Theorem~\Ref{ThmC80.70}.)
    Then one must have
        \begin{equation}
        \label{EqnF.30}
        f(c) \,=\, \lim_{k \,{\rightarrow}\, {\infty}} f(x_{k}) \,=\, \lim_{k \,{\rightarrow}\, {\infty}} g(x_{k}).
        \end{equation}
    The first equation reflects the fact that $f$ is continuous at $c$; the second equation reflects the fact that $f$ is an extension of $g$.
    The fact that $f(c) \,=\, \lim_{k \,{\rightarrow}\, {\infty}} g(x_{k})$ then implies that the value of $f$ at $c$ is completely determined by the original function $g$.
    In particular, there can be only one such function $f$, as claimed.

\V

        (b) Suppose first that $g$ has a continuous extension $f$ to $\overline{S}$.
    Let ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,)$ be a Cauchy sequence in $S$.
    Then ${\xi}$ converges to some number $c$ in ${\RR}$, and this point $c$ is an element of $\overline{S}$.
    Thus, by the continuity of $f$, one has $\lim_{k \,{\rightarrow}\, {\infty}} f(x_{k}) \,=\, f(c)$.
    In particular, the sequence $f{\circ}{\xi} \,=\, (f(x_{1}), f(x_{2}),\,{\ldots}\,)$ is convergent, hence it is a Cauchy sequence.
    But $f(x_{k}) \,=\, g(x_{k})$ for each $k$, since $f$ is an extension of $g$ from $S$ to $\overline{S}$ and each $x_{k}{\in}\,S$. 
    Thus $g$ preserves the Cauchy-Sequence Property on $S$.


        Conversely, suppose $g$ preserves the Cauchy-Sequence Property on $S$.
    Note that, by Theorem~\Ref{ThmF20.85}, it follows that $g$ is continuous on $S$.
    Let $c$ be a point of $\overline{S}$, and let ${\xi} \,=\, (x_{1}, x_{2},\,{\ldots}\,)$ and ${\zeta} \,=\, (z_{1}, z_{2}, \,{\dots}\,)$ be sequences in $S$ converging to $c$.
    Then, by Theorem~\Ref{ThmC20.90} (the `Odd/Even Convergence Theorem'),
    the sequence ${\tau} \,=\, (x_{1},z_{1},x_{2},z_{2},\,{\ldots}\,)$ also converges to $c$.
    In particular, ${\tau}$ is a Cauchy sequence in $S$, hence (by the hypothesis that $g$ preserves Cauchy sequences) $g{\circ}{\tau} \,=\, (g(x_{1}),g(z_{1}),g(x_{2}),g(z_{2}),\,{\ldots}\,)$ is a Cauchy sequence in ${\RR}$.
    Thus the sequence $g{\circ}{\tau}$ converges to some number $L$.
    By Theorem~\Ref{ThmC20.10A}~(b) it follows that the sequences $g{\circ}{\xi}$ and $g{\circ}{\zeta}$ also converge to $L$.

        Now define $f(c)$ to be $\lim_{k \,{\rightarrow}\, {\infty}} g(x_{k})$, where ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,)$ is any sequence in $S$ converging to $c$.
    The preceding discussion shows that the value $f(c)$ depends only on $c$, and not on the choice of sequence ${\xi}$.
    In other words, this process determines a function $f:\overline{S} \,{\rightarrow}\, {\RR}$.

        Finally, suppose that the function $f$ just constructed is {\em not} continuous at some point $c$ of $\overline{S}$.
    Then there exists ${\varepsilon}_{0}\,>\,0$ so that for each $k$ in ${\NN}$ there exists $z_{k}$ in $\overline{S}$ such that $|c-z_{k}|\,<\,1/(2k)$ but $|f(c)-f(z_{k})|\,\,{\geq}\,\,{\varepsilon}_{0}$.
    Since $z_{k}{\in}\overline{S}$, it follows from the definition of $f$ above that there must exist $x_{k}$ in $S$ such that $|z_{k}-x_{k}|\,<\,1/(2k)$ and $|f(z_{k})-g(x_{k})|\,<\,{\varepsilon}_{0}/2$.
    Now use the Triangle Inequality to obtain
        \begin{displaymath}
        |c-x_{k}|\,\,{\leq}\,\,|c-z_{k}| + |z_{k}-x_{k}|\,<\,\frac{1}{2k} + \frac{1}{2k} \,=\, \frac{1}{k}
        \end{displaymath}
    In particular, one has $\lim_{k \,{\rightarrow}\, {\infty}} x_{k} \,=\, c$; thus, by the construction of $f$, one also has $\lim_{k \,{\rightarrow}\, {\infty}} g(x_{k}) \,=\, f(c)$.
    Thus, there exists a number $B$ such that if $k\,\,{\geq}\,\,B$ then $|f(c)-g(x_{k})|\,<\,{\varepsilon}_{0}/2$.
    Now use the Triangle Inequality to get, for $k\,\,{\geq}\,\,B$,
        \begin{displaymath}
        |f(c)-f(z_{k})|\,\,{\leq}\,\,|f(c)-g(x_{k})| + |g(x_{k})-f(z_{k})|\,<\,
    \frac{{\varepsilon}_{0}}{2} + \frac{{\varepsilon}_{0}}{2} \,=\, {\varepsilon}_{0}.
        \end{displaymath}
    This contradicts the defining condition on the numbers $z_{k}$, namely that $|f(c)-f(z_{k})|\,\,{\geq}\,\,{\varepsilon}_{0}$ for {\em all} indices $k$.

    Since assuming that $f$ fails to be continuous at some point of $\overline{S}$ leads to a contradiction, it follows that $f$ is continuous at {\em each} point of $\overline{S}$.

\V

        (c) Suppose that $g$ has a continuous extension to $\overline{S}$, and let $W$ be any nonempty bounded subset of $S$.
    If ${\xi}$ is a Cauchy sequence in $W$ then it is clearly a Cauchy sequence in $S$, so by Part~(b) $g{\circ}{\xi}$ is a Cauchy sequence in ${\RR}$.
    Thus $g$ preserves the Cauchy-Sequence Property on $W$.

        Conversely, suppose that $g$ preserves the Cauchy-Sequence Property on $W$ for every nonempty bounded subset $W$ of $S$.
    Let ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,)$ be a Cauchy sequence in $S$.
    Since ${\xi}$ is convergent it must be bounded; see Theorem~\Ref{ThmC20.10A}~(c).
    That is, there exists a real number $M\,>\,0$ such that $-M\,\,{\leq}\,\,|x_{k}|\,\,{\leq}\,\,M$ for all $k$ in ${\NN}$.
    Let $W \,=\, S\,{\cap}\,[-M,M]$. Then $W$ is a bounded subset of $S$ which contains the terms $x_{k}$ of the sequence ${\xi}$.
    By the hypothesis that $g$ preserves the Cauchy-Sequence Property on bounded nonempty subsets of $S$, it follows that $g{\circ}{\xi}$ is a Cauchy sequence.
    Since ${\xi}$ can be any Cauchy sequence in $S$, it follows that $g$ preserves the Cauchy-Sequence Property on $S$.
    It then follows from Part~(b) that $g$ has a continuous extension to $\overline{S}$, as claimed.

\V
\V


            \subsection{\small{\bf Important Example -- Exponential Functions}}
            \label{ExampF20.100}

\V

        Let $b$ be a real number such that $b\,>\,1$. Let $S \,=\, {\QQ}$, the set of all rational numbers; note that $\overline{S}$, the closure of the set of rational numbers, is the set ${\RR}$.

        It is known that for every rational number $r$ there is a well-defined number $b^{r}$; see Example~\Ref{ExampD30.110}.
    Thus, for this fixed $b$, let $g_{b}:{\QQ} \,{\rightarrow}\, {\RR}$ be given by the rule
        \begin{displaymath}
        g_{b}(x) \,=\, b^{x} \mbox{ for all $x$ in ${\QQ}$}.
        \end{displaymath}
    Let ${\rho} \,=\, (r_{1},r_{2},\,{\ldots}\,)$ be a bounded monotonic sequence of rational numbers; to be definition, assume ${\rho}$ is monotonic-up and bounded above by $B$.
    Then, by the usual `order properties' of powers the corresponding sequence of values, $g_{b}{\circ}{\rho} \,=\, (b^{r_{1}},b^{r_{2}},\,{\ldots}\,)$ is also monotonic up and bounded above by $b^{B}$.
    In particular, by the Monotonic-Sequences Principle, the sequence $g_{b}{\circ}{\rho}$ is convergent, hence Cauchy.
    A similar argument shows that if ${\rho}$ is monotonic down and bounded below, then $g_{b}{\circ}{\rho}$ is Cauchy.
    It then follows from Theorem~\Ref{ThmF20.80} that $g_{b}$ preserves the Cauchy-Sequence Property on $S \,=\, {\QQ}$.
    Thus by Theorem~\Ref{ThmF20.90} the function $g_{b}$ has a unique continuous extension $f_{b}:{\RR} \,{\rightarrow}\, {\RR}$.

\V

            \subsection{\small{\bf Definition}}
            \label{DefF20.110}

        Let $b$ be a number such that $b\,>\,1$, and let $g_{b}:{\QQ} \,{\rightarrow}\, {\RR}$ and $f_{b}:{\RR} \,{\rightarrow}\, {\RR}$ be as in the preceding example.
    Then one calls the function $f_{b}$ the {\bf exponential function with base $b$}.
    The `modern' notation for this function is $\exp_{b}$; the old-fashioned `variables' notation for this function is $y \,=\, b^{x}$.

        If $0\,<\,b\,<\,1$ then one defines $b^{x}$ to mean ${\displaystyle \left(\frac{1}{b}\right)^{x}}$; and one sets $1^{x} \,=\, 1$.


\V

        \underline{Note}: It is an easy exercise to show that the exponential functions satisfy the usual `laws for exponents':
        \begin{displaymath}
        b^{x}{\cdot}b^{y} \,=\, b^{x+y}; \h \left(b^{x}\right)^{y} \,=\, b^{xy};
    \h b^{-x} \,=\, \frac{1}{b^{x}};
        \end{displaymath}
    and so on. Using the `exp' notation, this becomes
        \begin{displaymath}
        \exp_{b}(x+y) \,=\, \exp_{b}(x){\cdot}\exp_{b}(y); \h \exp_{\exp_{b}x}y \,=\, \exp_{b}(xy); \h \exp_{b}(-x) \,=\, \frac{1}{\exp_{b}(x)}.
        \end{displaymath}

\V
\V


        Theorem~\Ref{ThmF20.90} reduces the problem of the existence of continuous extensions
    to the question of whether a function on a bounded set preserves the Cauchy-Sequence Property.
    The next result provides an alternate approach that does not mention Cauchy sequences.

\V


            \subsection{\small{\bf Theorem} (Continuous Extensions and Uniform Continuity)}
            \label{ThmF20.120}

        Suppose that $g:S \,{\rightarrow}\, {\RR}$ is a function defined on a {\em bounded} nonempty subset $S$ of ${\RR}$.
    The following statements are equivalent:

\V

        \underline{Statement (1)} The function $g$ has a continuous extension to the closure $\overline{S}$ of $S$.

\V

        \underline{Statement (2)} The function $g$ is uniformly continuous on $S$.

\V

        \underline{Proof}

\V

        Suppose that Statement~(1) is true.
    If Statement~(2) were {\em not} true, there would exist ${\varepsilon}_{0}\,>\,0$ with the following property:
    for each $k$ in ${\NN}$ there exist points $t_{k}$ and $z_{k}$ in $S$ such that $|t_{k}-z_{k}|\,<\,1/k$ but $|g(t_{k})-g(z_{k})|\,\,{\geq}\,\,{\varepsilon}_{0}$.
    Let ${\tau} \,=\, (t_{1},t_{2},\,{\ldots}\,)$ and ${\zeta} \,=\, (z_{1},z_{2},\,{\ldots}\,)$ be the corresponding sequences.
    Since, by hypothesis, $S$ is a bounded set, it would then follow from the Bolzano-Weierstrass Theorem (i.e., Theorem~\Ref{ThmC30.10})
    that the sequence ${\tau})$ has a convergent subsequence ${\rho} \,=\, (r_{1},r_{2},\,{\ldots}\,)$.
    Let $c$ be the limit of the subsequence ${\rho}$; clearly $c$ would have to be an element of $\overline{S}$.
    Let $A$ be an infinite subset of ${\NN}$ such that ${\rho}$ is the subsequence of ${\tau}$ corresponding to $A$,
    and let ${\sigma} \,=\, (s_{1},s_{2},\,{\ldots}\,)$ be the subsequence of ${\zeta}$ corresponding to the same set $A$.
    The condition $|t_{k}-z_{k}|\,<\,1/k$ would imply that $|r_{j}-s_{j}|\,<\,1/j$ for each $j$ in ${\NN}$, and thus that ${\sigma}$ must also converge to $c$.
    It then follows from the results of Theorem~\Ref{ThmF20.90} that
        \begin{displaymath}
        f(c) \,=\, \lim_{j \,{\rightarrow}\, {\infty}} g(r_{j}) \,=\, \lim_{j \,{\rightarrow}\, {\infty}} g(s_{j}) \h ({\ast})
        \end{displaymath}
    In contrast, the inequality $|g(t_{k})-g(z_{k})|\,\,{\geq}\,\,{\varepsilon}_{0}$ for all $k$ would imply $|g(r_{j})-g(s_{j})|\,\,{\geq}\,\,{\varepsilon}_{0}$ for all $j$, contradicting Equation~$({\ast})$ above.

        Since assuming that Statement~(1) is true but Statement~(2) is false leads to a contradiction, then Statement~(1) implies Statement~(2), as claimed.

\V

        Conversely, suppose that Statement~(2) is true, and let ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,)$ be a Cauchy sequence in $S$.
    Let ${\varepsilon}\,>\,0$ be given. Then, by Statement~(2), there exists ${\delta}\,>\,0$ so that if $y,z$ in $S$ satisfy $|y-z|\,<\,{\delta}$ then $|g(y)-g(z)|\,<\,{\varepsilon}$.
    But by the definition of `Cauchy sequence', there exists $B$ so that if $j,k\,\,{\geq}\,\,B$ then $|x_{j}-x_{k}|\,<\,{\delta}$.
    Combine these facts to conclude that if $j,k\,\,{\geq}\,\,B$ then $|g(x_{j})-g(x_{k})|\,<\,{\varepsilon}$.
    Thus, the sequence $g{\circ}{\xi} \,=\, (g(x_{1}),g(x_{2}),\,{\ldots}\,)$ is a Cauchy sequence in ${\RR}$.
    Thus, Statement~(2) implies Statement~(1), as claimed.

\V

             \subsection{\small{\bf Remarks}}
            \label{RemrkF20.130}   

\V

\hspace*{\parindent}(1) The proof above that Statement~(2) implies Statement~(1) does not use the hypothesis that $S$ is a bounded set.

        In contrast, the proof that Statement~(1) implies Statement~(2) definitely needs that hypothesis.
    For example, let $g:{\QQ} \,{\rightarrow}\, {\RR}$ be given by the formula $g(r) \,=\, r^{2}$ for every rational number $r$.
    This function certainly has a continuous extension to $\overline{{\QQ}} \,=\, {\RR}$, namely the squaring function $f(x) \,=\, x^{2}$ for all $x$ in ${\RR}$;
    that is, Statement~(1) holds in this case.
    However, Statement~(2) does not hold; see Example~\Ref{ExampD40.30}~(2).

\V
\V

                \section{{\bf Open Sets and Continuity in ${\RR}$}}
                \label{SectF30}

\V
\V

        There is a beautiful characterization of the concept of `continuity' which does not mention `limit' or `convergence';
    neither does it mention `${\varepsilon}$' or `${\delta}$'.
    In fact, it formulates the concept entirely in terms of `open sets'.

\V

             \subsection{\small{\bf Theorem}}
            \label{ThmF30.20}

\V

        Let $f:X \,{\rightarrow}\, {\RR}$ be a real-valued function whose domain is a nonempty subset $X$ of ${\RR}$.
    A necessary and sufficient condition for $f$ to be continuous on $X$ is this:

     \h \underline{Condition O} For every open set $V$ in ${\RR}$ the corresponding inverse image $f^{-1}[V]$ is of the form $X\,{\cap}\,U$ for some open set $U$ in ${\RR}$.

\V

        {\bf Proof} Suppose that Condition O holds. Let $x$ be a point of $X$ and let ${\varepsilon}\,>\,0$ be given.
    If one sets $V \,=\, (f(x)-{\varepsilon},f(x)+{\varepsilon})$, then $V$ is an open set in ${\RR}$ -- indeed, it is an open interval -- and thus (by Condition~O) there exists an open set $U$ in ${\RR}$ such that $f^{-1}[V] \,=\, U\,{\cap}\,X$.
    In particular, since $f(x){\in}V$ by construction, it follows that $x{\in}U\,{\cap}\,X$ and thus $x{\in}U$.
    By definition of $U$ being `open', it then follows that there exists ${\delta}\,>\,0$ such that the open interval $(x-{\delta},x+{\delta})$ is a subset of $U$,
    and thus
        \begin{displaymath}
        (x-{\delta},x+{\delta})\,{\cap}\,X \,{\subseteq}\, U\,{\cap}\,X \,=\, f^{-1}[V].
        \end{displaymath}
    That is, if $y{\in}X$ and $|y-x|\,<\,{\delta}$, then $f(y){\in}V$ and thus $|f(y)-f(x)|\,<\,{\varepsilon}$.
    Thus, $f$ is continuous at $x$; and since this works for every $x$ in $X$, it follows that $f$ is continuous on $X$.

        Conversely, suppose that $f$ is continuous on $X$. Let $V$ be an open subset of ${\RR}$, and let $x$ be a point of $f^{-1}[V]$, so that $f(x){\in}V$.
    By the definition of $V$ being open, it follows that there exists ${\varepsilon}_{x}\,>\,0$, depending on $x$,
    such that $(f(x)-{\varepsilon}_{x},f(x)+{\varepsilon}_{x}) \,{\subseteq}\, V$.
    It then follows, by the continuity of $f$, that there is a corresponding ${\delta}_{x}\,>\,0$ such that if $y{\in}X$ and $|y-x|\,<\,{\delta}_{x}$,
    then $|f(y)-f(x)|\,<\,{\varepsilon}_{x}$; in particular, $f(y){\in}V$, hence $y{\in}f^{-1}[V]$.
    Let $U_{x} \,=\, (x-{\delta}_{x},x+{\delta}_{x})$. Then $U_{x}$ is an open set in ${\RR}$ and $U_{x}\,{\cap}\,X \,{\subseteq}\, f^{-1}[V]$.
    Let $U \,=\, {\bigcup}_{x{\in}f^{-1}[V]} U_{x}$. Then $U$ is an open subset of ${\RR}$, since each set $U_{x}$ is an open interval;
    and $U \,{\subseteq}\, f^{-1}[V]$, since the same is true for each set $U_{x}\,{\cap}\,X$.
    Finally, $f^{-1}[V] \,{\subseteq}\, U\,{\cap}\,X$, since if $x{\in}f^{-1}[V]$ then $x{\in}U_{x}$ and (of course) $x{\in}X$,
    hence $x{\in}U\,{\cap}\,X$.
    Thus, $f^{-1}[V] \,=\, U\,{\cap}\,X$, with $U$ open in ${\RR}$, and the desired result follows.

\V
\V

             \subsection{\small{\bf Remark}}
            \label{RemrkF30.30}

\V

\hspace*{\parindent}The argument given above can be modified easily to characterize the continuity of a function at a single point in terms of inverse images of open sets; the details are left as an exercise.


\V
\V

        The preceding theorem may, at first, appear to be just a curiosity. In fact, it may seem almost an annoyance:
    `Do we really need yet another characterization of continuity?'
    If anything, this characterization seems even {\em less} useful than the sequential and ${\varepsilon}{\delta}$ characterizations of continuity,
    in that it obscures the fundamental idea that a small change in the input of the function causes a small change in the corresponding output.

        However, it often happens in the evolution of mathematics that looking at a subject in a new way leads to far-reaching generalizations.
    That is the situation here. In fact, the branch of mathematics called `General Topology' (or `Point-set Topology')
    is based on the concept of open sets. Then the preceding theorem can be -- and is -- used as the basis for the concept of `continuity' in this subject.
    Likewise, the concept of `closed subset' can be defined in terms of open sets; see Corollary~\Ref{CorC80.77}.

    Although a treatment of `point-set topology' is well outside the scope of these {\em Notes},
    it does make sense to seek other concepts in ${\RR}$ which can be characterized completely in terms of the open subsets of ${\RR}$.
    The next couple of results do just that.
    To simplify the phrasing, however, it is useful to introduce some terminology.

\V

             \subsection{\small{\bf Definition}}
            \label{ThmF30.35}

\V

\hspace*{\parindent}(1) Let ${\cal F}$ be a family of sets. One says that the family ${\cal F}$ {\bf covers a set $X$}, or that {\bf ${\cal F}$ is a covering of $X$},
    provided that each element $x$ in $X$ is an element of at least one of the sets in the family ${\cal F}$.
        \underline{Equivalent formulation}: $X \,{\subseteq}\, {\bigcup}\, {\cal F}$.

\V

        (b) If, in addition, each set in the family ${\cal F}$ is an open subset of ${\RR}$,
    then one says that ${\cal F}$ is an {\bf open cover of $X$}.

\V

        {\bf Remark} The sets in the family need not be mutually disjoint; indeed, in most cases of interest these sets can have considerable overlap.
    Also, the union of the sets in the family ${\cal F}$ can be -- and often is -- strictly bigger than $X$.


\V

             \subsection{\small{\bf Theorem} (The Heine-Borel Theorem)}
            \label{ThmF30.40}

\V

        Let $X$ be a nonempty subset of ${\RR}$. Then the following conditions are equivalent:

\V
        \h (1) The set $X$ is a compact subset of ${\RR}$.

        \h (2) If ${\cal F}$ is a family of open subsets of ${\RR}$ which covers $X$,
    then there is a finite subfamily ${\cal F}'$ of ${\cal F}$ which also covers $X$.

\V

        {\bf Proof} Suppose that $X$ is compact. Then $X$ is bounded, so $a \,=\, {\inf}\,X$ and $b \,=\, {\sup}\,X$ are finite;
    and $X$ is closed, so $a$ and $b$ are elements of $X$. Clearly $X$ is a subset of the closed interval $[a,b]$.
    Let $U_{0} \,=\, [a,b]{\setminus}X$. Then it is easy to see that $U_{0}$ is an open subset of ${\RR}$.
    Indeed, by Corollary~\Ref{CorC80.77} one knows that ${\RR}{\setminus}X$ is an open set in ${\RR}$,
    and by Theorem~\Ref{ThmB30.180} one knows that this open set can be expressed as the disjoint union of open intervals in ${\RR}$.
    Since $a$ and $b$ are in $X$, and thus not in ${\RR}{\setminus}X$, two of the disjoint open intervals in question are $(-{\infty},a)$ and $(b,+{\infty})$.
    Since ${\RR}{\setminus}X$ is the disjoint union of the sets $(-{\infty},a)$, $(b,+{\infty})$ and $[a,b]{\setminus}X \,=\, U_{0}$,
    it follows that $U_{0}$ is the union of the remaining disjoint intervals forming ${\RR}{\setminus}X$. Thus, $U_{0}$ is open.

    Now let ${\cal F}$ be an open cover of $X$, and let ${\cal F}_{0} \,=\, {\cal F}\,{\cup}\,\{U_{0}\}$.
    Then ${\cal F}_{0}$ is an open cover of the interval $[a,b]$.
    Suppose that no finite subfamily of ${\cal F}_{0}$ is an open cover of $[a,b]$.
    Construct a bisection sequence $I_{1}$, $I_{2}$,\,{\ldots}\, (see Definition~\Ref{DefB30.05}) as follows:

\V

        \h (i)\, $I_{1} \,=\, [a,b]$.

        \h (ii) If $I_{k}$ has the property that no finite subfamily of ${\cal F}_{0}$ covers $I_{k}$,
    then the same must be true for at least one of the two halves of $I_{k}$; let $I_{k+1}$ be one of those halves.

\V

\noindent It follows from the Bisection Principle that the intersection of the intervals $I_{k}$ is a singleton set $\{c\}$.
    Let $U_{c}$ be an element of the family ${\cal F}_{0}$ such that $c{\in}U_{c}$.
    Let ${\delta}\,>\,0$ be small enough that $(c-{\delta},c+{\delta}) \,{\subseteq}\, U_{c}$.
    If $k$ is large enough that $(b-a)/2^{k-1}\,<\,{\delta}$, then $I_{k} \,{\subseteq}\, (c-{\delta},c+{\delta}) \,{\subseteq}\, U_{c}$, contrary to the hypothesis that no finite subfamily of ${\cal F}_{0}$ covers $I_{k}$.

        Thus, there must be a finite subfamily ${\cal F}_{0}'$ of ${\cal F}_{0}$ which covers $[a,b]$.
    Without loss of generality, assume that $U_{0}{\in}{\cal F}'_{0}$; indeed,
    adding one more set to a finite family of sets still gives a finite family, and its union still covers $[a,b]$.
    However, $X \,=\, [a,b]{\setminus}U_{0}$, so the union of the sets in the smaller subfamily ${\cal F}' \,=\, {\cal F}'_{0}{\setminus}\{U_{0}\}$ also covers $X$,
    since $U_{0}$ covers no points of $X$.
    That is, Condition~(1) implies Condition~(2).

        Conversely, suppose that Condition~(2) holds. For each $x$ in $X$ let $U_{x} \,=\, (x-1,x+1)$.
    Then clearly the family ${\cal F} \,=\, \{U_{x}:x{\in}X\}$ is an open cover of $X$.
    Since Condition~(2) holds, there must be a finite subfamily $\{U_{x_{1}}, U_{x_{2}},\,{\ldots}\,U_{x_{m}}\}$ which covers $X$.
    It follows that if $x{\in}X$ then $x{\in}U_{x_{j}}$ for some $j \,=\, 1,2,\,{\ldots}\,m$, and thus 
    $|x|\,\,{\leq}\,\,\max\,\{|x_{1}|+1,|x_{2}|+1,\,{\ldots}\,|x_{m}|+1\}$; in particular, $X$ is bounded.

        Next, suppose that $X$ is not closed, and let ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,)$ be a sequence in $X$ which converges to some number $c$ not in $X$.
    Then it is clear that the set $Y \,=\, \{c,x_{1},x_{2},\,{\ldots}\,x_{k},\,{\ldots}\,\}$ is closed in ${\RR}$.
    Let $U_{0} \,=\, {\RR}{\setminus}Y$, so that $U_{0}$ is an open subset of ${\RR}$. Also, for each $k$ in ${\NN}$ let ${\delta}_{k} \,=\, |c-x_{k}|/2$.
    Note that ${\delta}_{k}\,>\,0$ (since $c$ is not in $X$ but $x_{k}$ is), and that $\lim_{k \,{\rightarrow}\, {\infty}} {\delta}_{k} \,=\, 0$.
    Now let $U_{k} \,=\, (x_{k}-{\delta}_{k},x_{k}+{\delta}_{k})$ for each $k$ in ${\NN}$,
    and let ${\cal F} \,=\, \{U_{0},U_{1},\,{\ldots}\,U_{k},\,{\ldots}\,\}$.
    Clearly the family ${\cal F}$ covers $X$; thus by Condition~(2) there is a finite subfamily ${\cal F}'$ with the same property.
    Without lose of generality one may assume that there is an index $m$ such that ${\cal F}' \,=\, \{U_{0},U_{1},\,{\ldots}\,U_{m}\}$.
    (Indeed, if the sets $U_{j_{1}}$, $U_{j_{2}}$, \,{\ldots}\, $U_{j_{n}}$ together form a cover of $X$,
    simply let $m$ be the largest of the indices $j_{1}$,\,{\ldots}\,$j_{n}$.)
    If $k$ is large enough that ${\delta}_{k}\,<\,{\delta}_{j}/2$ for each $j \,=\, 1,2,\,{\ldots}\,m$, then
        \begin{displaymath}
        |c-x_{k}| \,=\, 2{\delta}_{k}\,\,{\leq}\,\,{\delta}_{j} \mbox{ for each $j \,=\, 1,2,\,{\ldots}\,m$}.
        \end{displaymath}
    Thus, by the Modified Triangle Inequality one has
        \begin{displaymath}
        |x_{j}-x_{k}| \,=\, |(x_{j}-c) + (c-x_{k})|\,\,{\geq}\,\,||x_{j}-c| - |x_{k}-c||\,\,{\geq}\,\, 2{\delta}_{j} - {\delta}_{j} \,=\, {\delta}_{j}.
        \end{displaymath}
    In particular, $x_{k}$ is not in $U_{j}$ for $j \,=\, 1,2,\,{\ldots}\,m$. Since $x_{k}$ is also not in $U_{0}$,
    it follows that $X$ is not the union of the sets $U_{0}$, $U_{1}$,\,{\ldots}\,$U_{m}$, contrary to Condition~(2).

\V
\V

        One can also characterize the convex sets in ${\RR}$ in terms of open sets.

\V 

             \subsection{\small{\bf Theorem}}
            \label{ThmF30.50}

\V

        Let $X$ be a nonempty subset of ${\RR}$. Then the following statements are equivalent:

\V

        \h (1) The set $X$ is a convex subset of ${\RR}$.

        \h (2) For every pair of open sets $U$ and $V$ in ${\RR}$ such that $U\,{\cup}\,V \,{\supseteq}\, X$, $U\,{\cap}\,X \,\,{\neq}\,\, {\emptyset}$ and $V\,{\cap}\,X \,\,{\neq}\,\, {\emptyset}$, one has $U\,{\cap}\,V \,\,{\neq}\,\, {\emptyset}$.

\V

        The simple proof is left to the reader.

\V
\V

                \section{{\bf Miscellaneous Results}}
                \label{SectF35}

\V
\V

        Example~\Ref{ExampF05.30}~(6) shows that it is possible for a sequence $(f_{1},f_{2},\,{\ldots}\,)$ of continuous functions
    to converge pointwise to a continuous function $f$ on a compact set, but without the convergence being uniform on that set.
    A key feature of that example is the sudden appearance -- and disappearance -- of `bumps' at which the the sequence stops moving steadily towards the limit function, at least for a while.
    The next result shows that something like those `bumps' would be needed in any other example of this phenomenon.

\V


             \subsection{\small{\bf Theorem} (Dini's Uniform-Convergence Theorem)}
            \label{ThmF35.20}

\V

        Suppose that ${\varphi} \,=\, (f_{1},f_{2},\,{\ldots}\,f_{k},\,{\ldots}\,)$ is a sequence of continuous functions
    which converges pointwise on a nonempty compact set $X$ to a continuous function $f$.

\V


        (a) Suppose, in addition, that this convergence is `monotonic up' on $X$, in the sense that for each $x$ in $X$ the numerical sequence 
    ${\varphi}(x) \,=\, (f_{1}(x),f_{2}(x),\,{\ldots}\,)$ is monotonic up. Then the convergence of the sequence ${\varphi}$ to $f$ is uniform on $X$.

\V

        (b) Likeswise, if the convergence is monotonic down, then ${\varphi}$ converges uniformly to $f$ on $X$.

\V

        The straight-forward proof is left as an exercise.

\V
\V


             \subsection{\small{\bf Theorem}}
            \label{ThmF35.30}

\V

        Suppose that $X$ is a compact nonempty subset of ${\RR}$ and that $f:X \,{\rightarrow}\, {\RR}$ is a continuous function with domain $X$.
    Let $Y \,=\, f[X]$ denote the image of the function $f$. Then:

\V

        (a) The set $Y$ is compact.

\V

        (b) If, in addition, the function $f$ is one-to-one on $X$, then the inverse function $f^{-1}:Y \,{\rightarrow}\, X$ is continuous on $Y$.

\V

        The proof is left as an exercise.

\V
\V

        Many of the most fruitful methods for solving equations of various sorts, in both pure and applied mathematics, are based on the concept of a `fixed point'.

\V

             \subsection{\small{\bf Definition}}
            \label{DefF35.40}

\V

        Suppose that $f:X \,{\rightarrow}\, X$ is a function defined on a set $X$ such that all the values of $f$ are also in $X$.
    A point $c$ in $X$ is said to be a {\bf fixed point of $f$} provided $f(c) \,=\, c$.

\V

             \subsection{\small{\bf Example}}
            \label{ExampF35.50}

\V

        Let $C$ be a fixed positive number, and define $f:(0,+{\infty}) \,=\, (0,+{\infty})$ by the rule
        \begin{displaymath}
        f(x) \,=\, \frac{1}{2}\left(x+\frac{C}{x}\right).
        \end{displaymath}
    A fixed point for this function would be a number $x\,>\,0$ such that $f(x) \,=\, x$; that is,
        \begin{displaymath}
        \frac{1}{2}\left(x+\frac{C}{x}\right) \,=\, x
        \end{displaymath}
    Multiply both sides by $2x$ to get the equivalent condition $x^{2} + C \,=\, 2x^{2}$, i.e., $x^{2} \,=\, C$.
    That is, seeking a fixed point for this function is the same as seeking the positive square root of the given number~$C$.

\V

        In Theorem~\Ref{ThmC20.70} Heron's Method instructs one to choose an initial approximation $x_{1}\,>\,0$ of $\sqrt{C}$,
    and use it to form the sequence of approximations $(x_{1},x_{2},\,{\ldots}\,)$, where $x_{k+1} \,=\, {\displaystyle \frac{1}{2}\left(x_{k} + \frac{C}{x_{k}}\right)}$, $k \,=\, 1,2,\,{\ldots}\,$.
    In terms of the function $f$ in the preceding example, this recursive formula can be written $x_{k+1} \,=\, f(x_{k})$;
    Theorem~\Ref{ThmC20.70} then guarantees that this sequence converges to a fixed point of $f$.
    The existence of examples such as this have led to an extensive theory of fixed points.
    The next result illustrates that theory.

\V

             \subsection{\small{\bf Theorem} (The Banach Fixed-Point Theorem in ${\RR}$)}
            \label{ThmF35.60}

\V

        Suppose that $f:X \,{\rightarrow}\, X$ is a continuous function defined on a nonempty closed set in ${\RR}$,
    and whose values are also in $X$. Assume in addition that the following condition is satisfied:
        \begin{displaymath}
        \mbox{There is a constant ${\lambda}$, with $0\,\,{\leq}\,\,{\lambda}\,<\,1$, such that
    $|f(y)-f(x)|\,\,{\leq}\,\,{\lambda}|y-x|$ for all $x,y$ in $X$~$({\ast})$}
        \end{displaymath}
\noindent Then $f$ has a unique fixed point $c$ in $X$. More precisely, let $x_{0}$ be any number in $X$,
    and let ${\xi} \,=\, (x_{0},x_{1},x_{2},\,{\ldots}\,)$ be the sequence defined recursively by the rule $x_{k} \,=\, f(x_{k-1})$.
    Then $c \,=\, \lim_{k \,{\rightarrow}\, {\infty}} x_{k}$.

\V

        {\bf Proof} Note that, by Inequality~$({\ast})$, for each $n\,\,{\geq}\,\,1$ one has
        \begin{displaymath}
        |x_{n+1}-x_{n}| \,=\, |f(x_{n})-f(x_{n-1})|\,\,{\leq}\,\,{\lambda}|x_{n}-x_{n-1}|.
        \end{displaymath}
    By repeatedly using this fact one finally gets
        \begin{displaymath}
        |x_{n+1}-x_{n}|\,\,{\leq}\,\,{\lambda}^{n}|x_{1}-x_{0}|.
        \end{displaymath}
    Then for each $k$ in ${\NN}$ one has
        \begin{displaymath}
        |x_{n+k+1} - x_{n}|\,\,{\leq}\,\,|x_{n+k+1} - x_{n+k}| + |x_{n+k} - x_{n+k-1}| + \,{\ldots}\,+ |x_{n+1} - x_{n}|\,\,{\leq}\,\,
    \left({\lambda}^{n+k} + {\lambda}^{n+k-1} + \,{\ldots}\, + {\lambda}^{n}\right)|x_{1}-x_{0}|.
        \end{displaymath}
    That is, by Part~(a) of Theorem~\Ref{ThmB25.80},
        \begin{displaymath}
        |x_{n+k+1} - x_{n}|\,\,{\leq}\,\,\frac{{\lambda}^{n+k+1}}{1-{\lambda}}|x_{1}-x_{0}|.
        \end{displaymath}
    Since $|{\lambda}|\,<\,1$ it follows that $\lim_{n \,{\rightarrow}\, {\infty}} {\lambda}^{n+k+1} \,=\, 0$,
    it follows easily that the sequence ${\xi}$ is a Cauchy sequence, and thus converges to some number $c$.
    Since $x_{n}{\in}X$ for each $n$, and $X$ is closed by hypothesis, it follows that $c$ is also in $X$.
    In addition, since $f$ is continuous, one has $\lim_{n \,{\rightarrow}\, {\infty}} f(x_{n}) \,=\, f(c)$.
    And since ${\xi}$ converges to $c$, it follows that $\lim_{n \,{\rightarrow}\, {\infty}} x_{n+1} \,=\, c$.
    Combining all this with the recursive formula $x_{n+1} \,=\, f(x_{n})$. one gets $c \,=\, f(c)$; that is, $c$ is a fixed point of $f$.

        Now suppose that $c_{1}$ and $c_{2}$ are both fixed points of $f$ in $X$.
   Then
        \begin{displaymath}
        |c_{2}-c_{1}|\,\,{\leq}\,\,|f(c_{2})-f(c_{1})|\,\,{\leq}\,\,{\lambda}|c_{2}-c_{1}|.
        \end{displaymath}
    Since $|{\lambda}|\,<\,1$, it follows that $|c_{2}-c_{1}| \,=\, 0$; that is, $c_{2} \,=\, c_{1}$.
    In other words, the fixed point is unique, as claimed.

\V


             \subsection{\small{\bf Example}}
            \label{ExampF35.70}

\V

        Let $f(x) \,=\, {\displaystyle \frac{1}{2}\left(x+\frac{2}{x}\right)}$, and let $X \,=\, [1,2]$.
    It is easy to show that $f$ maps $X$ into $X$.
    Also, note that $f'(x) \,=\, {\displaystyle \frac{1}{2}\left(1-\frac{2}{x^{2}}\right)}$.
    It is clear that if $x{\in}[1,2]$ then $|f'(x)|\,\,{\leq}\,\,1/2$.
    Then it follows from the Mean-Value Theorem that $|f(y)-f(x)|\,\,{\leq}\,\,\frac{1}{2}|y-x|$ for all $x,y$ in $[1,2]$.
    Then the Banach Fixed-Point Theorem can be used to show that if $x_{0}{\in}[1,2]$ and if $x_{n+1} \,=\, x_{n}$
    for each $n$ in ${\NN}$, then $\lim_{n \,{\rightarrow}\, {\infty}} x_{n} \,=\, \sqrt{2}$. (This is a special case of Heron's Method.)

\V
\V

             \subsection{\small{\bf Theorem} (The Brouwer Fixed-Point Theorem in ${\RR}$)}
            \label{ThmF35.80}

\V

        Suppose that $f:[a,b] \,{\rightarrow}\, [a,b]$ is a continuous function defined on a closed bounded interval $[a,b]$ with values in the same interval.
    Then $f$ has at least one fixed point in $[a,b]$.

\V

        {\bf Proof} Since $f$ maps $[a,b]$ into $[a,b]$, it follows that $a\,\,{\leq}\,\,f(x)\,\,{\leq}\,\,b$ for all $x$ in $[a,b]$.
    Thus one also has $a-x\,\,{\leq}\,\,f(x)-x\,\,{\leq}\,\,b-x$ for all such $x$.
    In particular, $0\,\,{\leq}\,\,f(a)-a\,\,{\leq}\,\,b-a$ and $a-b\,\,{\leq}\,\,f(b)-b\,\,{\leq}\,\,0$.
    In particular, let $M$ be the maximum value of $f(x)-x$ for $x$ in $[a,b]$ and let $m$ be the corresponding minimum value.
    Then $m\,\,{\leq}\,\,0$ and $M\,\,{\geq}\,\,0$, so by the Intermediate-Value Theorem for Continuous Functions,
    there must exist a number $c$ in $[a,b]$ such that $f(c)-c \,=\, 0$.
    This $c$ is a fixed point of $f$ in $[a,b]$.

\V
\V


             \subsection{\small{\bf Remarks}}
            \label{ThmF35.90}

\V

\hspace*{\parindent}(1) Both the Banach Theorem and the Brouwer Theorem described here are very special cases of the full theorems that bear the same names.

\V

        (2) A function which satisfies Condition~$({\ast})$ in Theorem~\Ref{ThmF35.60} is called a {\bf contraction mapping on $X$}.
    For that reason, the Banach theorem is often called the {\bf Contraction-Mapping Theorem} (or the {\bf Contraction-Mapping Principle}).
    

                \section{{\bf Discontinuities of Functions}}
                \label{SectF40}

\V
\V

        In this section we complement our previous study of continuous functions with the consideration of functions which {\em fail} to be continuous at some points of their domains.
    The discussion begins by providing a measure of `how badly' a function is discontinuous at a point.


\V


             \subsection{\small{\bf Definition} (Oscillation over a Set)}
            \label{DefF40.20}

\V

        Let $f:S \,{\rightarrow}\, {\RR}$ be a {\em bounded} function defined on a nonempty set $S$ of ${\RR}$.
    The {\bf oscillation of $f$ over $S$} is the number ${\Omega}_{S}(f)$ given by the formula
        \begin{equation}
        \label{EqnF.60}
        {\Omega}_{S}(f) \,=\, {\sup}\,\{|f(y)-f(x)|: x,y{\in}S\}.
        \end{equation}
    In the terminology of Definition~\Ref{DefB30.190}, ${\Omega}_{S}(f)$ is the {\bf diameter of the image set $f[S]$}.

\V

        \underline{Remark} Let $f$ and $S$ be as above, and $x$ be a point of the set $S$.
    Define ${\varphi}_{S,f,x}:(0,+{\infty}) \,{\rightarrow}\, {\RR}$ by the rule
        \begin{displaymath}
        {\varphi}_{S,f,x}(r) \,=\, {\Omega}_{S\,{\cap}\,(x-r,x+r)} (f)
        \end{displaymath}
    It follows from Theorem~\Ref{ThmB30.200} that ${\varphi}_{S,f,x}(r)\,\,{\geq}\,\,0$ for all $r\,>\,0$, and that as $r$ decreases so does ${\varphi}_{S,f,x}(r)$.
    In particular, $\lim_{r{\searrow}0} {\varphi}_{S,f,x}(r)$ exists and is a nonnegative real number.

        Note: Usually the set $S$, the function $f$ and the point $x$ under discussion are clear from the context.
    In such a case we normally write ${\varphi}(r)$ instead of the more precise ${\varphi}_{S,f,x}(r)$.

\V

             \subsection{\small{\bf Definition} (Oscillation at a Point)}
            \label{DefF40.30}

        Let $f$, $S$, $x$ and ${\varphi}$ be as in the preceding `Remark'.
    Then the {\bf oscillation of $f$ at $x$ with respect to $S$} is the number ${\omega}_{S}(f;x)$ given by
        \begin{displaymath}
        {\omega}_{S}(f;x) \,=\, \lim_{r{\searrow}0} {\varphi}(r).
        \end{displaymath}
    That is,
        \begin{equation}
        \label{EqnF.70}
        {\omega}_{S}(f;x) \,=\, \lim_{r{\searrow}0} {\Omega}_{S\,{\cap}\,(x-r,x+r)}(f).
        \end{equation}
    If the set $S$ is clear from the context -- for example, if $S$ is the full domain of $f$ --
    one sometimes abbreviates the notation to ${\omega}(f;x)$;
    and to emphasize that this quantity can be viewed as a `function of $x$ on $S$', it is often written ${\omega}_{f}(x)$.

\V

             \subsection{\small{\bf Theorem}}
            \label{ThmF40.40}

        Suppose that $f:S \,{\rightarrow}\, {\RR}$ is a bounded function whose domain is the nonempty set $S$.
    Let $c$ be a point of $S$.
    Then a necessary and sufficient condition for $f$ to be continuous at $c$ is that ${\omega}_{S}(f;c) \,=\, 0$.

\V

        \underline{Proof} Suppose that ${\omega}_{S}(f;c) \,=\, 0$.
    Let $r\,>\,0$ be given.
    By definition of ${\varphi}$, if $x$ in $S$ satisfies $|x-c|\,<\,r$, then one has
        \begin{displaymath}
        |f(x)-f(c)|\,\,{\leq}\,\,{\sup}\,\{|f(y)-f(z)|: y,z{\in}S\,{\cap}\,(c-r,c+r)\} \,=\, {\varphi}(r) \h ({\ast})
        \end{displaymath}
    Now let ${\varepsilon}\,>\,0$  be given. By the hypothesis that ${\omega}_{S}(f;c) \,=\, 0$,
    it follows that there exists ${\delta}\,>\,0$ such that if $0\,<\,r\,<\,{\delta}$ then $0\,\,{\leq}\,\,{\varphi}(r)\,<\,{\varepsilon}$.
    For such ${\delta}$ one then gets, using Inequality~$({\ast})$ above, $|f(x)-f(c)|\,<\,{\varepsilon}$ when $x{\in}S$ and $|x-c|\,<\,{\delta}$.
    Thus $f$ is continuous at $c$, as claimed.

        Conversely, suppose that $f$ is continuous at $c$.
    Note that if $x$ and $y$ are in $S$, then
        \begin{displaymath}
        |f(y)-f(x)| \,=\, |f(y)-f(c)+f(c)-f(x)|\,\,{\leq}\,\,|f(y)-f(c)|+|f(c)-f(x)| \h ({\ast}{\ast})
        \end{displaymath}
    Now let ${\varepsilon}\,>\,0$ be given, and let ${\delta}\,>\,0$ be small enough that if $z{\in}S$ and $|z-c|\,<\,{\delta}$,
    then $|f(z)-f(c)|\,<\,{\varepsilon}/4$.
    By Inequality~$({\ast}{\ast})$ above it follows that if $0\,<\,r\,<\,{\delta}$ and $x,y{\in}S\,{\cap}\,(c-r,c+r)$, then
        \begin{displaymath}
        |f(y)-f(x)|\,\,{\leq}\,\,\frac{{\varepsilon}}{4}+\frac{{\varepsilon}}{4} \,=\, \frac{{\varepsilon}}{2}.
        \end{displaymath}
    Since this inequality holds for {\em all} $x$ and $y$ in $S\,{\cap}\,(c-r,c+r)$, it follows that the supremum of the set of all such numbers $|f(x)-f(y)|$ is also no bigger than ${\varepsilon}/2$.
    That is, if $0\,<\,r\,<\,{\delta}$ then
        \begin{displaymath}
        {\varphi}(r)\,\,{\leq}\,\,\frac{{\varepsilon}}{2}\,<\,{\varepsilon}.
        \end{displaymath}
    It follows that $\lim_{r{\searrow}0} {\varphi}(r) \,=\, 0$; that is, ${\omega}_{S}(f;c) \,=\, 0$,  as claimed.

\V
\V

        In light of the preceding theorem, it is natural to think of the quantity ${\omega}_{S}(f;c)$ as providing an informal `measure' of the `degree of discontinuity of $f$ at $c$':
    the bigger this quantity, the `more discontinuous at $c$' the function $f$ is.


\V
\V

        Because of Theorem~\Ref{ThmC90.90}, the discontinuities of a monotonic function defined on an interval $I$ are easy to classify;
    but first some more terminology.


\V
\V

             \subsection{\small{\bf Definition}}
            \label{DefF40.60}

        Suppose that $f:I \,{\rightarrow}\, {\RR}$ is defined on an interval $I$ in ${\RR}$, and that $c$ is an interior point of $I$.
    Assume that the one-sided limits $\lim_{x{\nearrow}c} f(x)$ and $\lim_{x{\searrow}c} f(x)$ both exist and are finite.
    In light of Remark~\Ref{RemrkC90.12}~(2), this can be stated, `suppose that $f(c-)$ and $f(c+)$ exist'.

\V

        (1) If $f(c-) \,\,{\neq}\,\, f(c+)$ then one says that $f$ has a {\bf jump discontinuity at $c$}, or that $f$ has a {\bf discontinuity of Type~1 at $c$}.
    One refers to the quantity $f(c+)-f(c-)$ as the {\bf jump of $f$ at $c$}.

\V

        (2) If $f(c-) \,=\, f(c+)$ but this common value does not equal $f(c)$, then one says that $f$ has a {\bf removable discontinuity at $c$}.

\V

        (3) Similar terminology is used if $c{\in}I$ but $c$ is an endpoint of $I$.
    Thus, if $c$ is the left endpoint of $I$ and $f(c+)$ exists, one says that $f$ has a jump discontinuity at $c$ provided $f(c+) \,\,{\neq}\,\, f(c)$;
    one then calls the quantity $f(c+)-f(c)$ the jump of $f$ at $c$.
    Likewise, if $c$ is the right endpoint of $I$ and $f(c-)$ exists, one says that $f$ has a jump discontinuity at $c$ provided $f(c-) \,\,{\neq}\,\, f(c)$, and one then calls the difference $f(c)-f(c-)$ the jump of $f$ at $c$.

\V

        (4) If $f(c-)$ and $f(c)$ both exist and are finite, then one also calls the difference $f(c)-f(c-)$ the {\bf left-hand jump of $f$ at $c$}.
    One sometimes denotes this jump by ${\Delta}_{-}f(c)$.
    Likewise, if $f(c+)$ and $f(c)$ both exist and are finite, then one calls $f(c+)-f(c)$ the {\bf right-hand jump of $f$ at $c$}, denoted ${\Delta}_{+} f(c)$.


        \underline{Remarks}

\V

        (1) Some texts use a slightly different definition of `jump discontinuity',
    in that they allow the possibility that $f(c-) \,=\, f(c+)$ as long as $f(c)$ takes on a different value.

\V

        (2) The reason for the name `removable discontinuity' is because by merely redefining the value of $f$ at $c$ to be the common value $f(c-) \,=\, f(c+)$, one can make $f$ continuous at $c$.

\V
\V

             \subsection{\small{\bf Theorem}}
            \label{ThmF40.70}

        Suppose that $f:[a,b] \,{\rightarrow}\, {\RR}$ is monotonic up on the closed bounded interval $[a,b]$.
    Then:

\V

        (a) Every discontinuity of $f$ on $[a,b]$ is a jump discontinuity, and the jump of $f$ at each such point is a positive number.

\V

        (b) Let $A$ denote the set of discontinuities of $f$ on $[a,b]$. If $x_{1}$, $x_{2}$,\,{\ldots}\,$x_{k}$ is any finite (nonempty) collection of points in $A$, with $a\,\,{\leq}\,\,x_{1}\,<\,x_{2}\,<\,\,{\ldots}\,\,<\,x_{k}\,\,{\leq}\,\,b$, then the sum of the jumps of $f$ at these points is at most $f(b)-f(a)$.

\V

        (c) The set of discontinuities of $f$ on $[a,b]$ is countable.

\V

        (d) A similar result holds for any function $g:[a,b] \,{\rightarrow}\, {\RR}$ which is monotonic down on $[a,b]$,
    except that in this case the jumps are all negative numbers, and the sum of the jumps of $g$ at any finite set of discontinuities is bounded below by $g(b)-g(a)$.

\V

        \underline{Proof}

\V

        (a) This is essentially the content of Theorem~\Ref{ThmC90.90}.

\V

        (b) Suppose, to be definite, that $a\,<\,x_{1}$ and $x_{k}\,<\,b$. Note that the sum $S$ of the jumps at the points $x_{1}$,\,{\ldots}\,$x_{k}$ is given by
        \begin{displaymath}
        S \,=\, (f(x_{1}+)-f(x_{1}-)) + (f(x_{2}+)-f(x_{2}-)) + \,{\ldots}\, +
    (f(x_{k-1}+)-f(x_{k-1}-)) + (f(x_{k}+)-f(x_{k}-))
        \end{displaymath}
    Form the following `collapsing sum'  $f(b)-f(a) \,=\, $
        \begin{displaymath}
        f(b) + (f(x_{k}+)-f(x_{k}+)) + (f(x_{k}-)-f(x_{k}-)) +
    (f(x_{k-1}+)-f(x_{k-1}+)) + (f(x_{k-1}-)-f(x_{k-1}-)) + \,{\ldots}\, +
        \end{displaymath}
        \begin{displaymath}
    + \,{\ldots}\, +(f(x_{1}+)-f(x_{1}+)) + (f(x_{1}-)-f(x_{1}-)) - f(a).
        \end{displaymath}
    After using the Generalized Associative and Commutative Laws for Addition to rearrange terms, one can rewite the preceding equation as 
        \begin{displaymath}
    f(b)-f(a) \,=\, (f(b)-f(x_{k}+)) + (f(x_{k}+)-f(x_{k}-)) + (f(x_{k}-)-f(x_{k-1}+)) +  +\,{\ldots}\, 
        \end{displaymath}
        \begin{displaymath}
      + \,{\ldots}\, +(f(x_{2}+)-f(x_{2}-)) + (f(x_{2}-)-f(x_{1}+)) + (f(x_{1}+)-f(x_{1}-)) + (f(x_{1}-)-f(a)) \h ({\ast})
        \end{displaymath}
    Note that the right side of this last equation consists of two types of terms:

\V

        \h (i)\, The jumps of $f$; that is, the terms of the form $f(x_{j}+)-f(x_{j}-)$ for $j \,=\, 1,2,\,{\ldots}\,k$.

        \h (ii) The `nonjump' terms; that is, the terms of the form $f(x_{j}-)-f(x_{j-1}+)$ for $j \,=\, 2,3,\,{\ldots}\,k$,
    as well as the two `endpoint' terms $f(b)-f(x_{k}+)$ and $f(x_{1}-)-f(a)$.

\V

\noindent It is clear from the hypothesis that $f$ is monotonic up, combined with the fact that $x_{j-1}\,<\,x_{j}$ for each $j$,
    that all the terms of Type~(ii) are nonnegative.
    Thus removing them from the right side of $({\ast})$ produces a smaller quantity on the right. More precisely, one gets
        \begin{displaymath}
        f(b)-f(a)\,\,{\geq}\,\,f(x_{k}+)-f(x_{k}-) + (f(x_{k-1}+)-f(x_{k-1}-)) + \,{\ldots}\,+ (f(x_{1}+)-f(x_{1}-)).
        \end{displaymath}
    That is, $f(b)-f(a)$ is at least as large as the sum of the jumps of $f$ at $x_{1}$, $x_{2}$,\,{\ldots}\,$x_{k}$, as claimed.

        The reader is encouraged to made the minor changes needed in the preceding argument to handle the case in which $f$ has a jump at either of the endpoints $a$ or $b$.

\V

        (c) For each positive integer $m$ let $A_{m}$ denote the set of all discontinuities $x$ of $f$ on $[a,b]$ such that the jump of $f$ at $x$ is at least $1/m$.
    It follows by Part~(b) that the set $A_{m}$ cannot have $N$ elements if $N$ is a positive integer such that $N\,>\,m(f(b)-f(a))$.
    Indeed, the sum of $N$ jumps, each at least $1/m$ in size, must be at least $N/m$, and if $N\,>\,m(f(b)-f(a))$, the sum of these jumps would have to exceed $f(b)-f(a)$, contrary to the conclusions of Part~(b).

    Finally, note that by Part~(a), if $x$ is a discontinuity of $f$ on $[a,b]$ then the jump of $f$ at $x$ is positive. In particular, there must exist a positive integer $m$ such that this jump is at least as large as $1/m$.
    In other words, the set $D$ of all discontinuities of $f$ on $[a,b]$ is the union of the countable family of sets $A_{1}$, $A_{2}$,\,{\ldots}\,, and each set in this family is countable (in fact, finite).
    Now Theorem~\Ref{ThmA20.70} implies that the union $D$ is itself a countable set, as claimed.

\V

        (d) The case of a function $g$ which is monotonic down on $[a,b]$ can be reduced to the monotonic-up case just discussed:
    look at the function $f \,=\, -g$, which is monotonic up on $[a,b]$.
    As is usual in such reductions, the details are left as an exercise.

\V
\V


        The preceding theorem tells us, in effect, that if $f:[a,b] \,{\rightarrow}\, {\RR}$ is monotonic, then its discontinuities cannot be overly `wild'.
    Indeed, the set of points at which the function $f$ {\em is} continuous is an uncountable subset of $[a,b]$,
    since one obtains this set by removing the (at most countably many) discontinuities of $f$ from $[a,b]$;
     see Theorem~\Ref{ThmA20.112}.
    Moreover, the discontinuities themselves cannot involve `wild' fluctuations of the values of $f$, since they are all simple jump discontinuities.

        It is also clear that any function formed, by any reasonable algebraic method, from a finite number of functions that are monotonic on $[a,b]$ need not be monotonic, yet also cannot be too wild.
    For instance, if $f_{1}$, $f_{2}$,\,{\ldots}\,$f_{k}$ are monotonic on $[a,b]$ and $c_{1}$, $c_{2}$,\,{\ldots}\,$c_{k}$ are constants,
    then the linear combination $g \,=\, c_{1}f_{1} +\,{\ldots}\,+c_{k}f_{k}$ has at most a countable number of discontinuities, and each of these discontinuities is,
    at worst, either a jump discontinuity or a removable discontinuity. Similarly, the product of finitely many monotonics,
    while not necessarily monotonic itself, has at worst a countable number of either jump or removable discontinuities.

        With all this in mind, it makes sense to consider the following class of functions.

\V
\V

             \subsection{\small{\bf Definition}}
            \label{DefF40.80}

        Let $I$ be an interval in ${\RR}$.
    A function $f:I \,{\rightarrow}\, {\RR}$ is said to be of {\bf hybrid type on the interval $I$} provided there exist functions $g:I\,\,{\geq}\,\,{\RR}$ and $h:I \,{\rightarrow}\, {\RR}$,
    both being monotonic {\em up} on $I$, such that $f(x) \,=\, g(x)-h(x)$ for all $x$ in $I$.

        The set of all functions $f$, with domain $I$, which are of hybrid type on $I$ is denoted ${\cal H}_{I}$.


\V

        \underline{Remarks}

\V

        (1) Since $-h$ is monotonic down when $h$ is monotonic up, it would have been possible to phrase the definition as follows:

        `The function $f$ is of hybrid type if it can be expressed as the {\em sum} of a monotonic up function with a monotonic down function on $I$.'

\noindent Experience shows, however, that having $g$ and $h$ be of the same type of monotonicity,
    and using the minus sign, ultimately causes less confusion than omitting the minus sign but having $g$ and $h$ of opposite monotonicity.
    More importantly, expressing a function as the difference of monotonic-up functions is standard in the literature;
    while expressing them as the sum of a monotonic-up and a monotonic-down is not standard.

\V

        (2) The terminology of a `hybrid type' of function, although quite reasonable, is not standard.
    We switch to the standard terminology later, when it makes more sense.


\V
\V

             \subsection{\small{\bf Examples}}
            \label{ExampF40.90}

\V

\hspace*{\parindent}(1) Let $g(x) \,=\, 3x^{2}$ and $h(x) \,=\, 2x^{3}$.
    Then both functions are monotonic on the interval $I \,=\, [0,+{\infty})$.
    Now let $f:[0,+{\infty}) \,{\rightarrow}\, {\RR}$ be the corresponding hybrid function,
    given by $f(x) \,=\, g(x)-h(x) \,=\, 3x^{2}-2x^{3}$ for all $x\,\,{\geq}\,\,0$.
    By using elementary calculus one easily sees that $f$ is increasing on the interval $[0,1]$ and decreasing on the interval $[1,+{\infty})$; in other words, $f$ is a true `hybrid', in the sense of being neither monotonic up throughout the entire interval $[0,+{\infty})$ nor monotonic down throughout.

\V

        (2) Suppose that $f:[0,+{\infty}) \,{\rightarrow}\, {\RR}$ is a function with the following property:
    there exists a number $c\,>\,0$ such that $f$ is monotonic up on the subinterval $[0,c]$ and monotonic down on $[c,+{\infty})$.
    Then $f$ is of hybrid type on $[0,+{\infty})$.
    Indeed, one can choose $g$ and $h$ so that $h$ is constant on $[0,c]$ and $g$ is constant on $[c,+{\infty})$.
    More precisely, define $g$ and $h$ as follows:
        \begin{displaymath}
        g(t) \,=\, 
                    \left\{
        \begin{array}{ll}
        f(t) & \mbox{for $0\,\,{\leq}\,\,t\,\,{\leq}\,\,c$} \\
        f(c) & \mbox{for $c\,<\,t\,<\,+{\infty}$}
        \end{array}
                    \right.
        \end{displaymath}
    Likewise,
        \begin{displaymath}
        h(t) \,=\, 
                    \left\{
        \begin{array}{cl}
         0 & \mbox{for $0\,\,{\leq}\,\,t\,\,{\leq}\,\,c$} \\
        f(c)-f(t) & \mbox{for $c\,<\,t\,<\,+{\infty}$}
        \end{array}
                    \right.
        \end{displaymath}
    It is clear that $f(t) \,=\, g(t)-h(t)$ for {\em all} $t$ in $[0,+{\infty})$, and that $g$ and $h$ are both monotonic up on $[0,+{\infty})$.
    (Don't be fooled by the minus sign in the expression $f(c)-f(t)$ which is used for $h$ on $[1,+{\infty})$.
    On that portion of the domain the function $f$ is monotonic {\em down}, hence $f(c)-f(t)$ is monotonic {\em up} on the same portion.)
    Thus, the function $f$ is of hybrid type.

        \underline{Remarks on This Example}:

        \h (i)\, The method illustrated here generalizes to any function which alternates between intervals on which it is monotonic up and intervals on which it is monotonic down;
    think `sine' and `cosine'.
    The idea in the general case is quite similar to what appears here, but the detailed calculations can be a bit messy.
    The general case is left as an exercise.

       \h (ii) There is a simple `physical' interpretation of the function $f$ in this example.
    Namely, think of the input $t$ for the quantity $f(t)$ as the time, and the corresponding output $x \,=\, f(t)$ as the location at time $t$ of an object moving along the $x$-axis.
    Then the function $g$ provides a record of of the {\em forward} motion of the object; that is, motion to the right along the $x$-axis.
    Indeed, for any time interval $[t_{1},t_{2}]$,
    with $0\,\,{\leq}\,\,t_{1}\,<\,t_{2}$, the difference $g(t_{2})-g(t_{1})$ is the total distance the object moved to the right during that time interval.
    Likewise, $h(t_{2})-h(t_{1})$ is the total distance the object moved to the left during that time interval.
    Thus $f(t_{2}) - f(t_{1})  \,=\, (g(t_{2}) - g(t_{1})) - (h(t_{2}) - h(t_{1}))$ is the {\em net} distance traveled by the object during the given time interval.

\V

        (3) Let $f:[0,+{\infty}) \,{\rightarrow}\, {\RR}$ be the function discussed above in Example~(1).
    From the results of that example, it is clear that the method used in Example~(2), with $c \,=\, 1$, applies here as well.
    That method then yields the expression $f(x) \,=\, g(x)-h(x)$, where
        \begin{displaymath}
        g(x) \,=\, 
                    \left\{
        \begin{array}{cl}
        3x^{2}-2x^{3} & \mbox{for $0\,\,{\leq}\,\,x\,\,{\leq}\,\,1$} \\
        1 & \mbox{for $1\,<\,x\,<\,+{\infty}$}
        \end{array}
                    \right.
        \end{displaymath}
    and
        \begin{displaymath}
        h(x) \,=\, 
                    \left\{
        \begin{array}{cl}
         0 & \mbox{for $0\,\,{\leq}\,\,x\,\,{\leq}\,\,1$} \\
        1-(3x^{2}-2x^{3}) & \mbox{for $1\,<\,x\,<\,+{\infty}$}
        \end{array}
                    \right.
        \end{displaymath}

\V

        (4) The Dirichlet function (see Example~\Ref{ExampD20.53}~(1)) is {\em not} of hybrid type.
    Indeed, it has uncountably many discontinuities, whereas a function of hybrid type can have at most countably many such points.

\V

        (5) Suppose that $f:I \,{\rightarrow}\, {\RR}$ is a function with bounded derivative on an interval $I$.
    Then $f$ is a hybrid function in $I$. Indeed, let $M$ be an upper bound for $|f'|$ on $I$, and let $c$ be a point of $I$. Define $g,h:I \,{\rightarrow}\, {\RR}$ by
        \begin{displaymath}
        g(x) \,=\, f(c) + M(x-c) \mbox{ and } h(x) \,=\, \left(D^{-1}_{c} (M-f')\right)(x) \mbox{ for all $x$ in $I$}.
        \end{displaymath}
    Clearly $g'(x) \,=\, M \,=\, 0$ for all $x$ in $I$, and $h'(x) \,=\, M-f'(x) \,=\, 0$ for all $x$ in $I$.
    Thus $g$ and $h$ are certainly monotonic up on $I$.
    It is also easy to verify that $f(x) \,=\, g(x) - h(x)$ for all $x$ in $I$. It now follows that $f$ is of hybrid type on $I$.

\V
\V


\StartSkip{
        Functions which are hybrid on a given closed bounded interval play an especially important role in the theory.
    The next result shows that such functions form a rather substantial class.

\V

             \subsection{\small{\bf Theorem}}
            \label{ThmF40.100}

        Let $[a,b]$ be a closed bounded interval in ${\RR}$; and, as in Definition~\Ref{DefF40.80},
    let ${\cal H}_{[a,b]}$ be the set of all functions $:[a,b] \,{\rightarrow}\, {\RR}$, with domain $[a,b]$, such that $f$ is of hybrid type on $[a,b]$.
    Then:

\V

        (a) If $f{\in}{\cal H}_{[a,b]}$, then $f$ is bounded on $[a,b]$.
    In addition, $f$ has at most countably many discontinuities, and each of them is either a jump discontinuity or a removable discontinuity.

\V

        (b) The set ${\cal H}_{[a,b]}$ is closed under addition, substraction and multiplication.
    That is, if $f_{1}$ and $f_{2}$ are in ${\cal H}_{[a,b]}$, then so are $f_{1}+f_{2}$, $f_{1}-f_{2}$ and $f_{1}{\cdot}f_{2}$.


\V

        \underline{Proof}

\V

        (a) Note that if $f{\in}{\cal H}_{[a,b]}$ then, by definition, one has $f \,=\, g-h$ on $[a,b]$ for some monotonic-up functions $g$ and $h$.
    Since $g(a)\,\,{\leq}\,\,g(x)\,\,{\leq}\,\,g(b)$ and $h(a)\,\,{\leq}\,\,h(x)\,\,{\leq}\,\,h(b)$ for all $x$ in $[a,b]$,
    it follows that
        \begin{displaymath}
        g(a)-h(b)\,\,{\leq}\,\,g(x)-h(x)\,\,{\leq}\,\,g(b)-h(a)
        \end{displaymath}
    for all $x$ in $[a,b]$.
    That is, $g(a)-h(b)\,\,{\leq}\,\,f(x)\,\,{\leq}\,\,g(b)-h(a)$ for all $x$ in $[a,b]$.
    Thus, $f$ is bounded on $[a,b]$, as claimed.

        As for the discontinuities of $f$, note that if $x$ is a point of $[a,b]$ at which both $g$ and $h$ is continuous,
    then $f$ is continuous at $x$ as well, by Theorem~\Ref{ThmD20.60B}.
    Thus, the only points at which $f$ can be discontinuous must lie in the union of the set of points at which $g$ is discontinuous with the set of points at which $h$ is discontinuous.
    These two sets are themselves countable, by the results of Theorem~\Ref{ThmF40.70}; hence so is any subset of their union, by Theorem~\Ref{ThmA20.50}.

        Finally, let $c$ be a point of discontinuity of $f$ in $[a,b]$.
    Assume, for simplicity, that $a\,<\,c\,<\,b$; a similar proof works if $c$ is one of the endpoints $a$ or $b$.
    By the usual rules for limits one has
        \begin{displaymath}
        \lim_{x{\nearrow}c} f(x) \,=\, \lim_{x{\nearrow}c} g(x) - \lim_{x{\nearrow}c} h(x);
        \end{displaymath}
    the one-sided limits on the right side of this equation exist because of the properties of monotonic functions.
    Likewise, one has
        \begin{displaymath}
        \lim_{x{\searrow}c} f(x) \,=\, \lim_{x{\searrow}c} g(x) - \lim_{x{\searrow}c} h(x).
        \end{displaymath}
    That is, the quantities $f(c-)$ and $f(c+)$ exist, and one has $f(c-) \,=\, g(c-)-h(c-)$ and $f(c+) \,=\, g(c+)-g(c-)$.
    This implies that at such a point either the function $f$ has a jump discontinuity (if $f(c-) \,\,{\neq}\,\, f(c+)$),
    or $f$ has a removable discontinuity (if $f(c-) \,=\, f(c+)$).
    The latter possibility {\em can} occur for $f$, even though neither of the monotonic functions $g$ or $h$ has a removable discontinuity.
    Indeed, note that
        \begin{displaymath}
        f(c+)-f(c-) \,=\, (g(c+)-h(c+)) - (g(c-) - h(c-)) \,=\, (g(c+)-g(c-)) - (h(c+)-h(c-)).
        \end{displaymath}
    That is, if the jumps of $g$ and $h$ at $c$ are equal, then the jump of $f$ at $c$ will be $0$.

\V

        (b) The fact that ${\cal H}_{[a,b]}$ is closed under addition and subtraction is obvious.
    Indeed, suppose that $f_{1}$ and $f_{2}$ are elements of ${\cal H}_{[a,b]}$.
    Then there exist monotonic-up functions $g_{1},g_{2},h_{1}h_{2}:[a,b] \,{\rightarrow}\, {\RR}$ such that
        \begin{displaymath}
        f_{1}(x) \,=\, g_{1}(x)-h_{1}(x) \mbox{ and }
        f_{2}(x) \,=\, g_{2}(x)-h_{2}(x) \mbox{ for all $x$ in $[a,b]$}.
        \end{displaymath}
    Thus one has
        \begin{displaymath}
        f_{1}+f_{2} \,=\, (g_{1}-h_{1}) + (g_{2}-h_{2})
     \,=\, (g_{1} + g_{2}) - (h_{1} + h_{2}).
        \end{displaymath}
    Since $g_{1}+g_{2}$ and $h_{1}+h_{2}$ are also monotonic up on $[a,b]$,
    it follows that $(f_{1}+f_{2}){\in}{\cal H}_{[a,b]}$, as claimmed.
    A similar proof works to show that $(f_{1}-f_{2}){\in}{\cal H}_{[a,b]}$.

        The proof for multiplication is not quite so easy. Indeed, it is {\em not} the case that the product of monotonic functions is also monotonic.
    Consider, for example, the functions $g_{1}(x) \,=\, x$ and $g_{2}(x) \,=\, x^{3}$ for all $x$ in ${\RR}$; in particular, they are monotonic up on the interval $[-1,1]$.
    It is also clear that $g_{1}{\cdot}g_{2}$ is the function whose value at $x$ is $x^{4}$.
    This function is decreasing on $(-{\infty},0]$ and increasing on $[0,+{\infty})$.
    Thus on any interval containing $0$ in its interior -- in particular, on the interval $[-1,1]$ -- it fails to  be monotonic over the entire interval.
    The  problem, of course, is the fact that the values of the functions $g_{1}$ and $g_{2}$ change sign on the interval $[-1,1]$.

        The way around this is to recall that the functions $g_{1},g_{2}, h_{1},h_{2}$ are bounded on $[a.b]$.
    This, let $C\,>\,0$ be a constant large enough that the functions $G_{1} \,=\, C+g_{1}$, $G_{2} \,=\, C+g_{2}$, $H_{1} \,=\, C+h_{1}$ and $H_{2} \,=\, C+h_{2}$ are all positive throughout the interval $[a,b]$.
    Then note that
        \begin{displaymath}
        f_{1}{\cdot}f_{2} \,=\, (g_{1}-h_{1}){\cdot}(g_{2}-h_{2}) \,=\, 
    ((C+g_{1})-(C+h_{1})){\cdot}((C+g_{2})-(C+h_{2})) \,=\, 
        \end{displaymath}
        \begin{displaymath}
    (G_{1}-H_{1}){\cdot}(G_{2}-H_{2}) \,=\, (G_{1}G_{2} + H_{1}H_{2}) - (G_{1}H_{2} + G_{2}H_{1}).
        \end{displaymath}
    It is a simple exercise (left to the reader, of course) to verify that the product of {\em positive} monotonic-up functions is monotonic up.
    The desired result now follows easily.
}%\EndSkip

\V
\V

\begin{quotation}
{\footnotesize \underline{Historical Note}
        The theory of hybrid functions was developed by Camille Jordan in an 1881 paper.
    It grew out of a theorem of Dirichlet which plays an important role in the development of the theory of Fourier series.

        More specifically, Dirichlet showed that if $g:[0,{\delta}]  \,{\rightarrow}\, {\RR}$ is continuous on an interval $[0,{\delta}]$, and if $g$ is piecewise monotonic on $[0,{\delta}]$ (see Definition~\Ref{DefD20.58A}~(4)), then
        \begin{displaymath}
        \lim_{p \,{\rightarrow}\, +{\infty}} \frac{2}{{\pi}} \int_{0}^{{\delta}} g(t)\,\frac{{\sin}\,(pt)}{t} \,dt \,=\, g(0+)
        \end{displaymath}
    Dirichlet remarked that the `piecewise monotonicity' condition could be weakened and still yield this equation.
    Jordan showed, in fact, that $g$ being a hybrid function on $[0,{\delta}]$ would suffice.
}%EndFootNoteSize
\end{quotation}

\V
\V

             \subsection{\small{\bf Remarks}}
            \label{RemrkF40.100}

\V

\hspace*{\parindent}(1) If $f:I \,{\rightarrow}\, {\RR}$ can be expressed on $I$ as the difference of monotonic functions,
    this expression is not unique.
    Indeed, note that if $f \,=\, g-h$, where $g$ and $h$ are monotonic on $I$, then $f \,=\, ({\psi}+g)-({\psi}+h)$, where ${\psi}:I \,{\rightarrow}\, {\RR}$ is any function that is monotonic up on $I$;
    clearly ${\psi}+g$ and ${\psi}+h$ are also monotonic up on $I$.

\V

        (2) The ambiguity in the choice of $g$ and $h$ in the decomposition $f \,=\, g-h$ can be reduced somewhat.
    Indeed, choose a point $a$ in $I$, and note that if $f:I \,{\rightarrow}\, {\RR}$ is a hybrid function then it is possible to find monotonic functions $\tilde{g}:I \,{\rightarrow}\, {\RR}$ and $\tilde{h}:I \,{\rightarrow}\, {\RR}$ such that $f(x) \,=\, f(a)+\hat{g}(x) - \tilde{h}(x)$ for all $x$ in $I$, and $\tilde{g}(a) \,=\, \tilde{h}(a) \,=\, 0$.
    Indeed, express $f$ as $f \,=\, g-h$, where $g$ and $h$ are monotonic up on $I$, and then set $\tilde{g}(x) \,=\, g(x)-g(a)$ and $\tilde{h}(x) \,=\, h(x)-h(a)$ for all $x$ in $I$.
    Then
        \begin{displaymath}
        f(x) \,=\, f(a)+\tilde{g}(x) - \tilde{h}(x).
        \end{displaymath}
\noindent We shall say that such a decomposition of $f$ is {\bf based at the number $a$}.

        \underline{Note}: To simplify the notation later on, we shall normally transpose the term $f(a)$ to the left side and express the desired condition as
        \begin{displaymath}
        f(x)-f(a) \,=\, \tilde{g}(x) - \tilde{h}(x).
        \end{displaymath}

\V

        (3) In light of the preceding remarks, it is natural to ask whether every decomposition $f(x)-f(a) \,=\, \tilde{g}(x) - \tilde{h}(x)$ of the type considered in Remark~(2) arises, from some fixed decomposition $f - f(a) \,=\, g_{a}-h_{a}$, by adding a suitable monotonic-up function ${\psi}$ as in Remark~(1).
    More precisely, do there exist monotonic-up functions $g_{a},h_{a}:I \,{\rightarrow}\, {\RR}$, satisfying $g_{a}(a) \,=\, h_{a}(a) \,=\, 0$
    and $f-f(a) \,=\, g_{a}-h_{a}$,
    such that if $\tilde{g}$ and $\tilde{h}$ are as in Remark~(2), then there exists monotonic-up ${\psi}:I \,{\rightarrow}\, {\RR}$ such that $\tilde{g} \,=\, {\psi}+g_{a}$ and $\tilde{h} \,=\, {\psi}+h_{a}$?
    If such $g_{a}$ and $h_{a}$ exist, it is easy to see that ${\psi}(a) \,=\, 0$. Since ${\psi}$ is monotonic up on $I$, one must then have, for each $x$ in $I$,
        \begin{displaymath}
        {\psi}(x)\,\,{\geq}\,\,0 \mbox{ if $x\,\,{\geq}\,\,a$} \mbox{ and }
        {\psi}(x)\,\,{\leq}\,\,0 \mbox{ if $x\,\,{\leq}\,\,a$}.
        \end{displaymath}
    Since $\tilde{g}(x) \,=\, {\psi}(x)+g_{a}(x)$ for all $x$ in $I$, it follows that
        \begin{displaymath}
        \tilde{g}(x)\,\,{\geq}\,\,g_{a}(x) \mbox{ if $x\,\,{\geq}\,\,a$} \mbox{ and }
        \tilde{g}(x)\,\,{\leq}\,\,g_{a}(x) \mbox{ if $x\,\,{\leq}\,\,a$}.
        \end{displaymath}
    These inequalities provide some information about the nature of the desired functions $g_{a}$ and $h_{a}$, assuming that they actually exist.
    Namely, it is clear that, when $x\,\,{\geq}\,\,a$, $g_{a}(x)$ must be a lower bound of the set of numbers of the form $\tilde{g}(x)$;
    while when $x\,\,{\leq}\,\,a$, $g_{a}(x)$ must be an upper bound for the set of numbers of the form $\tilde{g(x)}$.
    It then is reasonable to guess that perhaps $g_{a}(x)$ should be the {\em greatest} of the lower bounds of this set when $x\,\,{\geq}\,\,a$,
    and that $g_{a}(x)$ should be the {\em least} of the upper bounds of this set of numbers when $x\,\,{\geq}\,\,a$ but equal the {\em greatest} of the lower bounds when $x\,\,{\leq}\,\,a$.
    Similar remarks hold concerning $h_{a}(x)$ in relation to numbers of the form $\tilde{h}(x)$.

\V

        (4) There is an alternate viewpoint which leads to essentially the same conclusions.
    Namely, suppose that $\tilde{g}$ and $\tilde{h}$ could be expressed as $\tilde{g} \,=\, {\psi}+g_{a}$ and $\tilde{h} \,=\, {\psi}+h_{a}$ as in the preceding remark.
    Then for each $x_{1}$ and $x_{2}$ in $I$ with $x_{1}\,<\,x_{2}$ one would have
        \begin{displaymath}
        \tilde{g}(x_{2}) - \tilde{g}(x_{1}) \,=\, ({\psi}(x_{2})-{\psi}(x_{1})) + (g_{a}(x_{2})-g_{a}(x_{1}))\,\,{\geq}\,\,g_{a}(x_{2}) - g_{a}(x_{1})
        \end{displaymath}
    with the final inequality reflecting the fact that ${\psi}$ is montonic up.
    Moreover, this final inequality would actually reduce to an equation if, and only if, ${\psi}$ is constant on the interval $[x_{1},x_{2}]$.
    Likewise, one would have
        \begin{displaymath}
        \tilde{h}(x_{2}) - \tilde{h}(x_{1}) \,=\, ({\psi}(x_{2})-{\psi}(x_{1})) + (h_{a}(x_{2})-h_{a}(x_{1}))\,\,{\geq}\,\,h_{a}(x_{2}) - h_{a}(x_{1}).
        \end{displaymath}
    That is, the `preferred' functions $g_{a}$ and $h_{a}$ would represent $f$ as the difference of monotonic-up functions in an `optimal' way, with a minimum of `jumping'.

\V
\V

        The next results shows that the ideas suggested in Remarks~(3) and~(4) above are valid.

\V
\V


             \subsection{\small{\bf Theorem} (Jordan's Theorem for Functions of Hybrid Type)}
            \label{ThmF40.110}

\V

        Let $I$ be an interval in ${\RR}$. Suppose that $f:I \,{\rightarrow}\, {\RR}$ is a function of hybrid type on the interval $I$.
    Then for each point $a$ in $I$ there is a unique pair of functions $g_{a},h_{a}:I \,{\rightarrow}\, {\RR}$ with the following properties:

\V

        \h (i)\,\, The functions $g_{a}$ and $h_{a}$ are monotonic up on $I$.

\V

        \h (ii)\, $f(x) - f(a) \,=\, g_{a}(x)-h_{a}(x)$ for all $x$ in $I$, and $g_{a}(a) \,=\, h_{a}(a) \,=\, 0$.

\V

        \h (iii) Suppose that $g,h:I \,{\rightarrow}\, {\RR}$ are monotonic-up functions on $I$ such that $f(x) - f(a) \,=\, g(x)-h(x)$ for all $x$ in $I$,
    and $g(a) \,=\, h(a) \,=\, 0$.
    Then $g(x)\,\,{\geq}\,\,g_{a}(x)$ and $h(x)\,\,{\geq}\,\,h_{a}(x)$ for all $x$ in $I$ such that $x\,\,{\geq}\,\,a$;
    likewise, $g(x)\,\,{\leq}\,\,g_{a}(x)$ and $h(x)\,\,{\leq}\,\,h_{a}(x)$ for all $x$ in $I$ such that $x\,<\,a$.

\V

        \underline{Proof} Let ${\cal G}_{I;a}$ denote the set of all functions $g:I \,{\rightarrow}\, {\RR}$ such that $g$ and $g-f$ are monotonic up on $I$, and $g(a) \,=\, 0$.

        \underline{Claim} The set ${\cal G}_{I;a}$ is nonempty.

        \underline{Proof of Claim} The hypothesis that $f$ is of hybrid type on $I$ means that there exists a pair of functions $\hat{g}$ and $\hat{h}$ which are monotonic up on $I$ such that $f \,=\, \hat{g}-\hat{h}$.
    Let $g \,=\, \hat{g}-\hat{g}(a)$
    Clearly $g$ is monotonic up on $I$ and $g(a) \,=\, 0$.
    Also,
        \begin{displaymath}
        g-f \,=\, \hat{g}-\hat{g}(a)-f \,=\, \left(\hat{g}-\hat{g}(a)\right)-\left(\hat{g}-\hat{h}\right) \,=\, \hat{h}-\hat{g}(a).
        \end{displaymath}
    Since $\hat{h}$ is monotonic up, it follows that $\hat{h}-\hat{g}(a)$, i.e., $g-f$, is also monotonic up.
    Thus, the function $g$ constructed this way is an element of ${\cal G}_{I;a}$; in particular, the set ${\cal G}_{I;a}$ is nonempty, as claimed.


        To construct the desired functions $g_{a}$ and $h_{a}$, first note that if $g{\in}{\cal G}_{I;a}$ then, since $g$ is monotonic up on $I$,
    one has $g(x)\,\,{\geq}\,\,g(a) \,=\, 0$ for all $x$ in $I$ such that $x\,\,{\geq}\,\,a$;
    likewise, $g(x)\,\,{\leq}\,\,0$ for all $x$ in $I$ such that $x\,\,{\leq}\,\,a$.


        Now define $g_{a}:I \,{\rightarrow}\, {\RR}$ by the rule
        \begin{equation}
        \label{EqnF.75}
        g_{a}(x) \,=\, \left\{
        \begin{array}{ll}
        {\inf}\,\{g(x): g{\in}{\cal G}_{I;a} \mbox{ if $x{\in}I$ and  $x\,\,{\geq}\,\,a$\}} \\
        {\sup}\,\{g(x): g{\in}{\cal G}_{I;a} \mbox{ if $x{\in}I$ and  $x\,\,{\leq}\,\,a$\}}.
        \end{array}
        \right.
        \end{equation}
    Define $h_{a}:I \,{\rightarrow}\, {\RR}$ to be the function given by
        \begin{displaymath}
        h_{a}(x) \,=\, g_{a}(x)-(f(x)-f(a)) \mbox{ for each $x$ in $I$}.
        \end{displaymath}

        \underline{Remarks} (1) The indicated `sup' and `inf' exist since ${\cal G}_{I;a} \,\,{\neq}\,\, {\emptyset}$.

        (2) Equation~\Ref{EqnF.75} provides two different formulas for the number $g_{a}(a)$.
    However, the two formulas yield the same value. Indeed, by definition, if $g{\in}{\cal G}_{I;a}$ then $g(a) \,=\, 0$, and the supremum and infimum of the singleton set $\{0\}$both equal~$0$.

        (3) Since each $g$ in ${\cal G}_{I;a}$ is monotonic up on $I$,
    it then becomes clear that the infimum and supremum in Equation~\Ref{EqnF.75} are both finite.

\V

        \underline{Proof of Property (i)} Suppose $g_{a}$ is {\em not} monotonic up on $I$.
    Then there must exist numbers $x_{1}$ and $x_{2}$ in $I$, with $x_{1}\,<\,x_{2}$,
    such that $g_{a}(x_{1})\,>\,g_{a}(x_{2})$.

        \underline{Case 1} Suppose that $x_{1}\,\,{\geq}\,\,a$, so that $a\,\,{\leq}\,\,x_{1}\,<\,x_{2}$.
    By Equation~\Ref{EqnF.75} one then has $g_{a}(x_{2}) \,=\, {\inf}\,\{g(x_{2}):g{\in}{\cal G}_{I;a}\}$.
    It then follows from the defining properties of `infimum' (see Definition~\Ref{DefB30.90}) that there exists an element $\hat{g}$ in ${\cal G}_{I;a}$ such that $g_{a}(x_{1})\,>\,\hat{g}(x_{2})\,\,{\geq}\,\,g_{a}(x_{2})$.
    Since $\hat{g}{\in}{\cal G}_{I;a}$, one knows that $\hat{g}$ is monotonic up on $I$ and thus $\hat{g}(x_{1})\,\,{\leq}\,\,\hat{g}(x_{2})$.
    However, by the definition of $g_{a}(x_{1})$ as the infimum of numbers of the form $g(x_{1})$ with $g$ in ${\cal G}_{I;a}$,
    it also follows that $g_{a}(x_{1})\,\,{\leq}\,\,\hat{g}(x_{1})$.
    Combining all the above then yields
        \begin{displaymath}
g_{a}(x_{1})\,\,{\leq}\,\,\hat{g}(x_{1})\,\,{\leq}\,\,\hat{g}(x_{2})\,<\,g_{a}(x_{1}),
        \end{displaymath}
    which is clearly impossible. Thus, if $x_{1}$ and $x_{2}$ are elements of $I$ such that $a\,\,{\leq}\,\,x_{1}\,<\,x_{2}$, it follows that $g_{a}(x_{1})\,\,{\leq}\,\,g_{a}(x_{2})$.

        \underline{Case 2} Suppose that $x_{2}\,\,{\leq}\,\,a$, so that $x_{1}\,<\,x_{2}\,<\,a$.
    Assuming that $g_{a}(x_{1})\,>\,g_{a}(x_{2})$ then implies, because $g_{a}(x_{1})$ is defined as a certain supremum,
    that there is $\hat{g}$ in ${\cal G}_{I;a}$ such that $g_{a}(x_{1})\,\,{\geq}\,\,\hat{g}(x_{1})\,>\,g_{a}(x_{2})$.
    As before, one knows that $\hat{g}$ is monotonic up on $I$, so that $\hat{g}(x_{2})\,\,{\geq}\,\,\hat{g}(x_{1})\,>\,g_{a}(x_{2})$.
    However, the definition of $g_{a}(x_{2})$ as a certain supremum implies $g_{a}(x_{2})\,\,{\geq}\,\,\hat{g}(x_{2})$.
    Combining the preceding then implies
        \begin{displaymath}
        g_{a}(x_{2})\,\,{\geq}\,\,\hat{g}(x_{2})\,\,{\geq}\,\,\hat{g}(x_{1})\,>\,g_{a}(x_{2}),
        \end{displaymath}
    which is impossible. Thus if $x_{1}$ and $x_{2}$ are elements of $I$ such that $x_{1}\,<\,x_{2}\,\,{\leq}\,\,a$, then $g_{a}(x_{1})\,\,{\leq}\,\,g_{a}(x_{2})$.

        \underline{Case 3} The only situation yet to be handled is when $x_{1}\,<\,a\,\,{\leq}\,\,x_{2}$.
    By Case~(1) it is clear that $g_{a}(a)\,\,{\leq}\,\,g_{a}(x_{2})$. Likewise, by Case~(2) it is clear that $g_{a}(x_{1})\,\,{\leq}\,\,g_{a}(a)$.
    Now apply the Transitivity Law for Order to conclude that $g_{a}(x_{1})\,\,{\leq}\,\,g_{a}(x_{2})$, as required.

    Next, define $h_{a}$ by the rule
        \begin{displaymath}
        h_{a}(x) \,=\, g_{a}(x)-(f(x)-f(a)) \mbox{ for all $x$ in $I$}.
        \end{displaymath}
    An argument similar to that given for $g_{a}$ can be used to show that $h_{a}$ is monotonic up on $I$.
    Indeed, suppose that $h_{a}$ is {\em not} monotonic up on $I$.
    Then, as before, there exist numbers $x_{1}$ and $x_{2}$ in $I$, with $x_{1}\,<\,x_{2}$, such that $h_{a}(x_{1})\,>\,h_{a}(x_{2})$.
    That is, one has
        \begin{displaymath}
        g_{a}(x_{1})-(f(x_{1})-f(a))\,>\,g_{a}(x_{2})-(f(x_{2})-f(a));
        \end{displaymath}
    equivalently,
        \begin{displaymath}
        g_{a}(x_{1})-f(x_{1})\,>\,g_{a}(x_{2})-f(x_{2}).
        \end{displaymath}
    As above, there are three cases to consider.

        \underline{Case 1$'$} Suppose that $a\,\,{\leq}\,\,x_{1}\,<\,x_{2}$.
    For convenience let $C \,=\, g_{a}(x_{1})-f(x_{1})$, so that $C\,>\,g_{a}(x_{2})-f(x_{2})$.
    Since
        \begin{displaymath}
        g_{a}(x_{2})-f(x_{2}) \,=\, {\inf}\,\{g(x_{2}): g{\in}{\cal G}_{I;a}\} - f(x_{2}) \,=\, {\inf}\,\{g(x_{2})-f(x_{2}): g{\in}{\cal G}_{I;a}\},
        \end{displaymath}
    it follows from properties of `infimum' that there exists a number of the form $\hat{g}(x_{2})-f(x_{2})$, with $\hat{g}$ in ${\cal G}_{I;a}$,
    such that $C\,>\,\hat{g}(x_{2})-f(x_{2})\,\,{\geq}\,\,g_{a}(x_{2})-f(x_{2})$.
    That is,
        \begin{displaymath}
        g_{a}(x_{1})-f(x_{1})\,>\,\hat{g}(x_{2})-f(x_{2})\,\,{\geq}\,\,g_{a}(x_{2})-f(x_{2}).
        \end{displaymath}
    However, since $\hat{g}{\in}{\cal G}_{I;a}$, one knows that $\hat{g}-f$ is monotonic up on $I$.
    In particular, one then has
        \begin{displaymath}
        \hat{g}(x_{2})-f(x_{2})\,\,{\geq}\,\,\hat{g}(x_{1})-f(x_{1})\,\,{\geq}\,\,g_{a}(x_{1})-f(x_{1});
        \end{displaymath}
    the final inequality follows from the definition of $g_{a}(x_{1})$ as an infimum.
    Combining all these results then yields
        %\begin{displaymath}
       $ g_{a}(x_{1})-f(x_{1})\,>\,\hat{g}(x_{2})-f(x_{2})\,\,{\geq}\,\,g_{a}(x_{1})-f(x_{1})$,
        %\end{displaymath}
    which is impossible.


        \underline{Case 2$'$} A similar proof shows that the inequality $h_{a}(x_{1})\,>\,h_{a}(x_{2})$, with $x_{1}\,<\,x_{2}$, is impossible if $x_{1}\,<\,x_{2}\,<\,a$.


        \underline{Case 3$'$} If $x_{1}\,<\,a\,\,{\leq}\,\,x_{2}$, then an argument using the results of Case~(1$'$) and Case~(2$'$) shows that once again it is impossible to have $h_{a}(x_{1})\,>\,h_{a}(x_{2})$.

\V

        \underline{Proof of Property (ii)} The formula $f(x)-f(a) \,=\, g_{a}(x)-h_{a}(x)$ follows easily from the definition of of $h_{a}(x)$ as the quantity $g_{a}(x)-(f(x)-f(a))$.
    Substituting $x \,=\, a$ into the formula above then yields
        %\begin{displaymath}
        $0 \,=\, f(a)-f(a) \,=\, g_{a}(a)-h_{a}(a)$,
        %\end{displaymath}
    so that in any event one has $h_{a}(a) \,=\, g_{a}(a)$.
    The fact that $g_{a}(a) \,=\, 0$ follows from one of the defining properties of the set ${\cal G}_{I;a}$;
    namely, if $g{\in}{\cal G}_{I;a}$, then $g(a) \,=\, 0$.
    In particular, $g_{a}(a)$ is the infimum of the singleton set $\{0\}$, hence $g_{a}(a) \,=\, 0$.

\V

        \underline{Proof of Property (iii)} If $g$ and $h$ are as described in the statement of~(iii) then clearly $g{\in}{\cal G}_{I;a}$.

        \underline{Case 1} Suppose that $x$ in $I$ satisfies $x\,\,{\geq}\,\,a$.
    Then the definition of $g_{a}(x)$ as an infimum implies $g(x)\,\,{\geq}\,\,g_{a}(x)$.
    Likewise, the fact that $f(x)-f(a) \,=\, g(x)-h(x) \,=\, g_{a}(x)-h_{a}(x)$ implies that $h(x) \,=\, g(x)-(f(x)-f(a))\,\,{\geq}\,\,g_{a}(x)-(f(x)-f(a)) \,=\, h_{a}(x)$.

        \underline{Case 2} Suppose that $x$ in $I$ satisfies $x\,<\,a$. Then the definition of $g_{a}(x)$ as a supremum implies $g(x)\,\,{\leq}\,\,g_{a}(x)$.
    Likewise, the formula $f(x)-f(a) \,=\, g(x)-h(x) \,=\, g_{a}(x)-h_{a}(x)$ implies $h(x)\,\,{\leq}\,\,h_{a}(x)$.

\V

        Finally, the fact that the functions $g_{a}$ and $h_{a}$ constructed above are the only ones which satisfy (i), (ii) and~(iii) now follows easily.
    Indeed, suppose that $\overline{g_{a}}$ and $\overline{h}_{a}$ are also functions which have Properties~(i), (ii) and~(iii).
    Since $\overline{g}_{a}$ and $\overline{h}_{b}$ satisfy (i) and (ii), it follows from the fact that $g_{a}$ and $g_{b}$ satisfy~(iii) that $\overline{g}_{a}(x)\,\,{\geq}\,\,g_{a}(x)$ and $\overline{h}_{a}(x)\,\,{\geq}\,\,h_{a}(x)$ for all $x$ in $I$ such that $x\,\,{\geq}\,\,a$.
    By reversing the roles of $\overline{g}_{a}$ and $\overline{h}_{a}$ with those of $g_{a}$ and $h_{a}$, one likewise gets $g_{a}(x)\,\,{\geq}\,\,\overline{g}_{a}(x)$ and $h_{a}(x)\,\,{\geq}\,\,\overline{h}_{a}(x)$ for all $x$ in $I$ such that $x\,\,{\geq}\,\,a$.
    Thus, $\overline{g}_{a}(x) \,=\, g_{a}(x)$ and $\overline{h}_{a}(x) \,=\, h_{a}(x)$ for all $x$ in $I$ such that $x\,\,{\geq}\,\,a$.
    A similar argument shows that $\overline{g}_{a}(x) \,=\, g_{a}(x)$ and $\overline{h}_{a}(x) \,=\, h_{a}(x)$ for all $x$ in $I$ such that $x\,<\,a$.

\V
\V
             \subsection{\small{\bf Definition}}
            \label{DefF40.115}

\V

        Let $f:I \,{\rightarrow}\, {\RR}$ be a function of hybrid type on an interval $I$, and let $a$ be a point of $I$.
    The expression $f-f(a) \,=\, g_{a}-h_{a}$ obtained above is called the {\bf Jordan splitting of $f$ at $a$};
    the functions $g_{a}$ and $h_{a}$ are then called the corresponding {\bf components of the Jordan splitting of $f$ at $a$}.

\V

        {\bf Remark} Some authors would use the phrase `Jordan {\em decomposition}' instead of `Jordan {\em splitting}'.
    However, the `Jordan decomposition' terminology is also used in other parts of mathematics, so we prefer the `splitting' terminology to lessen the chance for confusion.

\V
\V

        The next pair of results tell us, in effect, that the Jordan splitting provides an {\em optimal} representation of a hybrid function as the difference of two monotonic-up functions,
    in the sense indicated in Remark~\Ref{RemrkF40.100}~(4) above.

\V

             \subsection{\small{\bf Theorem}}
            \label{ThmF40.130}

        Suppose that $f:I \,{\rightarrow}\, {\RR}$ is a function of hybrid type on the interval $I$.
    Let $a$ be an element of $I$ and let $f-f(a) \,=\, g_{a}-h_{a}$ be a Jordan splitting of $f$;
    note that, in particular, one has $g_{a}(a) \,=\, h_{a}(a) \,=\, 0$.
    Let $g$ and $h$ be monotonic-up functions on $I$ such that $f \,=\, g-h$.
    Suppose that $x_{1}$ and $x_{2}$ are elements of $I$ such that $x_{1}\,<\,x_{2}$. Then
        \begin{equation}
        \label{IneqF.80}
        g(x_{2})-g(x_{1})\,\,{\geq}\,\,g_{a}(x_{2})-g_{a}(x_{1})
        \end{equation}
    and
        \begin{equation}
        \label{IneqF.90}
        h(x_{2})-h(x_{1})\,\,{\geq}\,\,h_{a}(x_{2})-h_{a}(x_{1}).
        \end{equation}
    Moreover, if one gets equality in either of these inequalities, then $g$ and $h$ differ from $g_{a}$ and $h_{a}$ by constants on the interval $[x_{1},x_{2}]$.
    More precisely, $g(x) \,=\, g_{a}(x) + g(x_{1})-g_{a}(x_{1})$ and $h(x) \,=\, h_{a}(x) + h(x_{1})-h_{a}(x_{1})$ for all $x$ in $[x_{1},x_{2}]$.

\V

        \underline{Proof} Let $\hat{g} \,=\, g-g(a)$ and $\hat{h} \,=\, h-h(a)$.
    Then it is clear that $\hat{g}$ and $\hat{h}$ are in ${\cal G}_{I;a}$.

        Now let $x_{1}$ and $x_{2}$ be in $I$ and satisfy $x_{1}\,<\,x_{2}$. Then one has
        \begin{displaymath}
         (i)\, \h g(x_{2}) - g(x_{1}) \,=\, \hat{g}(x_{2}) - \hat{g}(x_{1})
    \mbox{ and }
         (ii)  \h h(x_{2}) - h(x_{1}) \,=\, \hat{h}(x_{2}) - \hat{h}(x_{1})
    \h ({\ast})
        \end{displaymath}
    There are several cases to consider.

        \h \underline{Case 1} Suppose that $x_{1}\,\,{\leq}\,\,a\,\,{\leq}\,\,x_{2}$.
    (Note that this includes the cases $a \,=\, x_{1}\,<\,x_{2}$ and $x_{1}\,<\,a \,=\, x_{2}$.)
    Then by the very constructions of the functions $g_{a}$ and $h_{a}$ one has
        \begin{displaymath}
        \hat{g}(x_{1})\,\,{\leq}\,\,g_{a}(x_{1}) \mbox{ and } \hat{h}(x_{1})\,\,{\leq}\,\,h_{a}(x_{1}).
        \end{displaymath}
    Likewise,
        \begin{displaymath}
        \hat{g}(x_{2})\,\,{\geq}\,\,g_{a}(x_{2}) \mbox{ and } \hat{h}(x_{2})\,\,{\geq}\,\,h_{a}(x_{2}).
        \end{displaymath}
    It follows easily that
        \begin{displaymath}
        \hat{g}(x_{2}) - \hat{g}(x_{1})\,\,{\geq}\,\,g_{a}(x_{2}) - g_{a}(x_{1}) \mbox{ and } \hat{h}(x_{2}) - \hat{h}(x_{1})\,\,{\geq}\,\,h_{a}(x_{2}) - h_{a}(x_{1}).
        \end{displaymath}
    Combine this with Part~$(i)$ of Equation~$({\ast})$ above to get
        \begin{displaymath}
        g(x_{2}) - g(x_{1})\,\,{\geq}\,\,g_{a}(x_{2})-g_{a}(x_{1});
        \end{displaymath}
    likewise, combine with Part~$(ii)$ of~$({\ast})$ to get
        \begin{displaymath}
        h(x_{2}) - h(x_{1})\,\,{\geq}\,\,h_{a}(x_{2})-h_{a}(x_{1});
        \end{displaymath}


        \underline{Case 2} Now assume that $a\,<\,x_{1}\,<\,x_{2}$. Suppose that it is {\em not} the case that $g(x_{2})-g(x_{1})\,\,{\geq}\,\,g_{a}(x_{2}) - g_{a}(x_{1})$;
    or, in light of Equation~$({\ast})$, suppose that $\hat{g}(x_{2})-\hat{g}(x_{1})\,<\,g_{a}(x_{2}) - g_{a}(x_{1})$.
    Define a new function $G:I \,{\rightarrow}\, {\RR}$ by the rule
        \begin{displaymath}
        G(x) \,=\, \left\{
        \begin{array}{ll}
        g_{a}(x)   & \mbox{if $x\,\,{\leq}\,\,x_{1}$} \\
        g(x) - g(x_{1}) + g_{a}(x_{1}) & \mbox{if $x_{1}\,<\,x\,<\,x_{2}$} \\
        g_{a}(x) - \left((g_{a}(x_{2}) - g_{a}(x_{1})) - (g(x_{2})-g(x_{1}))\right)   & \mbox{if $x\,\,{\geq}\,\,x_{2}$}
        \end{array}
        \right.
        \end{displaymath}
    Then it is easy to verify that $G{\in}{\cal G}_{I;a}$; the details are left as an exercise.
    Furthermore, if $g(x_{2}) - g(x_{1})\,<\,g_{a}(x_{2}) - g_{a}(x_{1})$, then it follows that $G(x_{2})\,<\,g_{a}(x_{2})$; but this would contradict the construction of $g_{a}(x_{2})$ as an infimum.

        A similar argument shows that one must have $h(x_{2})-h(x_{1})\,\,{\geq}\,\,h_{a}(x_{2}) - h_{a}(x_{1})$.

        \underline{Case 3} Assume now that $x_{1}\,<\,x_{2}\,<\,a$. Then an argument similar to that in Case~(2) shows that $g(x_{2}) - g(x_{1})\,\,{\geq}\,\,g_{a}(x_{2}) - g_{a}(x_{1})$ and that $h(x_{2}) - h(x_{1})\,\,{\geq}\,\,h_{a}(x_{2})-h_{a}(x_{1})$.

        Inequalities~\Ref{IneqF.80} and~\Ref{IneqF.90} now follow.

        Finally, suppose that $x{\in}[x_{1},x_{2}]$. 
    Then by what has just been proved one has
        \begin{displaymath}
        g(x_{2}) - g(x)\,\,{\geq}\,\,g_{a}(x_{2})-g_{a}(x) \mbox{ and } 
        g(x) - g(x_{1})\,\,{\geq}\,\,g_{a}(x)-g_{a}(x_{1})
        \end{displaymath}
    Add the terms of these inequalities to get
        \begin{displaymath}
        g(x_{2}) - g(x_{1}) \,=\, (g(x_{2}) - g(x)) + (g(x) - g(x_{1}))\,\,{\geq}\,\,(g_{a}(x_{2}) - g_{a}(x)) + (g_{a}(x) - g_{a}(x_{1})) \,=\, g_{a}(x_{2}) - g_{a}(x_{1}).
        \end{displaymath}
    Now suppose that $g(x_{2}) - g(x_{1}) \,=\, g_{a}(x_{2}) - g_{a}(x_{1})$.
 Then all the inequalities above reduce to equations.
    In particular, one must have
        \begin{displaymath}
        g(x_{2}) - g(x) \,=\, g_{a}(x_{2}) - g_{a}(x) \mbox{ for all $x$ in $[x_{1},x_{2}]$}
        \end{displaymath}
    It follows that $g_{a}(x) - g(x) \,=\, g_{a}(x_{2}) - g(x_{2})$ for all $x$ in $[x_{1},x_{2}]$;
    that is, the functions $g$ and $g_{a}$ differ by a constant on the interval $[x_{1},x_{2}]$, as claimed.

        A similar argument shows that if $h(x_{2}) - h(x_{1}) \,=\, h_{a}(x_{2}) - h_{a}(x_{1})$, then $h$ and $h_{a}$ differ by a constant on $[x_{1},x_{2}]$.

\V
\V

             \subsection{\small{\bf Corollary}}
            \label{CorF40.140}

        Suppose that $f$ is a function of hybrid type on the interval $I$; let $a$ be a point of $I$, and let $g, h:I \,{\rightarrow}\, {\RR}$ be functions on $I$.
    Then the following conditions are equivalent:

        \h (i)\, The functions $g$ and $h$  are monotonic up on $I$ and $f \,=\, g-h$.

        \h (ii) There exists a monotonic-up function ${\psi}:I \,{\rightarrow}\, {\RR}$ such that $g \,=\, {\psi}+g_{a}+f(a)$ and $h \,=\, {\psi}+h_{a}$.


\V

        {\bf Proof} The fact that Statement~(ii) implies Statement~(i) is obviously true;
    see Remark~\Ref{RemrkF40.100}~(1).
    
        Now suppose that Statement~(i) is true. Set ${\psi} \,=\, g-g_{a}-f(a)$.

        \underline{Claim 1} The function ${\psi}$ is monotonic up on $I$.
  
        \underline{Proof of Claim 1} Note that if $x_{1}$ and $x_{2}$ are in $I$ and $x_{1}\,<\,x_{2}$, then by Inequality~\Ref{IneqF.80} one has
        \begin{displaymath}
        {\psi}(x_{2})-{\psi}(x_{1}) \,=\, (g(x_{2})-g(x_{1})) - (((g_{a}(x_{2})-f(a))-(g_{a}(x_{1})-f(a))) \,=\, (g(x_{2})-g(x_{1})) - (g_{a}(x_{2})-g_{a}(x_{1}))\,\,{\geq}\,\,0
        \end{displaymath}
    Thus ${\psi}$ is monotonic up on $I$, as claimed.

        \underline{Claim 2} One also has $h \,=\, {\psi}+h_{a}$.

        \underline{Proof of Claim 2} Note that
        %\begin{displaymath}
        $f \,=\, g-h \,=\, g_{a}-h_{a}+f(a)$,
        %\end{displaymath}
    hence
        %\begin{displaymath}
        $g-g_{a}-f(a) \,=\, h-h_{a}$.
       % \end{displaymath}
    The claim now follows from the definition of ${\psi}$.
 
\V
\V

        The next result shows that the dependence on the base point $a$ of the Jordan splitting $f-f(a) \,=\, g_{a} - h_{a}$ is simple.
    

             \subsection{\small{\bf Corollary}}
            \label{CorF40.145}

        Suppose that $f:I \,{\rightarrow}\, {\RR}$ is a function of hybrid type on the interval $I$.
    Let $a$ and $b$ be elements of $I$, and let $g_{a},h_{a}$ and $g_{b},h_{b}$ be the corresponding Jordan splittings of $f$.
    Then
        \begin{displaymath}
        g_{b}(x) \,=\, g_{a}(x)-g_{a}(b) \mbox{ and }
        h_{b}(x) \,=\, h_{a}(x)-h_{a}(b) \mbox{ for all $x$ in $I$}.
        \end{displaymath}
    In particular, for each pair of numbers $x_{1},x_{2}$ in $I$ one has
        \begin{displaymath}
        g_{b}(x_{2})-g_{b}(x_{1}) \,=\, g_{a}(x_{2})-g_{a}(x_{1}) \mbox{ and }
        h_{b}(x_{2})-h_{b}(x_{1}) \,=\, h_{a}(x_{2})-h_{a}(x_{1}).
        \end{displaymath}

\V

        The simple proof is left as an exercise.


        The preceding results allows one to determine Jordan splittings in some important cases.

\V

             \subsection{\small{\bf Theorem}}
            \label{ThmF40.150}

\V

        Suppose that $f:I \,{\rightarrow}\, {\RR}$ is a function of hybrid type on the interval $I$.
    Let $a$ be a point of $I$, and let $g_{a}$ and $h_{a}$ denote the components of the corresponding Jordan splitting.

\V

        (a) If $f$ is monotonic up on $I$ then $h_{a}$ is constant zero on $I$;
    likewise, if $f$ is monotonic down on $I$, then $g_{a}$ is constant zero on $I$.

\V

        (b) Suppose that $c$ is a point of $I$ which is not the left-hand endpoint of $I$,
    so that the left-hand jumps ${\Delta}_{-} f(c)$, ${\Delta}_{-} g_{a}(c)$ and ${\Delta}_{-} h_{a}(c)$ at $c$ of the functions $f$, $g_{a}$ and $h_{a}$ exist and are finite.
    (See Part~(4) of Definition~\Ref{DefF40.60} for the descriptions of the left-hand and right-hand jumps.) Then:

       \h (i)\,\, If ${\Delta}_{-}f(c)\,\,{\geq}\,\,0$ then ${\Delta}_{-} h_{a}(c) \,=\, 0$ and ${\Delta}_{-}g_{a}(c) \,=\, {\Delta}_{-} f(c)$.


       \h (ii)\, If, instead, ${\Delta}_{-} f(c)\,\,{\leq}\,\,0$ then ${\Delta}_{-} g_{a}(c) \,=\, 0$ and ${\Delta}_{-} h_{a}(c) \,=\, -{\Delta}_{-} f(c)$.

\noindent Likewise, suppose that $c$ is not a right-hand endpoint of $I$.
    Then:

        \h (iii) If ${\Delta}_{+}f(c)\,\,{\geq}\,\,0$ then ${\Delta}_{+}g_{a}(c) \,=\, {\Delta}_{+} f(c)$ and ${\Delta}_{+} h_{a}(c) \,=\, 0$.

        \h (iv) If ${\Delta}_{+}f(c)\,\,{\leq}\,\,0$ then ${\Delta}_{+}g_{a}(c) \,=\, 0$ and ${\Delta}_{+} h_{a}(c) \,=\, {\Delta}_{+} f(c)$.

\V

        {\bf Proof}

\V

        (a) If $f$ is monotonic up on $I$, define $g:I \,{\rightarrow}\, {\RR}$ and $h:I \,{\rightarrow}\, {\RR}$ by the rules
        \begin{displaymath}
        g(x) \,=\, f(x) \mbox{ and } h(x) \,=\, 0 \mbox{ for all $x$ in $I$}.
        \end{displaymath}
    Then $g$ is monotonic up on $I$ because of the hypothesis on $f$, and $h$ is monotonic up on $I$ because it is constant on $I$.
    Also, one has $g-h \,=\, f-0 \,=\, f$. Thus, Theorem~\Ref{ThmF40.130}, and in particular Inequality~\Ref{IneqF.90},
    can be used to imply that for each $x_{1}$ and $x_{2}$ in $I$ with $x_{1}\,<\,x_{2}$ one has
        \begin{displaymath}
        0 \,=\, h(x_{2})-h(x_{1})\,\,{\geq}\,\,h_{a}(x_{2})-h_{a}(x_{1})\,\,{\geq}\,\,0.
        \end{displaymath}
    (The equation $0 \,=\, h(x_{2})-h(x_{1})$ uses the fact that $h$ is a constant function on $I$;
    the inequality $h_{a}(x_{2})-h_{a}(x_{1})\,\,{\geq}\,\,0$ reflects the fact that the function $h_{a}$ is monotonic up on $I$.)
    It follows that $0\,\,{\geq}\,\,h_{a}(x_{2})-h_{a}(x_{1})\,\,{\geq}\,\,0$, which implies that $h_{a}(x_{2}) \,=\, h_{a}(x_{1})$.
    Since this is true for every such pair of numbers $x_{1}$ and $x_{2}$ in $I$, it follows that $h_{a}$ is constant on $I$, as claimed; and since $h_{a}(a) \,=\, 0$, that constant value is zero.

        To prove that $g_{a}$ is constant zero if $f$ is monotonic down, apply the preceding result to the monotonic-up function $-f $.

\V

        (b) (i) Suppose that ${\Delta}_{-} f(c)\,\,{\geq}\,\,0$. Since $f \,=\, g_{a}-h_{a}+f(a)$, it follows from basic limit laws that
        \begin{displaymath}
        {\Delta}_{-} f(c) \,=\, {\Delta}_{-} g_{a}(c) - {\Delta}_{-} h_{a}(c) \h ({\ast})
        \end{displaymath}
    Note that, by definition,
        %\begin{displaymath}
        ${\Delta}_{-} g_{a}(c) \,=\, g_{a}(c)-\lim_{x{\nearrow}c} g_{a}(c)\,\,{\geq}\,\,0$,
        %\end{displaymath}
    where the inequality on the right uses the fact that $g_{a}$ is monotonic up on $I$ and in the given limit $x$ approaches $c$ from the left.
    Likewise, one has
        \begin{displaymath}
        {\Delta}_{-} h_{a}(c) \,=\, h_{a}(c) - \lim_{x{\nearrow}c} h_{a}(x)\,\,{\geq}\,\,0 \h ({\ast}{\ast})
        \end{displaymath}
    For convenience, set ${\Delta} \,=\, {\Delta}_{-} h_{a}(c)$, and suppose, in contradiction to the conclusion of Statement~(i), that ${\Delta} \,\,{\neq}\,\, 0$.
    In light of Inequality~$({\ast}{\ast})$ it follows that ${\Delta}\,>\,0$.
    Now define $g,h:I \,{\rightarrow}\, {\RR}$ by the rules
        \begin{displaymath}
        g(x) \,=\, \left\{
        \begin{array}{ll}
        g_{a}(x) + f(a) & \mbox{if $x{\in}I$ and $x\,<\,c$} \\
        g_{a}(x) + f(a) - {\Delta} & \mbox{if $x{\in}I$ and $x\,\,{\geq}\,\,c$},
        \end{array}
                                \right.
        \end{displaymath}
    and
        \begin{displaymath}
        h(x) \,=\, \left\{
        \begin{array}{ll}
        h_{a}(x) & \mbox{if $x{\in}I$ and $x\,<\,c$} \\
        h_{a}(x) - {\Delta} & \mbox{if $x{\in}I$ and $x\,\,{\geq}\,\,c$},
        \end{array}
                                \right.
        \end{displaymath}
    It is obvious that $g(x)-h(x) \,=\, g_{a}(x)-h_{a}(x)+f(a) \,=\, f(x)$ for all $x$ in $I$.
    It is also clear that $g$ and $h$ are monotonic up on $I$. Indeed, suppose that $x_{1}$ and $x_{2}$ are in $I$ and $x_{1}\,<\,x_{2}$.
    If $x_{2}\,<\,c$ then $x_{1}\,<\,c$ hence
        \begin{displaymath}
        g(x_{2})-g(x_{1}) \,=\, (g_{a}(x_{2})+f(a)) - (g_{a}(x_{1})+f(a)) \,=\, g_{a}(x_{2})-g_{a}(x_{1})\,\,{\geq}\,\,0
        \end{displaymath}
    because $g_{a}$ is monotonic up on $I$.
    Likewise, if $x_{1}\,\,{\geq}\,\,c$, so that $x_{2}\,>\,c$, then
        \begin{displaymath}
        g(x_{2})-g(x_{1}) \,=\, (g_{a}(x_{2})+f(a)-{\Delta}) - (g_{a}(x_{1})+f(a)-{\Delta}) \,=\, g_{a}(x_{2})-g_{a}(x_{1})\,\,{\geq}\,\,0.
        \end{displaymath}
    Finally, if $x_{1}\,<\,c$ and $x_{2}\,\,{\geq}\,\,c$, then
        %\begin{displaymath}
        $g(x_{2}) \,=\, g_{a}(x_{2})+f(a)-{\Delta}\,\,{\geq}\,\,g_{a}(c)+f(a)-{\Delta}$,
        %\end{displaymath}
and
        %\begin{displaymath}
        $g(x_{1}) \,=\, g_{a}(x_{1})+f(a)$.
        %\end{displaymath}
    It then follows that
        \begin{displaymath}
        g(x_{2})-g(x_{1})\,\,{\geq}\,\,(g_{a}(c)+f(a)-{\Delta})-(g_{a}(x_{1})+f(a)) \,=\, g_{a}(c)-g_{a}(x_{1})-{\Delta}
        \end{displaymath}
    However, it follows, from the fact that $g_{a}$ is monotonic up on $I$, together with Equation~$({\ast})$, that
        \begin{displaymath}
        g_{a}(c)-g_{a}(x_{1})\,\,{\geq}\,\,{\Delta}_{-} g_{a}(c) \,=\, 
    {\Delta}_{-} f(c) + {\Delta}\,\,{\geq}\,\,{\Delta},
        \end{displaymath}
    where the inequality on the right uses the hypothesis that ${\Delta}_{-} f(c)\,\,{\geq}\,\,0$.
    It follows from these results that
        %\begin{displaymath}
        $g(x_{2})-g(x_{1})\,\,{\geq}\,\,g_{a}(c)-g_{a}(x_{1})-{\Delta}\,\,{\geq}\,\,0$.
        %\end{displaymath}
    It is clear now that $g$ is monotonic up on $I$. An even simpler argument shows that $h$ is also monotonic up on $I$.
    It now follows that $g$ and $h$ satisfy the hypotheses for Theorem~\Ref{ThmF40.130}.
    In particular, Inequality~\Ref{IneqF.90} then implies that if $x_{1}$ is in $I$  and $x_{1}\,<\,c$, then
        %\begin{displaymath}
        $h(c)-h(x_{1})\,\,{\geq}\,\,h_{a}(c)-h_{a}(x_{1})$;
       % \end{displaymath}
    that is,
        %\begin{displaymath}
        $(h_{a}(c)-{\Delta})-h_{a}(x_{1})\,\,{\geq}\,\,h_{a}(c)-h_{a}(x_{1})$.
       % \end{displaymath}
    This inequality cannot be correct, since ${\Delta}\,>\,0$. Thus, assuming ${\Delta}_{-} h_{a}(c) \,\,{\neq}\,\, 0$ leads to a contradiction, so one must have ${\Delta}_{-} h_{a}(c) \,=\, 0$, as claimed.
    Combining this with Equation~$({\ast})$ then implies ${\Delta}_{-} g_{a}(c) \,=\, {\Delta}_{-} f(c)$, also as claimed, so Statement~(i) is true.


        The proofs of Statements (ii), (iii) and (iv) are similar, and are left as exercises.

\V
\V

             \subsection{\small{\bf Corollary}}
            \label{CorF40.160}

        Suppose that $f:I \,{\rightarrow}\, {\RR}$ is a function of hybrid type on the interval $I$.
    Let $a$ be a point of $I$, and let $g_{a}$ and $h_{a}$ denote the components of the corresponding Jordan splitting.
    If $f$ is continuous at a point $c$ of $I$, then $g_{a}$ and $h_{a}$ are also continuous at $c$.

\V

        {\bf Proof} The hypothesis that $f$ is continuous at $c$ implies that each one-sided jump of $f$ at $c$ is~$0$.
    It then follows from the preceding theorem that each one-sided jump at $c$ of $g_{a}$ and of $h_{a}$ is also~$0$, and thus $g_{a}$ and $h_{a}$ are both continuous at~$c$, as claimed.

\V
\V

             \subsection{\small{\bf Example}}
            \label{ExampF40.170}

\V

\hspace*{\parindent}Let $f(x) \,=\, {\cos}\,x$ on the interval $I \,=\,[0,+{\infty})$.
    One knows from elementary calculus that the function $f$ is continuous on $I$.
    Also, $f$ is monotonic down on intervals of the form $[(2k-2){\pi},(2k-1){\pi}]$,
    and monotonic up on intervals of the form $[(2k-1){\pi},2k{\pi}]$, with $k$ in ${\NN}$.
    It then follows from Theorem~\Ref{ThmF40.150} that for each $a$ in $I$, one has $g_{a}$ constant on every interval of the form $[(2k-2){\pi},(2k-1){\pi}]$,
    while $h_{a}$ is constant on every interval of the form $[(2k-1){\pi},2k{\pi}]$, with $k$ in ${\NN}$.

        In particular, let $a \,=\, 0$. Then, since $f(0) \,=\, {\cos}\,0 \,=\, 1$, one sees that
        \begin{displaymath}
        g_{0}(x) \,=\, 0, \, h_{0}(x) \,=\, 1-({\cos}\,x), \, 
    0\,\,{\leq}\,\,x\,\,{\leq}\,\,{\pi}.
        \end{displaymath}
    Note that on the interval $[0,{\pi}]$ we do have
        \begin{displaymath}
        f(x) \,=\, g_{0}(x)-h_{0}(x) + f(0), \h g_{0}(0) \,=\, h_{0}(0) \,=\, 0,
        \end{displaymath}
    aa required. Similarly, one hsa
        \begin{displaymath}
        g_{0}(x) \,=\, ({\cos}\,x)+1, \, h_{0}(x) \,=\, 2, \,
    {\pi}\,\,{\leq}\,\,x\,\,{\leq}\,\,2{\pi}.
        \end{displaymath}
    Note: If you are confused by the numbers $1$ and $2$ appearing in the expressions for $g_{0}$ and $h_{0}$,
    recall that in this interval $h_{0}$ must be constant. However, since $f$ is continuous on $I$ it follows from Corollary~\Ref{CorF40.160}) that $g_{a}$ and $h_{a}$ must also be continuous on $I$.
    In particular, the constant value of $h_{0}$ on the second subinterval $[{\pi},2{\pi}]$ must equal the value of $h_{0}$ at the right endpoint of the first subinterval $[0,{\pi}]$.
    That value is $h_{0}({\pi}) \,=\, 1-{\cos}\,({\pi}) \,=\, 1-(-1) \,=\, 2$.
    Then the number $1$ appearing in the formula for $g_{0}$ comes from the fact that $g_{0}(x) \,=\, f(x)-f(0)+h_{0}(x) \,=\, {\cos}\,x-1+2 \,=\, {\cos}\,x+1$.

        The reader is encouraged to determine the formulas for $g_{0}$ and $h_{0}$ on the intervals $[2{\pi},3{\pi}]$, $[3{\pi},4{\pi}]$, and so on.

\V
\V

        In the preceding discussion, there are no restrictions on the type of interval $I$ being used.
    In most treatments of these topics, however, one considers only closed bounded intervals;
    that is, intervals of the form $[{\alpha},{\beta}]$, with ${\alpha}$ and ${\beta}$ in ${\RR}$ and ${\alpha}\,<\,{\beta}$.
    The next result tells us, in effect, that restricting attention to such intervals does not materially affect the theory.

\V
             \subsection{\small{\bf Theorem}}
            \label{ThmF40.180}

\V

        \hspace*{\parindent}Suppose that $f:I \,{\rightarrow}\, {\RR}$ is a function defined on an interval $I$.
    Let $a$ be a point of $I$, and suppose that there exists a nonempty family ${\cal F}$ of subintervals of $I$ such that

        \h (i)\,\, the union of the intervals in the family ${\cal F}$ is the original interval $I$;

        \h (ii)\, the number $a$ is an element of each interval $J$ in the family ${\cal F}$ ;

        \h (iii) for each interval $J$ in ${\cal F}$, the restriction $f|_{J}:J \,{\rightarrow}\, {\RR}$ of $f$ to $J$ is of hybrid type on $J$.

\noindent Then $f$ is of hybrid type on $I$.

\V

        {\bf Outline of Proof} Let $J$ be an interval in the family ${\cal F}$. By Statement~(iii), together with Theorem~\Ref{ThmF40.110},
    there exist unique functions $g_{a;J}:J \,{\rightarrow}\, {\RR}$ and $h_{a;J}: \,{\rightarrow}\, {\RR}$, monotonic up on $J$,
    such that $f|_{J} \,=\, g_{a;J}-h_{a;J} + f(a)$ and $g_{a;J}(a) \,=\, h_{a;J}(a) \,=\, 0$.
    Now define functions $g,h:I \,{\rightarrow}\, {\RR}$ by the following rule:

        If $x{\in}I$, let $J$ be an interval in the family ${\cal F}$ such that $x{\in}J$;
    such $J$ exists because Statement~(i) holds. Now define $g(x) \,=\, g_{a;J}(x)$ and $h(x) \,=\, h_{a;J}(x)$.
    It is easy to see, by the `uniqueness' of the functions $g_{a;J}$ and $h_{a;J}$ mentioned above,
    that if $x$ is an element of more than one interval in the family ${\cal F}$, it does not matter which such interval is used as $J$.
    It is then easy to verify that $g$ and $h$ are monotonic up on $I$, that $g(a) \,=\, h(a) \,=\, 0$, and that $f(x) \,=\, g(x)-h(x)+f(a)$ for all $x$ in $I$, and that the expresion $f \,=\, g-h+f(a)$ is the Jordan splitting at $a$ of $f$.


\V
\V

             \subsection{\small{\bf Remark} (Jordan's Observation)}
            \label{RemrkF40.185}
    
        In his original paper on these topics, Jordan gave a simple criterion for a function $f:[a,b] \,{\rightarrow}\, {\RR}$,
    defined on a closed bounded interval $[a,b]$, to be of hybrid type.
    The criterion is based on the following observation:


\V

        Suppose that $f:[a,b] \,{\rightarrow}\, {\RR}$ can be expressed in the form $f \,=\, g-h$, for monotonic-up functions $g$ and $h$.
    Note that if $c$ and $d$ are any points of $[a,b]$ such that $c\,<\,d$, then
        \begin{displaymath}
        f(d)-f(c) \,=\, (g(d)-g(c)) - (h(d)-h(c))\,\,{\leq}\,\,g(d)-g(c),
        \end{displaymath}
    and 
        \begin{displaymath}
        -(f(d)-f(c)) \,=\, (h(d)-h(c))-(g(d)-g(c))\,\,{\leq}\,\,h(d)-h(c);
        \end{displaymath}
   in both cases one uses the fact that $g$ and $h$ are monotonic up, and thus $g(d)-g(c)\,\,{\geq}\,\,0$ and $h(d)-h(c)\,\,{\geq}\,\,0$.
    Since $|f(d)-f(c)|$ equals one of the numbers $f(d)-f(c)$ or $-(f(d)-f(c))$, it follows that $|f(d)-f(c)|$ is no larger than one of the nonnegative numbers $g(d)-g(c)$ and $h(d)-h(c)$.
    That is,
        %\begin{displaymath}
        $|f(d)-f(c)|\,\,{\leq}\,\,{\max}\,\{(g(d)-g(c)), (h(d)-h(c))\}$.
       %\end{displaymath}
    More generally, let $x_{0}$, $x_{1}$, $x_{2}$,\,{\ldots}\,$x_{k}$ be points of $I$ such that $a \,=\, x_{0}\,<\,x_{1}\,<\,\,{\ldots}\,\,<\,x_{k-1}\,<\,x_{k} \,=\, b$;
    that is, $\{x_{0},\,{\ldots}\,x_{k}\}$ is a partition of the interval $[a,b]$.
    One then has
        \begin{displaymath}
        |f(x_{j})-f(x_{j-1})|\,\,{\leq}\,\,{\max}\,\{(g(x_{j})-g(x_{j-1})),(h(x_{j})-h(x_{j-1}))\} \mbox{ for each $j \,=\, 1,2,\,{\ldots}\,k$}.
        \end{displaymath}
    By adding these quantities, one then gets
        \begin{displaymath}
        \sum_{j=1}^{k} |f(x_{j})-f(x_{j-1})|\,\,{\leq}\,\,\sum_{j=1}^{k} {\max}\,\{(g(x_{j})-g(x_{j-1})),(h(x_{j})-h(x_{j-1}))\}.
        \end{displaymath}
    The sum on the right side of this last inequality is not easy to analyse directly.
    However, note that, since $g$ and $h$ are monotonic up, one has
        \begin{displaymath}
        {\max}\,\{(g(x_{j})-g(x_{j-1})),(h(x_{j})-h(x_{j-1}))\}\,\,{\leq}\,\,(g(x_{j})-g(x_{j-1})) + (h(x_{j})-h(x_{j-1})).
        \end{displaymath}
    Thus
        \begin{equation}
        \label{EqnF.92}
        \sum_{j=1}^{k} |f(x_{j})-f(x_{j-1})|\,\,{\leq}\,\,
        \sum_{j=1}^{k} {\max}\,\{(g(x_{j})-g(x_{j-1})),(h(x_{j})-h(x_{j-1}))\}\,\,{\leq}\,\,
        \]
        \[
    \sum_{j=1}^{k} (g(x_{j})-g(x_{j-1})) + \sum_{j=1}^{k} (h(x_{j})-h(x_{j-1})) \,=\, (g(b)-g(a)) + (h(b)-h(a)).
        \end{equation}
    In other words, a necessary condition for $f:[a,b] \,{\rightarrow}\, {\RR}$ to be of hybrid type on $[a,b]$ is that the set of numbers of the form $\sum_{j=1}^{k} |f(x_{j})-f(x_{j-1})|$, where $a \,=\, x_{0}\,<\,x_{1}\,<\,\,{\ldots}\,\,<\,x_{k-1}\,<\,x_{k} \,=\, b$ is a partition of $[a,b]$, is bounded above.
    What Jordan proved, in effect, is that this condition is also sufficient for $f$ to be of hybrid type on $[a,b]$.


\V
\V

             \subsection{\small{\bf Definition}}
            \label{DefF40.190A}

        Let $f:[a,b] \,{\rightarrow}\, {\RR}$ be a real-valued function defined on a closed bounded interval $[a,b]$.
    Let ${\cal P}_{[a,b]}$ denote the set of all partitions of the interval $[a,b]$.

\V

        (1) Suppose that $P \,=\, \{a \,=\, x_{0}\,<\,x_{1}\,<\,\,{\ldots}\,\,<\,x_{k} \,=\, b\}$ is an element of ${\cal P}_{[a,b]}$, 
    and associate with $f$ and $P$ the number $V(f,P)$ be given by the formula
        \begin{equation}
        \label{EqnF.95A}
        V(f,P) \,=\, \sum_{j=1}^{k} |f(x_{j})-f(x_{j-1})|,
        \end{equation}
    Then the quantity $V_{[a,b]}(f) \,=\, {\sup}\,\{V(f,P): P{\in}{\cal P}_{[a,b]}\}$ is called the {\bf total variation of $f$ over the interval $[a,b]$}.
    
\V

        (2) One says that {\bf $f$ is of bounded variation on $[a,b]$} provided the quantity $V_{[a,b]}$ is a real number;
    that is, provided $V_{[a,b]}(f)\,<\,+{\infty}$.

        \underline{Note}: In light of the definition of `supremum', one can say instead that $f$ is of bounded variation on $[a,b]$
    provided that there exists a number $M$ such that $V(f,P)\,\,{\leq}\,\,M$ for all partitions $P$ of $[a,b]$.
    This follows more closely the approach used by Jordan in his treatment of the concept.

\V
\V


        In the original treatment of the concept of `function of bounded variation',
    Jordan first discusses the (slightly) more primitive ideas of `positive variation' and `negative variation'.

\V

             \subsection{\small{\bf Definition}}
            \label{DefF40.190B}

                Let $f:[a,b] \,{\rightarrow}\, {\RR}$ be a real-valued function defined on a closed bounded interval $[a,b]$.
    Suppose that $P \,=\, \{a \,=\, x_{0}\,<\,x_{1}\,<\,\,{\ldots}\,\,<\,x_{k} \,=\, b\}$ is a partition of $[a,b]$, 
    and associate with $f$ and $P$ the numbers $V^{+}(f,P)$ and $V^{-}(f,P)$ given by the formulas
        \begin{equation}
        \label{EqnF.95B}
        V^{+}(f,P) \,=\, \sum_{j=1}^{k} \left(f(x_{j})-f(x_{j-1})\right)^{+}, \mbox{ and }
        V^{-}(f,P) \,=\, \sum_{j=1}^{k} \left(f(x_{j})-f(x_{j-1})\right)^{-}. 
        \end{equation}
    (As usual, if $c$ is a real number, then $c^{+}$ and $c^{-}$ denote, respectively, the positive and negative part of $c$;
    see Definition~\Ref{DefB20.70}.)
    Then the quantities $V^{+}_{[a,b]}(f)$ and $V^{-}_{[a,b]}(f)$, given by
        \begin{displaymath}
        V^{+}_{[a,b]}(f) \,=\, {\sup}\,\{V^{+}(f,P): P{\in}{\cal P}_{[a,b]}\} \mbox{ and }
    V^{-}_{[a,b]}(f) \,=\, {\sup}\,\{V^{-}(f,P): P{\in}{\cal P}_{[a,b]}\}
        \end{displaymath}
   are called, respectively, the {\bf total positive variation} and the {\bf total negative variation} of $f$ over the interval $[a,b]$.
    If $V^{+}_{[a,b]}(f)$ is a real number (equivalently: if there is a number $M^{+}$ such that $V^{+}(f,P)\,\,{\leq}\,\,M^{+}$ for all partitions $P$ of $[a,b]$),
    then one says that $f$ is {\bf of bounded positive variation on $[a,b]$}.
    The concept of $f$ being of {\bf bounded negative variation on $[a,b]$} is defined similarly.

\V

             \subsection{\small{\bf Theorem}}
            \label{ThmF40.195}

\V


        Suppose that $f:[a,b] \,{\rightarrow}\, {\RR}$ is defined on the closed bounded interval $[a,b]$.
    Then:

        (a) For each partition $P$ of the interval $[a,b]$, if $P'$ is a refinement of $P$, then $0\,\,{\leq}\,\,V^{+}(f,P)\,\,{\leq}\,\,V^{+}(f,P')$, $0\,\,{\leq}\,\,V^{-}(f,P)\,\,{\leq}\,\,V^{-}(f,P')$ and $0\,\,{\leq}\,\,V(f,P)\,\,{\leq}\,\,V(f,P')$.

        In particular, if $P_{0}$ is any partition of $[a,b]$, then one has
        \begin{displaymath}
        V^{+}_{[a,b]}(f) \,=\, {\sup}\,\{V^{+}(f,P'): P'{\in}{\cal P}_{[a,b]} \mbox{ and } P' \,{\supseteq}\, P_{0}\}.
        \end{displaymath}
    Likewise, one has
        \begin{displaymath}
        V^{-}_{[a,b]}(f) \,=\, {\sup}\,\{V^{-}(f,P'): P'{\in}{\cal P}_{[a,b]} \mbox{ and } P' \,{\supseteq}\, P_{0}\},
        \end{displaymath}
    and
        \begin{displaymath}
        V_{[a,b]}(f) \,=\, {\sup}\,\{V(f,P'): P'{\in}{\cal P}_{[a,b]} \mbox{ and } P' \,{\supseteq}\, P_{0}\}.
        \end{displaymath}
    In other words, there is no loss in generality in computing these total variations by restricting one's attention to partitions of $[a,b]$ which contain a given finite collection of points in $[a,b]$.

\V

        (b) For each partition $P$ of $[a,b]$ one has
        \begin{equation}
        \label{EqnF.100A}
        V(f,P) \,=\, V^{+}(f,P) + V^{-}(f,P)
        \end{equation}
    and
        \begin{equation}
        \label{EqnF.100AA}
        f(b) - f(a) \,=\, V^{+}(f,P) - V^{-}(f,P)
        \end{equation}

\V

        (c) The function $f$ is of bounded variation on $[a,b]$ if, and only if, $V^{+}_{[a,b]}(f)$ and $V^{-}_{[a,b]}(f)$ are both finite.
    When this occurs, one has
        \begin{equation}
        \label{EqnF.100B}
        V_{[a,b]} (f) \,=\, V^{+}_{[a,b]}(f) + V^{-}_{[a,b]}(f)
        \end{equation}
    and
        \begin{equation}
        \label{EqnF.100BB}
        f(b) - f(a) \,=\, V^{+}_{[a,b]}(f) - V^{-}_{[a,b]}(f).
        \end{equation}

\V

        (d) If $f$ is of bounded variation on $[a,b]$ then $f$ is of bounded variation on every subinterval of $[a,b]$.
    Moreover, if $u$, $v$ and $w$ are numbers such that $a\,\,{\leq}\,\,u\,<\,v\,<\,w\,\,{\leq}\,\,b$, then
        \begin{equation}
        \label{EqnF.100C}
        V^{+}_{[u,w]}(f) \,=\, V^{+}_{[u,v]}(f) + V^{+}_{[v,w]}(f).
        \end{equation}
    Likewise, one has
        \begin{equation}
        \label{EqnF.100CC}
        V^{-}_{[u,w]}(f) \,=\, V^{-}_{[u,v]}(f) + V^{-}_{[v,w]}(f)
        \end{equation}
    and
        \begin{equation}
        \label{EqnF.100CCC}
        V_{[u,w]}(f) \,=\, V_{[u,v]}(f) + V_{[v,w]}(f).
        \end{equation}

\V
\V


        {\bf Proof}

\V

        (a) Suppose that $P \,=\, \{x_{0} \,=\, a\,<\,x_{1}\,<\,\,{\ldots}\,\,<\,x_{k-1}\,<\,x_{k} \,=\, b\}$.

        \underline{Special Case} Suppose that partition $P'$ has exactly one more point than $P$.
    Then $P'{\setminus}P \,=\, \{c\}$ for some number $c$ in $[a,b]$. Clearly there is an index $j_{0}$, with $1\,\,{\leq}\,\,j_{0}\,\,{\leq}\,\,k$, such that $x_{j_{0}-1}\,<\,c\,<\,x_{j_{0}}$.
    Assume, for the moment, that $1\,<\,j_{0}\,<\,k$. Then it is clear that
        \begin{displaymath}
        V^{+}(f,P') \,=\, (f(x_{1})-f(x_{0}))^{+}+\,{\ldots}\,+ (f(c)-f(x_{j_{0}-1}))^{+} + (f(x_{j_{0}})-f(c))^{+} + \,{\ldots}\,+ (f(x_{k})-f(x_{k-1}))^{+}.
        \end{displaymath}
    In comparison, on has
        \begin{displaymath}
        V(f,P)^{+} \,=\, (f(x_{1})-f(x_{0}))^{+} + \,{\ldots}\,+ (f(x_{j_{0}})-f(x_{j_{0}-1}))^{+} + \,{\ldots}\,+ (f(x_{k})-f_{x_{k-1}}^{+}.
        \end{displaymath}
    In other words, the sums forming the quantities $V(f,P')$ and $V(f,P)$ have exactly the same terms, with the following exception:
    the single term $(f(x_{j_{0}})-f(x_{j_{0}-1}))^{+}$ in the sum for $V^{+}(f,P)$ corresponds to the two terms $(f(c)-f(x_{j_{0}-1}))^{+} + (f(x_{j_{0}})-f(c))^{+}$ in the sum for $V^{+}(f,P')$.
    However, notice that
        \begin{displaymath}
        (f(x_{j_{0}})-f(x_{j_{0}-1}))^{+} \,=\, (f(x_{j_{0}})-f(c)+f(c)-f(x_{j_{0}-1}))^{+} \,=\, 
        \end{displaymath}
        \begin{displaymath}
        ((f(x_{j_{0}})-f(c))+(f(c)-f(x_{j_{0}-1})))^{+}\,\,{\leq}\,\,
    (f(c)-f(x_{j_{0}-1}))^{+} + (f(x_{j_{0}})-f(c))^{+},
        \end{displaymath}
    where the final inequality comes from Part~(d) of Theorem~\Ref{ThmB20.100}.
    It follows easily that $V^{+}(f,P)\,\,{\leq}\,\,V^{+}(f,P')$, in this special case that $P'$ has exactly one point more than $P$.

        If $P'$ is an arbitrary refinement of $P$, then $P'$ can be obtained by adjoining finitely many points to $P$, one  after another,
    and repeatedly using the special case above, together with the transitivity property of order.

        Now suppose that $P_{0}$ is a partition of $[a,b]$.

        \underline{Case 1} If $V^{+}_{[a,b]}(f) \,=\, +{\infty}$,
    then for each $M$ in ${\RR}$ there exists a partition $P$ of $[a,b]$ such that $V^{+}(f,P)\,\,{\geq}\,\,M$.
    Let $P' \,=\, P\,{\cup}\,P_{0}$, so that $P'$ is a refinement of $P$ which contains $P_{0}$.
    By what was just proved, one then has
        %\begin{displaymath}
        $V^{+}(f,P')\,\,{\geq}\,\,V^{+}(f,P)\,\,{\geq}\,\,M$.
        %\end{displaymath}
    It follows that
        %\begin{displaymath}
        ${\sup}\,\{V^{+}(f,P'): P'{\in}{\cal P}_{[a,b]} \mbox{ and } P' \,{\supseteq}\, P_{0}\}\,\,{\geq}\,\,M$.
        %\end{displaymath}
    Since this holds for each $M$, it follows that the supremum on the left equals $+{\infty} \,=\, V^{+}_{[a,b]}(f)$, as required.

        \underline{Case 2} If $V^{+}_{[a,b]}(f)$ is finite, then for every ${\varepsilon}\,>\,0$ there exists a partition $P$ of $[a,b]$ such that
        \begin{displaymath}
        V^{+}_{[a,b]}(f)-{\varepsilon}\,<\,V^{+}(f,P)\,\,{\leq}\,\,V^{+}_{[a,b]}(f).
        \end{displaymath}
    Let $P' \,=\, P\,{\cup}\,P_{0}$. Then by reasoning similar to that used above it follows that
        \begin{displaymath}
        V^{+}_{[a,b]}(f)-{\varepsilon}\,<\,{\sup}\,\{V^{+}(f,P'): P'{\in}{\cal P}_{[a,b]} \mbox{ and } P' \,{\supseteq}\, P_{0}\}\,\,{\leq}\,\,V^{+}_{[a,b]}(f).
        \end{displaymath}
    Since this is true for each ${\varepsilon}\,>\,0$, the equation $V^{+}_{[a,b]}(f) \,=\, {\sup}\,\{V^{+}(f,P'): P'{\in}{\cal P}_{[a,b]} \mbox{ and } P' \,{\supseteq}\, P_{0}\}$ follows.


        A similar proof works in for $V^{-}(f,P)$ and $V(f,P)$; the details are left as an exercise.


        %% CHECK OUT THE OMITTED PROOF ABOVE

\V

        (b) Equation~\Ref{EqnF.100A} follows easily from the observation that
        \begin{displaymath}
        |f(x_{j})-f(x_{j-1})| \,=\
        \left(f(x_{j})-f(x_{j-1})\right)^{+} + \left(f(x_{j})-f(x_{j-1})\right)^{-};
        \end{displaymath}
    see Part~(b) of Theorem~\Ref{ThmB20.100}.
    Equation~\Ref{EqnF.100AA} follows in a similar manner from the observation that
        %\begin{displaymath}
        $f(x_{j})-f(x_{j-1}) \,=\, \left(f(x_{j})-f(x_{j-1})\right)^{+} - \left(f(x_{j})-f(x_{j-1})\right)^{-}$,
        %\end{displaymath}
    together with the fact that
        \begin{displaymath}
        \sum_{j=1}^{n} (f(x_{j})-f(x_{j-1})) \,=\, f(b)-f(a).
        \end{displaymath}

\V

        (c) First, suppose that $f$ is of bounded variation on $[a,b]$. Then, by definition, there exists $M$ in ${\RR}$ such $V(f,P)\,\,{\leq}\,\,M$ for all partitions $P$ of $[a,b]$.
    It then, by Equation~\Ref{EqnF.100A}, one sees that
        \begin{displaymath}
        V^{+}(f,P)\,\,{\leq}\,\,V^{+}(f,P)+V^{-}(f,P) \,=\, V(f,P)\,\,{\leq}\,\,M \mbox{ and }
        V^{-}(f,P)\,\,{\leq}\,\,V^{+}(f,P)+V^{-}(f,P) \,=\, V(f,P)\,\,{\leq}\,\,M
        \end{displaymath}
    for all partitions $P$ of $[a,b]$.
    Conversely, suppose that there exist numbers $M^{+}$ and $M^{-}$ such that $V^{+}(f,P)\,\,{\leq}\,\,M^{+}$ and $V^{-}_{[a,b]}(f)\,\,{\leq}\,\,M^{-}$ for all such partitions $P$.
    Then, by Equation~\Ref{EqnF.100A} and the definitions of $V^{+}_{[a,b]}(f)$ and $V^{-}_{[a,b]}(f)$,
    for every partition $P$ of $[a,b]$ one has
        \begin{displaymath}
        V(f,P) \,=\, V^{+}(f,P)+V^{-}(f,P)\,\,{\leq}\,\,M^{+}+M^{-}.
        \end{displaymath}
    Thus, $f$ is of bounded variation on $[a,b]$. 

        Now suppose that $f$ is of bounded variation on $[a,b]$.
    Let ${\varepsilon}\,>\,0$ be given, and chose partitions $P^{+}$ and $P^{-}$ of $[a,b]$ such that
        \begin{displaymath}
V^{+}_{[a,b]}(f)-{\varepsilon}/2\,<\,V^{+}(f,P^{+})\,\,{\leq}\,\,V^{+}_{[a,b]}(f),
    \, \mbox{ and }
V^{-}_{[a,b]}(f)-{\varepsilon}/2\,<\,V^{-}(f,P^{-})\,\,{\leq}\,\,V^{-}_{[a,b]}(f).
        \end{displaymath}
    (That such partitions $P^{+}$ and $P^{-}$ exist follows from the definition of $V^{+}_{[a,b]}(f)$ and $V^{-}_{[a,b]}(f)$
    as suprema, and the fact, just proved, that when $f$ is of bounded variation on $[a,b]$ then all the preceding quantities are real numbers,
    so the indicated arithmetic makes sense.)
    Let $P'$ be any partition of $[a,b]$ which is simultaneously a refinement of $P^{+}$ and $P^{-}$;
    in other words, let $P'$ be a partition such that $P' \,{\supseteq}\, P^{+}\,{\cup}\,P^{-}$.
    Then, by Part~(a) (and the meaning of `supremum') one obtains
        \begin{displaymath}
V^{+}_{[a,b]}(f)-{\varepsilon}/2\,<\,V^{+}(f,P')\,\,{\leq}\,\,V^{+}_{[a,b]}(f),
    \, \mbox{ and }
V^{-}_{[a,b]}(f)-{\varepsilon}/2\,<\,V^{-}(f,P')\,\,{\leq}\,\,V^{-}_{[a,b]}(f).
        \end{displaymath}
    By the orders properties of ${\RR}$ it then follows that
        \begin{displaymath}
        V^{+}_{[a,b]}(f) + V^{-}_{[a,b]}(f) -{\varepsilon}\,<\,V^{+}(f,P')+V^{-}(f,P')\,\,{\leq}\,\,V^{+}_{[a,b]}(f) + V^{-}_{[a,b]}(f)
        \end{displaymath}
    By Part~(b) one can then write
        \begin{displaymath}
        V^{+}_{[a,b]}(f) + V^{-}_{[a,b]}(f) -{\varepsilon}\,<\,V(f,P')\,\,{\leq}\,\,V^{+}_{[a,b]}(f) + V^{-}_{[a,b]}(f)
        \end{displaymath}
    Since this holds for {\em every} partition $P'$ of $[a,b]$ such that $P' \,{\supseteq}\, P^{+}\,{\cup}\,P^{-}$,
    it follows from the last portion of Part~(a) (and properties of suprema) that
        \begin{displaymath}
        V^{+}_{[a,b]}(f) + V^{-}_{[a,b]}(f) -{\varepsilon}\,<\,V_{[a,b]}(f)\,\,{\leq}\,\,V^{+}_{[a,b]}(f) + V^{-}_{[a,b]}(f).
        \end{displaymath}
    Since this is true for all ${\varepsilon}\,>\,0$, Equation~\Ref{EqnF.100B} follows.

        Equation~\Ref{EqnF.100BB} follows in a like manner by using Equation~\Ref{EqnF.100AA}.

\V

        (d) The proof of this part is similar to what has come before, and is left as an exercise.

\V
\V


             \subsection{\small{\bf Theorem} (Jordan's Bounded-Variation Theorem)}
            \label{ThmF40.200}

        A necessary and sufficient condition for a function $f:[a,b] \,{\rightarrow}\, {\RR}$ to be of hybrid type on a closed bounded interval $[a,b]$ is that $f$ be of bounded variation on $[a,b]$.

\V

        {\bf Proof} The fact that this condition is necessary is obvious; see `Jordan's Observation' above.

    To see that it is also sufficient, note that if $f$ is of bounded variation on $[a,b]$,
    then (by Part~(d) of Theorem~\Ref{ThmF40.195}) for each $x$ such that $a\,<\,x\,\,{\leq}\,\,b$
    the function $f$ is of bounded variation on the subinterval $[a,x]$.
    Thus by Part~(c) of that theorem it follows that $V^{+}_{[a,x]}(f)$ and $V^{-}_{[a,x]}(f)$ are finite.
    Define functions $g,h:[a,b] \,{\rightarrow}\, {\RR}$ be the rules
        \begin{displaymath}
        g(a) \,=\, 0, \h g(x) \,=\, V^{+}_{[a,x]}(f) \mbox{ if $a\,<\,x\,\,{\leq}\,\,b$} \mbox{ and }        h(a) \,=\, 0, \h h(x) \,=\, V^{-}_{[a,x]}(f) \mbox{ if $a\,<\,x\,\,{\leq}\,\,b$}.
        \end{displaymath}
    It follows easily from Part~(d) of Theorem~\Ref{ThmF40.195} that $g$ and $h$  are monotonic up on $[a,b]$,
    and it follows from Equation~\Ref{EqnF.100BB} that
        %\begin{displaymath}
       $ f(x) \,=\, f(a) + g(x)-h(x) \mbox{ for all $x$ in $[a,b]$}$.
       % \end{displaymath}
    Thus, $f$ is a function of hybrid type on $[a,b]$, as claimed.

\V

             \subsection{\small{\bf Remarks}}
            \label{RemrkF40.210}

\V

\hspace*{\parindent}(1) It is a simple exercise to show that the functions $g$ and $h$ obtained in the proof of the preceding theorem are
    the functions $g_{a}$ and $h_{a}$ used to form the Jordan splitting of $f$ at the left-end point $a$; see Definition~\Ref{DefF40.115}.

\V

        (2) Many analysis texts use the alternate formula
        \begin{equation}
        \label{EqnF.110}
        f(x) - f(a) \,=\, V_{[a,x]}(f) + (V_{[a,x]}-f(x))
        \end{equation}
    to express $f(x)-f(a)$ as the difference of the monotonic-up functions $V_{[a,x]}(f)$ and $V_{[a,x]}(f)$.
    In light of Remark~(1) above and Equation~\Ref{EqnF.100B}, this is the same as writing
        \begin{displaymath}
        f(x)-f(a) \,=\, (g_{a}(x)+h_{a}(x)) + ((g_{a}(x)+h_{a}(x)) - (g_{a}(x)-h_{a}(x)) \,=\, (g_{a}(x)+h_{a}(x)) - 2h_{a}(x).
        \end{displaymath}
    Since $g_{a}$ and $h_{a}$ are monotonic up on $[a,b]$, it follows that Equation~\Ref{EqnF.110} does represent $f(x)-f(a)$ as the difference of two monotonic-up functions.

\V

        (3) It is easy to show that a function $f$ is of hybrid type on an interval $I$ if, and only if,
    $f$ is of bounded variation on each bounded subinterval $[a,b]$ of $I$. It is standard in analysis to use only the `bounded variation' terminology,
    so from here on we drop the `hybrid-type' terminology.

\V
\V

        We end with two examples which illustrate the fact that a continuous function can fail to be of bounded variation, while a function of bounded variation can `wiggle' rather badly.

\V

             \subsection{\small{\bf Examples}}
            \label{ExampF40.220}

\V

        (1) Let $y_{1}$, $y_{2}$,\,{\ldots}\,$y_{k}$,\,{\ldots}\, be a sequence of positive numbers such that $\lim_{k \,{\rightarrow}\, {\infty}} y_{k} \,=\, 0$.
    Let $X \,=\, {\displaystyle \left\{1, \frac{1}{2}, \frac{1}{2^{2}},\,{\ldots}\,\frac{1}{2^{k-1}},\,{\ldots}\,\right\}\,{\cup}\,\{0\}}$, and define ${\varphi}:X \,{\rightarrow}\, {\RR}$ by the rule
        \begin{displaymath}
        {\varphi}(x) \,=\, \left\{
        \begin{array}{cl}
        0 & \mbox{if $x \,=\, 1/2^{2m-2}$ for some $m$ in ${\NN}$ or if $x \,=\, 0$} \\
        y_{m} & \mbox{if  $x \,=\, 1/2^{2m-1}$ for some $m$ in ${\NN}$}
        \end{array}
                            \right.
        \end{displaymath}
    It is clear that $X$ is a closed subset of ${\RR}$ and that ${\varphi}$ is continuous on $X$.
    Let $f:[0,1] \,{\rightarrow}\, {\RR}$ be the piecewise-linear continuous extension of ${\varphi}$ to $[0,1]$ guaranteed by Theorem~\Ref{ThmF20.20},
    the Tietze Extension Theorem.
    It is easy to see that $V_{[1/2^{2m},1/2^{2m-2}]}(f) \,=\,2y_{m}$, Thus, by repeated use of Equation~\Ref{EqnF.100CCC}, one gets
        %\begin{displaymath}
        $V_{[1/2^{2m},1]}(f) \,=\, 2y_{1} + 2y_{2} + \,{\ldots}\, + 2y_{m}$
       % \end{displaymath}
    It is easy to see that a necessary and sufficient condition for $f$ to be of bounded variation on the full domain $[0,1]$ is that there exist a number $M$ such that $y_{1} + y_{2} + \,{\ldots}\, + y_{m}\,\,{\leq}\,\,M$ for all $m$ in ${\NN}$.

        \underline{Special Case} Let $y_{m} \,=\, {\ln}\,{\displaystyle \left(\frac{m+1}{m}\right)}$ for each $m$ in ${\NN}$.
    It is easy to see that $y_{m}\,>\,0$ and $\lim_{m \,{\rightarrow}\, {\infty}} y_{m} \,=\, 0$.
    However, by the usual properties of logarithms, one also has
        \begin{displaymath}
        y_{1} + y_{2} + \,{\ldots}\,+ y_{m}  \,=\, {\ln}\,\left(\frac{2}{1}\right) + {\ln}\,\left(\frac{3}{2}\right) + \,{\ldots}\,+ {\ln}\,\left(\frac{m+1}{m}\right) \,=\, 
    {\ln}\,\left(\frac{2}{1}{\cdot}\frac{3}{2}{\cdot}\,{\ldots}\,\frac{m}{m-1}{\cdot}\frac{m+1}{m}\right) \,=\, {\ln}\,(m+1).
        \end{displaymath}
    Since $\lim_{m \,{\rightarrow}\, {\infty}} {\ln}\,(m+1) \,=\, +{\infty}$, it is clear that with this choice of $y_{m}$ the resulting function $f$ is {\em not} of bounded variation on $[0,1]$.

\V
\V

        (2) Let $Z \,=\, \{z_{1},z_{2},\,{\ldots}\,z_{k},\,{\ldots}\,\}$ be a countably infinite subset of the open interval $(0,1)$,
    with the labeling set up so that if $i \,\,{\neq}\,\, j$ then $z_{i} \,\,{\neq}\,\, z_{j}$.
    Let $(y_{1},y_{2},\,{\ldots}\,)$ be a strictly decreasing infinite sequence of positive numbers such that $\lim_{k \,{\rightarrow}\, {\infty}} y_{k} \,=\, 0$.
    Define $f:[0,1] \,{\rightarrow}\, {\RR}$ by the rule
        \begin{displaymath}
        f(x) \,=\, \left\{
        \begin{array}{cl}
        y_{k} & \mbox{if $x \,=\, z_{k}$ for some $k$ in ${\NN}$} \\
          0   & \mbox{if $x$ is not in $Z$}
        \end{array}
                            \right\}
        \end{displaymath}
    Let ${\cal P} \,=\, \{0 \,=\, x_{0}\,<\,x_{1}\,<\,\,{\ldots}\,\,<\,x_{2n-1}\,<\,x_{2n} \,=\, 1\}$ be a partition of the interval $[0,1]$ into an even number $2n$ of subintervals,
    and consider the sum
        %\begin{displaymath}
        $S \,=\, \sum_{j=1}^{2n} |f(x_{j}) - f(x_{j-1})|$
        %\end{displaymath}
    The only way that one of the terms $|f(x_{j}) - f(x_{j-1})|$ can be nonzero is if at least one of the endpoints $x_{j-1}$ or $x_{j}$ is of the form $z_{k}$, and the largest that term could be is $y_{k}$.
    Moreover, a point $z_{k}$ can be an endpoint of no more than two of the intervals determined by the partition ${\cal P}$.
    Since the sequence $(y_{1},y_{2},\,{\ldots}\,)$ is strictly decreasing, the most the sum $S$ could be is $2y_{1} + 2y_{2} + \,{\ldots}\,+ 2y_{n}$.
    It now follows that if there is a number $M$ such that $y_{1} + y_{2} + \,{\ldots}\,+y_{m}\,\,{\leq}\,\,M$ for all $m$ in ${\NN}$,
    then $f$ is of bounded variation.

        \underline{Special Case} Let $Z \,=\, (z_{1},z_{2},\,{\ldots}\,)$ be the set of all rational numbers in the interval $(0,1)$,
    and let $y_{k} \,=\, a^{k}$, where $a$ is a fixed number such that $0\,<\,a\,<\,1$.
    Then it is clear that $y_{1} + \,{\ldots}\,+ y_{m}\,<\,1/(1-a)$.
    The corresponding function $f$ then is of bounded variation on $[0,1]$; however, it is monotonic on {\em no} subinterval of $[0,1]$, because the rationals are dense in $[0,1]$.

\newpage

\input{Exercises_M140AB_F_2017} %% NOTE: Automatically starts on a new page
