% M140AB_G.TeX  Notes for `Single-Variable Analysis'

% 
% Revised: 01/20/2017 %% Encoding Western ASCII 
%

                  \chapter{Infinite Sums (and Products) of Numbers}
                  \label{ChaptG}


        \underline{Quotes for Chapter~\Ref{ChaptG}}: \IndB{chapter quotes}{for Chapter~\Ref{ChaptG} (Infinite Sums (and Products) of Numbers}

\V

\begin{quotation}
{\footnotesize
        (1) $1+2+4+8+\,{\ldots}\,+2^{k}+\,{\ldots}\, \,=\, -1$

        (Formula often attributed to the great $18$th-century Swiss mathematician Leonhard Euler)

\V

        (2) ${\displaystyle 1 + \frac{1}{2} + \frac{1}{4} + \frac{1}{8} +\,{\ldots}\,+\frac{1}{2^{k}} + \,{\ldots}\, } \,=\, +{\infty}$

        (The so-called `Zeno's Formula'; associated with the famous `Achilles and the Tortoise' paradox of Zeno of Elea c. 450BC)

\V

        (3) ${\displaystyle \frac{{\pi}}{2} \,=\, \frac{1{\cdot}2{\cdot}2{\cdot}4{\cdot}4{\cdot}\,{\ldots}\,{\cdot}(2k)(2k){\cdot}\,{\ldots}\,}{3{\cdot}3{\cdot}5{\cdot}5{\cdot}\,{\ldots}\,{\cdot}(2k-1){\cdot}(2k-1)\,{\ldots}\,}}$

        (Formula attributed to the $16$-th century English mathematician John Wallis)

\V

        (4) `The Method of Fluxions and Infinite Series, with its Applications to the Geometry of Curve-Lines'

        (Title of the 1736 translation -- from Latin into English -- by John Colson, of Newton's 1671 (unpublished) book on calculus. `Fluxion' was Newton's terminology for what we call `derivative'.)

}%EndFootNoteSize
\end{quotation}

\V
\V

            \small{\bf Introduction}

\V

        Almost everyone who has a reason to read {\TheseNotes} has encountered, in a course in elementary calculus, the concept of `an infinite sum of numbers',
    but probably under the title of `an infinite {\em series} of numbers'.
    In most such calculus courses the theory of infinite series is based on a previous treatment of infinite {\em sequences} of numbers.
    The level of rigor one encounters in the treatment of sequences in such a course tends to be much lower than that found in {\TheseNotes};
    but if one accepts those results on sequences, then the proofs of the theorems for infinite series based on them are usually correct.
    It follows that we {\em could} base the treatment of `infinite series' in {\TheseNotes}
    directly on the theory of sequences which is developed in Chapter~\Ref{ChaptC}.
    Such an approach would have both advantages and disadvantages:


       \h (i)\, The main \underline{advantage} of such an approach here would be that everything would look familiar:
    the statements and proofs of theorems here would be identical with the analogous treatment in elementary calculus.

       \h (ii) The main \underline{{\em dis}advantage} of such an approach here would be that everything would look familiar:
    the reader who actually paid attention to the statements and proofs of theorems in that earlier course
    would see essentially nothing new here and thus would learn nothing new.

        Instead, in {\TheseNotes} we approach these ideas from a more advanced viewpoint which normally is not stressed in elementary calculus courses.

\V

        Regardless of what approach is taken, the first issue to be handled is to respond to an obvious question:

      \h  `Why would anyone wish to form the sum of infinitely many numbers?'
    \IndB{infinite sums}{Why would anybody wish to form the sum of infinitely many numbers?}

\noindent The appropriate response is that such sums arise fairly naturally, even in `applied' areas.

\V


            \subsection{\small{\bf Examples}}
            \label{ExampG20.10}

\V

        (1) Every grade-school student is taught about the decimal representation of numbers. For instance,
        \begin{displaymath}
        \frac{1}{3} \,=\, 0.333333\,{\ldots}\,,
        \end{displaymath}
    where the string of `dots' indicates that the expression on the right side of the equation `goes on forever'.

        What does the expression $0.333333\,{\ldots}\,$ mean? For example, the {\em finite} expression $0.333$ is a shorthand for a certain {\em finite}  sum:
        \begin{displaymath}
        0.333 \,=\, \frac{3}{10} + \frac{3}{100} + \frac{3}{1000}
        \end{displaymath}
    In the same spirit, the `infinite decimal' $0.333333\,{\ldots}\,$ should indicate the corresponding{\em infinite} sum:
        \begin{displaymath}
        0.333333\,{\ldots}\, \,=\, \frac{3}{10} + \frac{3}{10^{2}} + \frac{3}{10^{3}} + \,{\ldots}\,+ \frac{3}{10^{k}} + \,{\ldots}\,
        \end{displaymath}
    That is, one can express the fraction $1/3$ as an infinite sum:
        \begin{displaymath}
        \frac{1}{3} \,=\, \frac{3}{10} + \frac{3}{10^{2}} + \frac{3}{10^{3}} + \,{\ldots}\,+ \frac{3}{10^{k}} + \,{\ldots}\,
        \end{displaymath}

\V

        (2) In Section~\Ref{SectE60} we saw that if $f:{\RR} \,{\rightarrow}\, {\RR}$ is a $C^{{\infty}}$ function and $c$ is a number in ${\RR}$,
    then one can write the Taylor Approximation
        \begin{displaymath}
        f(x) \,=\, f(c) + f'(c)(x-c) + \,{\ldots}\,+ \frac{f^{(k)}(c)}{k!} + E_{k}(x),
        \end{displaymath}
    where the remainder $E_{k}(x)$ is given by Taylor's Formula with Remainder.
    We also saw in a few cases (the exponential, sine and cosine functions with $c \,=\, 0$) that $\lim_{k \,{\rightarrow}\, {\infty}} E_{k}(x) \,=\, 0$.
    Such examples lead one to consider the possibility of getting an exact formula for $f(x)$ as an `infinite sum' by letting $k \,{\rightarrow}\, {\infty}$, thereby eliminating the error $E_{k}$ entirely by pushing it `off to infinity':
        \begin{displaymath}
        f(x) \,=\, f(c) + f'(c)(x-c) + \frac{f''(c)}{2}(x-c)^{2} + \,{\ldots}\,+ \frac{f^{(k)}(c)}{k!} + \,{\ldots}\,
        \end{displaymath}
    For instance, the analysis carried out in Theorem~\Ref{ThmE60.85} suggests the following formulas:
        \begin{displaymath}
        e^{x} \,=\, 1+x+\frac{x^{2}}{2!} + \frac{x^{3}}{3!} + \,{\ldots}\,\frac{x^{k}}{k!} + \,{\ldots}\,
        \end{displaymath}
        \begin{displaymath}
        {\sin}\,x \,=\, x - \frac{x^{3}}{3!} + \frac{x^{5}}{5!} - \frac{x^{7}}{7!} + \,{\ldots}\, + (-1)^{k-1}\frac{x^{2k-1}}{(2k-1)!} + \,{\ldots}\,
        \end{displaymath}
        \begin{displaymath}
        {\cos}\,x \,=\, 1 - \frac{x^{2}}{2!} + \frac{x^{4}}{4!} - \frac{x^{6}}{6!} + \,{\ldots}\,+ (-1)^{k-1}\frac{x^{2k-2}}{(2k-2)!} + \,{\ldots}\,
        \end{displaymath}
    The use of such `infinite sums' has been one of the primary tools of calculus from the earliest days.
    Indeed, as the book title referred to in Chapter Quote~(4) hints, Newton's concept of we now call a `function' seemed to include that it can be expressed by an infinite sum of simple functions.


\V

        Needless to say, we could consider many more such examples from applied mathematics;
    but to save time let us simply assume that we all agree it is important to consider infinite sums in mathematics,
    and then move on to describe what such a sum might mean.

\V
\V

        There are two main approaches to the theory of infinite sums.

\V

        (A) \underline{The `Ordered Sum' Approach} One thinks of an `infinite sum' as the result of successively adding numbers, one after the other,
        -- hence the name `Ordered Sum' -- until all the numbers to be added are exhausted.
    This is the natural extension of the idea of `finite ordered sum' described  in Definition~\Ref{DefB10.30}, except that the additions need not stop.
    Of course with this approach one needs to impose on the numbers being added the order in which the addition is to take place.

        The example ${\displaystyle \frac{1}{3} \,=\, \frac{3}{10} + \frac{3}{10^{2}} + \frac{3}{10^{3}} + \,{\ldots}\,+ \frac{3}{10^{k}} + \,{\ldots}\,
}$ given above illustrates the analog, for infinite sums, of the `ordered sum' approach:
    one orders the numbers being added in terms of the exponents $k$ in the denumerators of the fractions $3/10^{k}$.

\VA

        (B) \underline{The `Unordered Sum' Approach} In this approach one need not impose an order on the numbers being added.%\\\

\V

        To make the distinction between these approaches clearer to the intuitiion, consider the following finite situation:

        One has a nonempty box $B$ containing $n$ distinct physical objects, with each object $x$ in the box having positive physical mass~$m(x)$.


        \underline{Problem} Determine the total mass $M$ of the objects in the box~$B$.

        \h \underline{Solution 1} Remove the $n$ objects in the box $B$ in some order: $x_{1}$ is the first object removed, $x_{2}$ is the second ,\,{\ldots}\,$x_{n}$ is the final one removed.
    Then form the (finite) ordered sum $m(x_{1}) + m(x_{2}) + \,{\ldots}\, + m(x_{n})$, as in Definition~\Ref{DefB10.30}. The result is the desired mass~$M$.

\VA

        \h  \underline{Solution 2} Place the entire box $B$ on, say, the left side of a balance scale; adjust for the weight of the box when empty.
    Then place enough standard-sized weights on the right side to overcome the weight of the box on the left.
    Replace standard weights on the right side by less heavy standard weights, and keep doing this until the two sides come into balance.
    Note that this `weighing' procedure does not require that one list out the objects in the box~$B$,
    or even to know the number~$n$ or the individual weights of these objects.

\VA

        Note that Solution 1 uses an ordered sum, but in fact the choice of the order in which the the objects are removed from the box is irrelevant.

\VV

        In elementary calculus one normally studies infinite `Ordered Sums' in some depth, but `Unordered Sums' not at all.
    However, the theory for `Unordered Sum' is actually simpler, so in {\TheseNotes} we begin with it.

\V
\V

        \underline{Final Note} It turns out that the theory of `infinite sums of real numbers' provides us, more or less for free,
    a corresponding theory for `infinite products of real numbers'.
    This is why the phrase `and Products' is included in the title of this chapter.
    That phrase is in parentheses, however, because the `infinite products' portion plays a much less prominent role in {\TheseNotes} than does the `infinite sums' part.
    The `products' treatment is largely relegated to the exercises.

\VV

%--------------------------

            \section{\small{\bf Finite Unordered Sums}}
            \label{SectG23}

\V


        The theory of unordered infinite sums to be presented here is based on properties of {\em finite} sums.
            Thus it is useful to reformulate the finite theory in a way that makes the generalization to infinite sums simpler.

        Recall that in Definition~\Ref{DefB10.30} the ordered sum of $k$ real numbers begins with an ordered $k$-tuple $(x_{1}, x_{2}, \,{\ldots\, x_{k)}}$ of real numbers.
    By Definition~\Ref{DefA30.30}, such a $k$-tuple is a real-valued function $f:{\NN}_{k}\,{\rightarrow}\,{\RR}$ from the set ${\NN}_{k}$ into~${\RR}$;
    the `orderedness' of the corresponding ordered sum $\sum_{j=1}^{k} x_{j}$ derives from the standard ordering on the domain ${\NN}_{k}$ of the function~$f$.
    The simplest way to formally introduce the idea of an `unordered' finite sum of $k$ real numbers is to replace the ordered set ${\NN}_{k}$ by a general set $X$, not necessarily ordered in any standard way, with~$k$ elements.
    
\V

            \subsection{\small{\bf Definition}}
            \label{DefB10.50}

        Let $X$ be a finite nonempty set with exactly $k\,\,{\geq}\,\,2$ elements, and let $f:X \,{\rightarrow}\, {\RR}$ be a real-valued function whose domain contains $X$ as a subset.

\V

       (1) The {\bf (unordered) sum of the function values of $f$ over the set $X$} is the number $\sum_{X} f$ given by
        \begin{displaymath}
       \sum_{X} f \,=\, f(g(1)) + f(g(2)) + \,{\ldots}\,+ f(g(k)),
        \end{displaymath}
    where $g$ is any bijection of $\{1,2,\,{\ldots}\,k\}$ onto $X$. It is sometimes convenient to use an `index' notation such as $\sum_{x{\in}X} f(x)$ instead of $\sum_{X} f$.

\V

        (2) The {\bf product of the function values of $f$ over $X$} is the number $\prod_{X} f$ given by
        \begin{displaymath}
      {\prod}_{X} f \,=\, f(g(1)) {\cdot} f(g(2)) {\cdot} \,{\ldots}\,{\cdot} f(g(k)).
        \end{displaymath}
    As with unordered sums, it is sometimes convenient to use an `index' notation, such as $\prod_{x{\in}X} f(x)$, instead of $\prod_{X} f$.

\V

        (3) If $X$ is a singleton set, $X \,=\, \{c\}$, then it is convenient to set ${\sum}_{X} f \,=\, {\prod}_{X} f \,=\, f(c)$.
    Likewise, it is convenient to write $\sum_{{\emptyset}} f \,=\, 0$ and ${\prod}_{{\emptyset}} f \,=\, 1$ for every function $f$.


\V

            \subsection{\small{\bf Remarks}}
            \label{RemrkB10.53}

\V

\hspace*{\parindent}(1) The conventions that $\sum_{{\emptyset}} f \,=\, 0$ and ${\prod}_{{\emptyset}} f \,=\, 1$ do not conflict with the requirement that the domain of a function must be a nonempty set.
    Indeed, in the preceding definition it is required only that the set $X$ over which the sum and product are to be taken be a {\em subset} of the domain of the function $f$.
    The empty set fulfills that requirement automatically.

\V

        (2) Theorem~\Ref{ThmB10.32} shows that the expressions ${\sum}_{X} f$ and ${\prod}_{X} f$ depend only on $f$ and $X$,
    but not on the specific choice of bijection $g$; that is, they do not depend on the order in which one adds or multiplies the values of $f$.
        Note that is the {\em expressions} -- that is, the manner in which the sums and products are written down -- that are being distinguished as either `ordered' or `unordered' here, not the {\em values} assigned to these expressions;
    indeed, the main conclusion of Theorem~\Ref{ThmB10.32} is that the values are the same, at least in the case of finite sums.
    We shall see, however, that in the generalization to infinite sums the commutative law is not true in general.


\V

        (4) In the special case $X \,=\, {\NN}_{k}$ the notations for the two types of sums take the form ${\sum}_{j{\in}{\NN}_{k}} f(j)$ (unordered sum) and $\sum_{j=1}^{k} f(j)$ (ordered sum).
   When using the former notation we ignore the standard order on ${\NN}_{k}$; when using the latter notation we use the standard order.

\V
\V

        Needless to say, the commutative law does not have much significance in the context of unordered sums.
    In contrast, the associative laws for addition and multiplication also have versions which apply to the unordered situation. %\\\

\V

            \subsection{\small{\bf Theorem} (Generalized Associative Laws for Unordered Finite Sums and Products)}
            \label{ThmB10.60}

        Let $f:X \,{\rightarrow}\, {\RR}$ be a real-valued function defined on a nonempty finite set $X$ with exactly $k$ elements.
    Let $C_{1}$, $C_{2}$,\,{\ldots}\,$C_{r}$ be nonempty subsets of $X$ that are mutually disjoint and whose union is $X$.
    Then
        \begin{equation}
        \label{EqnB.20A}
        {\sum}_{X} f \,=\, ({\sum}_{C_{1}} f) + ({\sum}_{C_{2}}) f + \,{\ldots}\, + ({\sum}_{C_{r}} f)
        \end{equation}
    and
        \begin{equation}
        \label{EqnB.20B}
        {\prod}_{X} f \,=\, ({\prod}_{C_{1}} f){\cdot}({\prod}_{C_{2}} f){\cdot}\,{\ldots}\,{\cdot}({\prod}_{C_{r}} f).
        \end{equation} %\\\
 
\V

        {\bf Proof} We shall verify Equation~\Ref{EqnB.20A}; the proof for Equation~\Ref{EqnB.20B} is similar, and is left as an exercise.

        Let $A$ be the set of all $r$ in ${\NN}$ for which Equation~\Ref{EqnB.20A} is true.

        Clearly $1{\in}A$; indeed, if $r \,=\, 1$ then $C_{r} \,=\, C_{1} \,=\, X$, and the condition to be proved reduces to
    ${\sum}_{X} f \,=\, {\sum}_{C_{1}} f$, which is true because $C_{1} \,=\, X$.

        Next, suppose that $r \,=\, 2$, and suppose that $C_{1}$ and $C_{2}$ are nonempty mutually disjoint subsets of $X$ such that $X \,=\, C_{1}\,{\cup}\,C_{2}$.
    For convenience let $Y \,=\, C_{1}$ and $Z \,=\, C_{2}$.
    Let $p$ be the number of points in $Y$, and let $y_{1}$, $y_{2}$,\,{\ldots}\,$y_{p}$ be these points.
    Likewise, let $q$ be the number of points of $Z$, and let $z_{1}$, $z_{2}$,\,{\ldots}\,$z_{q}$ be these points.
    By Part~(c) of Theorem~\Ref{ThmA15.30}, one then has $p+q \,=\, k$.
    Define $g:{\NN}_{k} \,{\rightarrow}\, X$ by the rule
        \begin{displaymath}
        g(j) \,=\,
        \left\{
        \begin{array}{ll}
        y_{j}    & \mbox{if $1\,\,{\leq}\,\,j\,\,{\leq}\,\,p$} \\
        z_{j-p}  & \mbox{if $p+1\,\,{\leq}\,\,j\,\,{\leq}\,\,k$} \,=\, p+q.
        \end{array}
        \right.
        \end{displaymath}
    Then it is clear that $g$ is a bijection of ${\NN}_{k}$ onto $X$.
    To simplify the notation, let us write $x_{j} \,=\, g(j)$ for all $j$ in ${\NN}_{k}$.
    Thus one has
        \begin{displaymath}
        x_{1} \,=\, y_{1}, \,x_{2} \,=\, y_{2},\,{\ldots}\,x_{p} \,=\, y_{p},\,x_{p+1} \,=\, z_{1},\, x_{p+1} \,=\, z_{2},\,{\ldots}\,x_{k} \,=\, x_{p+q} \,=\, z_{q}.
        \end{displaymath}
    By definition one has
        \begin{displaymath}
        {\sum}_{X} f \,=\, x_{1} + x_{2} + \,{\ldots}\, + x_{k}.
        \end{displaymath}
    Now from Part (b) of Theorem~\Ref{ThmB10.32} it follows that
        \begin{displaymath}
        {\sum}_{X} f \,=\, x_{1} + x_{2} + \,{\ldots}\, + x_{p} \,=\, (x_{1} + x_{2} + \,{\ldots}\, + x_{p}) + (x_{p+1} + x_{p+2} + \,{\ldots}\, + x_{p+q})
     \,=\,
         {\sum}_{Y} f +  {\sum}_{Y} f,
        \end{displaymath}
    as required.  In other words, $r \,=\, 2$ is in the set $A$.

        Next, suppose that $r{\in}A$, with $r\,\,{\geq}\,\,2$, and suppose that $C_{1}$, $C_{2}$,\,{\ldots}\,$C_{r}$, $C_{r+1}$ are mutually disjoint nonempty subsets of $X$ whose union is $X$.
    Let $Y \,=\, C_{1}\,{\cup}\,C_{2}\,{\cup}\,\,{\ldots}\,\,{\cup}\,C_{r}$ and $Z \,=\, C_{r+1}$.
    Then the fact that $2$ is in $A$ implies that
        \begin{displaymath}
        {\sum}_{X} f \,=\, {\sum}_{Y} f + {\sum}_{Z} f.
        \end{displaymath}
    And the induction hypothesis that $r$ is in $A$ implies that
        \begin{displaymath}
        {\sum}_{Y} f \,=\, {\sum}_{C_{1}} f + {\sum}_{C_{2}} f + \,{\ldots}\,+ {\sum}_{C_{r}} f.
        \end{displaymath}
    Combining these facts with the definition of ordered finite sums, one finally obtains
        \begin{displaymath}
        {\sum}_{X} f \,=\, \left({\sum}_{C_{1}} f + {\sum}_{C_{2}} f + \,{\ldots}\,+ {\sum}_{C_{r}} f\right) + {\sum}_{C_{r+1}} f \,=\, 
    {\sum}_{C_{1}} f + {\sum}_{C_{2}} f + \,{\ldots}\,+ {\sum}_{C_{r}} f + {\sum}_{C_{r+1}} f.
        \end{displaymath}
    In other words, $r+1$ is also in $A$. Now the induction is complete, and $A \,=\, {\NN}$ and the desired result follows.

\V
\V


        {\bf Example} Suppose that $X \,=\, \{a,b,c\}$ is a set with exactly three elements.
    Let $C_{1} \,=\, \{a\}$ and $C_{2} \,=\, \{b,c\}$.
    Then Equation~\Ref{EqnB.20A} takes the form
        \begin{displaymath}
        {\sum}_{X} f \,=\, {\sum}_{C_{1}} f + {\sum}_{C_{2}} f;
        \end{displaymath}
    that is,
        \begin{displaymath}
        {\sum}_{X} f \,=\, f(a) + (f(b)+f(c)).
        \end{displaymath}
    Likewise, if one applies Equation~\Ref{EqnB.20A} to the sets $C_{1}' \,=\, \{a,b\}$ and $C_{2}' \,=\, \{c\}$, one gets
        \begin{displaymath}
        {\sum}_{X} f \,=\, {\sum}_{C_{1}'} f + {\sum}_{C_{2}'} f \,=\, (f(a)+f(b)) + f(c).
        \end{displaymath}
    In particular,
        \begin{displaymath}
        f(a) + (f(b)+f(c)) \,=\, (f(a)+f(b))+f(c).
        \end{displaymath}
    If one sets $x \,=\, f(a)$, $y \,=\, f(b)$ and $z \,=\, f(c)$, then this  result takes the more familiar form
        \begin{displaymath}
        x+(y+z) \,=\, (x+y)+z.
        \end{displaymath}

        Similar applications of Equation~\Ref{EqnB.20B} imply
        \begin{displaymath}
        f(a){\cdot}(f(b){\cdot}f(c)) \,=\, (f(a){\cdot}f(b)){\cdot}f(c)
        \end{displaymath}
    In other words, the simplest nontrivial case of this theorem reduces to the usual Associative Laws for Addition and Multiplication.
    It is for this reason the the results of Theorem~\Ref{ThmB10.60} are called `{\em Generalized} Associative Laws for Unordered Finite Sums'.

\V
\V

            \subsection{\small{\bf Example}}
            \label{ExampB10.70}

        Consider the equation
        \begin{displaymath}
        1+(4+1)+(3+2) \,=\, 1+1+2+3+4 \h ({\ast})
        \end{displaymath}
    Proving this directly from the axioms could be time consuming. Here is how to get it from the preceding theorems.

        First, note that there are $5$ terms in the expression on the left, so let $X$ be a set with exactly $5$ elements.
    To be definite, let $X \,=\, \{a,b,c,d,e\}$. Define $f:X \,{\rightarrow}\, {\RR}$ by the rule $f(a) \,=\, 1$, $f(b) \,=\, 4$, $f(c) \,=\, 1$, $f(d) \,=\, 3$, $f(e) \,=\, 2$.
    Then note that
        \begin{displaymath}
        X \,=\, C_{1}\,{\cup}\,C_{2}\,{\cup}\,C_{3},
        \end{displaymath}
    where $C_{1} \,=\, \{a\}$, $C_{2} \,=\, \{b,c\}$, $C_{3} \,=\, \{d,e\}$.
    Note that the left side of Equation~$({\ast})$ equals
        \begin{displaymath}
        ({\sum}_{C_{1}} f)+ ({\sum}_{C_{2}} f) + ({\sum}_{C_{3}} f).
        \end{displaymath}
    Likeswise,
        \begin{displaymath}
        {\sum}_{X} f \,=\, f(a) + f(b) + f(c) + f(d) + f(e) \,=\, 1+4+1+3+2 \,=\, 1+1+2+3+4,
        \end{displaymath}
    where the last equation follows from the General Commutative Law for Addition.
    The desired equation now follows by applying the Generalized Associative Law.


\V
\V

            \subsection{\small{\bf Remark}}
            \label{RemrkB10.80}

        The careful reader may be bothered by the fact that the right side of Equation~\Ref{EqnB.20A} is written as an `ordered sum', while the left side is an `unordered sum';
    see Remark~\Ref{RemrkB10.53}~(1) for the meanings of these  phrases.
    This (minor) aesthetic flaw can easily be fixed by observing that the sets $C_{1}$, $C_{2}$,\,{\ldots}\,$C_{r}$ discussed in Theorem~\Ref{ThmB10.60}
    form a {\em partition} of the original set $X$; see Definition~\Ref{DefA50.85}~(1).
    More precisely, let ${\cal F} \,=\, \{C_{1},\,{\ldots}\,C_{r}\}$; then ${\cal F}$ is the partition in question.
    This suggests the following reformulation of Theorem~\Ref{ThmB10.60}; the simple proof is left as an exercise for the reader.

\V

            \subsection{\small{\bf Theorem} (Alternate Formulation of the Generalized Associative Laws for Unordered Finite Sums)}
            \label{ThmB10.90}

        Let $f:X \,{\rightarrow}\, {\RR}$ be a real-valued function defined on a nonempty finite set $X$ with exactly $k$ elements.
    Let ${\cal F}$ be a partition of $X$, and define a function $\hat{f}:{\cal F} \,{\rightarrow}\, {\RR}$ by the rule
        \begin{displaymath}
        \hat{f}(C) \,=\, {\sum}_{C} f \mbox{ for each $C$ in the family ${\cal F}$}.
        \end{displaymath}
    Then
        \begin{equation}
        \label{EqnB.30A}
        {\sum}_{X} f \,=\, {\sum}_{{\cal F}} \hat{f}.
        \end{equation}

        Likewise, let $\tilde{f}:{\cal F} \,{\rightarrow}\, {\RR}$ be given by the rule
        \begin{displaymath}
        \tilde{f}(C) \,=\, {\prod}_{C} f \mbox{ for each $C$ in ${\cal F}$}.
        \end{displaymath}
    Then
        \begin{equation}
        \label{EqnB.30B}
        {\prod}_{X} f \,=\, {\prod}_{{\cal F}} \tilde{f}.
        \end{equation}

\V
\V

            \subsection{\small{\bf `Real-life' Example}}
            \label{ExampB10.95}

        \underline{The Story Line} A certain person, whom we call `Pat', owns a chain of three stores in Chicago.
    The following table shows the net profit for each store during the six-month period January~1 through June~30 of a certain year:
        \begin{displaymath}
        \begin{array}{c|rrrrrr}  \hline
        \mbox{Month}   &
  \mbox{Jan} & \mbox{Feb} & \mbox{Mar} & \mbox{Apr} & \mbox{May} & \mbox{Jun} \\
        \mbox{Store A} &
       1213  &   1502     &    1101    &    987     &    322     &  -206      \\
        \mbox{Store B} &
       -433  &   -106     &     224    &    783     &   1200     &  1305      \\
        \mbox{Store C} &
        213  &    714     &     949    &    851     &    770     &   312      \\
        \end{array}
        \end{displaymath}

        \underline{Problem} Determine the net profit that Pat received from the chain of stores during this period.

        \underline{Solution} First note that the entire  situation can be  formulated along the lines of the previous discussion.
    Indeed, let $X$ be the set of all ordered pairs $(u,v)$ where $u$ is one of the three store letters $A$, $B$, $C$, and $v$ is one of the six months Jan, Feb, Mar, Apr, May, Jun.
    Next, define $f:X \,{\rightarrow}\, {\RR}$ by the rule $f(u,v)$ is the net profit of Store~$u$ during Month~$v$.
    Then it is clear that the number Pat wishes to know is ${\sum}_{X} f$. 
    This quantity is, by its nature, an `unordered sum', since there is no uniquely natural way to list out the $18$ numbers of the table to form a single ordered sum.
    However there are two obvious ways to `partition' the data in the table:

        \underline{Partition by `Store'} Let $S_{A}$ be the subset of $X$  corresponding to the six data points associated with Store~A. That is
        \begin{displaymath}
        S_{A} \,=\, \{(A,\mbox{Jan}), (A,\mbox{Feb}),\,{\ldots}\,(A,\mbox{Jun})).
        \end{displaymath}
    Likewise, let $S_{B}$ and  $S_{C}$ denote the subsets of $X$ corresponding to Stores~B and~C, respectively.
    Clearly the sets $S_{A}$, $S_{B}$ and $S_{C}$ form a partition ${\cal F} \,=\, \{S_{A},S_{B},S_{C}\}$ of $X$.
    Note that the corresponding function $\hat{f}:{\cal F} \,{\rightarrow}\, {\RR}$ is then given by
        \begin{displaymath}
        \hat{f}(S_{A}) \,=\, {\sum}_{S_{A}} f \,=\, \mbox{ the net profit of Store~A},
        \end{displaymath}
    and likewise for $\hat{f(S_{B})}$ and $\hat{f}(S_{C})$.
    Theorem~\Ref{ThmB10.90} then takes the form
        \begin{displaymath}
        {\sum}_{X} f \,=\, {\sum}_{{\cal F}} \hat{f}.
        \end{displaymath}
    This is then interpreted as saying that the net profit over the period for the entire chain is the sum of the profits of the individual stores over that period.

        In terms of the `table' notation used here: ${\sum}_{{\cal F}} \hat{f}$ calculates the sum of the entries of the table `row-by-row'.

\V

        \underline{Partition by `Month'} Let $T_{\mbox{Jan}}$ be the subset of $X$ corresponding to the three data points associated with the month of January.
    That is,
        \begin{displaymath}
        T_{\mbox{Jan}} \,=\, \{(A,\mbox{Jan}), (B,\mbox{Jan}), (C,\mbox{Jan})\}.
        \end{displaymath}
    Likewise, let $T_{\mbox{Feb}}$,\,{\ldots}\,$T_{\mbox{Jun}}$ denote the subsets of $X$ corresponding to February, March etc.
    Clearly the family ${\cal G}$, given by
        \begin{displaymath}
        {\cal G} \,=\, \{T_{\mbox{Jan}}, T_{\mbox{Feb}}, T_{\mbox{Mar}},
        T_{\mbox{Apr}}, T_{\mbox{May}}, T_{\mbox{Jun}}\},
        \end{displaymath}
    forms a second partition of $X$.
    Now define $f^{\#}:{\cal G} \,{\rightarrow}\, {\RR}$ by the rule
        \begin{displaymath}
        f^{\#}(T_{\mbox{Jan}}) f \,=\, {\sum}_{T_{\mbox{Jan}}} \,=\, \mbox{ the sum of the net profits from each of the three stores for January},
        \end{displaymath}
    and likewise for the other five months.
    In this case Theorem~\Ref{ThmB10.90} takes the form
        \begin{displaymath}
        {\sum}_{X} f \,=\, {\sum}_{{\cal G}} f^{\#}.
        \end{displaymath}
    This is then interpreted as saying that the net profit of the chain over the given six-month period is equal to the sum of the net profits for each of these months of the chain.

        In terms of the `table' notation used here: ${\sum}_{{\cal G}} f^{\#}$ calculates the sum of the entries of the table `column-by-column'.

        \underline{Practical Note} It is clear that the quantities ${\sum}_{{\cal F}} \hat{f}$ and ${\sigma}_{{\cal G}} f^{\#}$ must equal each other, since they are both equal to ${\sum}_{X} f$.
    Acountants use the fact that adding row-by-row ought to produce the same results as adding column-by-column as a way to (partially) verify the accuracy of their calculations.

\V
\V

        {\bf Remark} The formulation of the `General Associative Law' in Theorem~\Ref{ThmB10.90}, i.e., in terms of partitions,
    may seem artificial.
    %% TEMPORARY REFERENCE ~\Ref{ChaptG}
    However, we need to use an analog of this formulation in our treatment of `infinite sums' in Chapter~G.
    Also, such `partition' formulations appear frequently in other branches of advanced mathematics.

\V
\V

        The Generalized Commutative and Associative Laws can be used to prove the following facts which, although simple, are surprisingly useful.

\V

        \subsection{\small{{\bf Theorem}}}
        \label{ThmB10.100}

\V

\hspace*{\parindent}(a) Let $a$ and $b$ be real numbers. If $x_{1}$, $x_{2}$,\,{\ldots}\, $x_{k}$, are real numbers, then
        \begin{displaymath}
        b-a \,=\, (b-x_{k}) + (x_{k}-x_{k-1}) + (x_{k-1}-x_{k-2}) + \,{\ldots}\, + (x_{2}-x_{1}) + (x_{1}-a).
        \end{displaymath}

\V

        (b) Let $c$ and $d$ be nonzero real numbers. If $y_{1}$, $y_{2}$,\,{\ldots}\, $y_{k}$ are nonzero real numbers, then
        \begin{displaymath}
        \frac{d}{c} \,=\, \left(\frac{d}{x_{k}}\right){\cdot} 
                          \left(\frac{x_{k}}{x_{k-1}}\right){\cdot}
                          \left(\frac{x_{k-1}}{x_{k-2}}\right){\cdot}
        \,{\ldots}\,
                          \left(\frac{x_{2}}{x_{1}}\right){\cdot}
                          \left(\frac{x_{1}}{a}\right)
        \end{displaymath}

\V

        {\bf Proof}

\V

        (a) For convenience set $x_{0} \,=\, a$ and $x_{k+1} \,=\, b$, so the equation to be proved takes the form
        \begin{displaymath}
        x_{k+1}-x_{0} \,=\, (x_{k+1}-x_{k}) + (x_{k}-x_{k-1}) + (x_{k-1}-x_{k-2}) + \,{\ldots}\, + (x_{2}-x_{1}) + (x_{1}-x_{0}) \h ({\ast})
        \end{displaymath}
    Note that the expression on the right side of Equation~$({\ast})$ involves $2k+2$ indexed quantities:
        \begin{displaymath}
        x_{0}, x_{1},\,{\ldots}\,x_{k},x_{k+1}, -x_{1},\,{\ldots}\,-x_{k}
        \end{displaymath}
    (Recall that a term such as $x_{2}-x_{1}$ really is an addition, namely $x_{2} + (-x_{1})$.)
    There does not appear to be a completely obvious choice of labeling set $X$ and labeling function $f$ for this situation.
    Based on the order in which the terms are written on the right side of Equation~$({\ast})$,
    we choose $X \,=\, \{0,1,2,\,{\ldots}\,2k+1\}$ as the labeling set, and define the labeling function $f:X \,{\rightarrow}\, {\RR}$ by the rule
        \begin{displaymath}
        f(j) \,=\, \left\{
        \begin{array}{rl}
        x_{k+1-j} & \mbox{if $0\,\,{\leq}\,\,j\,\,{\leq}\,\,k$} \\
       -x_{2k+1-j} & \mbox{if $k+1\,\,{\leq}\,\,j\,\,{\leq}\,\,2k+1$}
        \end{array}
                            \right.
        \end{displaymath}
    This labeling corresponds to writing the numbers that appear in the desired sum in the following order:
        \begin{displaymath}
        x_{k+1}, x_{k}, x_{k-1},\,{\ldots}\,x_{2},x_{1}, -x_{k}, -x_{k-1},\,{\ldots}\,-x_{2}, -x_{1}, -x_{0}.
        \end{displaymath}
    Now let $C_{0}$, $C_{2}$,\,{\ldots}\,$C_{k}$ be the subsets of $X$ that correspond under $f$ to indices which appear in the differences in the parentheses on the right side of~$({\ast})$:
        \begin{displaymath}
        C_{0} \,=\, \{0,k+1\}, \h C_{1} \,=\, \{1,k+2\}, \h C_{2} \,=\, \{2, k+3\}, \h  \,{\cdots}\, C_{k-1} \,=\, \{k-1, 2k\}, \h C_{k}  \,=\, \{k,2k+1\}
        \end{displaymath}
    The general rule is $C_{j} \,=\, \{j, k+1+j\}$ for $0\,\,{\leq}\,\,j\,\,{\leq}\,\,k$.
    Note that $f(j) + f(k+1+j) \,=\, x_{k+1-j} + (-x_{k-j}) \,=\, x_{k+1-j}-x_{k-j}$.
    Thus the right side of Equation~$({\ast})$ equals
        \begin{displaymath}
        {\sum}_{C_{0}} f + {\sum}_{C_{1}} f + \,{\ldots}\, + {\sum}_{C_{k}} f.
        \end{displaymath}
    Since the sets $C_{0}$, $C_{1}$,\,{\ldots}\,$C_{k}$ are disjoint nonempty subsets of $X$ whose union is $X$, it then follows from the Generalized Associative Law for Addition that the right side of Equation~$({\ast})$ equals ${\sum}_{X} f$.

        Next, define new subsets $C_{0}'$, $C_{1}'$,\,{\ldots}\,$C_{k}'$ of $X$ as follows:
        \begin{displaymath}
        C_{0}' \,=\, \{0,2k+1\}, \h C_{1}' \,=\, \{1,k+1\}, \h C_{2}' \,=\, \{2,k+2\},\,{\ldots}\,C_{k}' \,=\, \{k,2k\}.
        \end{displaymath}
    Much as above, one sees that $f(0)+f(2k+1) \,=\, x_{k+1} - x_{0} \,=\, b-a$, $f(1)+f(k+1) \,=\, x_{k}-x_{k} \,=\, 0$, $f(2)+f(k+2) \,=\, x_{k-1}-x_{k-1} \,=\, 0$,\,{\ldots}\,$f(k)+f(2k) \,=\, x_{1}-x_{1} \,=\, 0$.
    Thus the sum
        \begin{displaymath}
        {\sum}_{C_{0}'} f + {\sum}_{C_{1}'} f + \,{\ldots}\,+{\sum}_{C_{k}'} f 
        \end{displaymath}
    equals the sum $(b-a) + 0 + 0 + \,{\ldots}\,+ 0$; that is, the value is~$b-a$, which of course is the {\em left} side of  Equation~$({\ast})$.
    But it is clear that the sets $C_{0}'$, $C_{1}'$,\,{\ldots}\, $C_{k}'$ are also nonempty mutually disjoint subsets of $X$ whose union is $X$.
    Thus the Generalized Associative Law for Addition applies once again to imply that the left side of Equation~$({\ast})$ also equals ${\sum}_{X} f$.


        Since, as just been shown, both sides of Equation~$({\ast})$ equal the same quantity, namely ${\sum}_{X} f$,
    it follows that the two sides of this equation equal each other; which is a fancy way of saying that the equation is true.

\V

        (b) The proof of this part is similar, and is left as an exercise.


\V
\V

        {\bf Remark} These equations should bring back happy memories from high-school algebra:
    they form the basis for the classic `Add-and-Subtract' Trick (Part~(a)) and the `Multiply-and-Divide Trick' (Part~(b)).

\V
\V


        The preceding results keep addition and multiplication separated. The next result combines both operations.

\V

            \subsection{\small{\bf Theorem} (Generalized Distributive Law)}
            \label{ThmB10.110}

\V

        Suppose that $f_{1},f_{2},\,{\ldots}\,f_{m}:X \,{\rightarrow}\, {\RR}$ are real-valued functions defined on a nonempty finite set $X$,
    and that $c_{1}$, $c_{2}$,\,{\ldots}\,$c_{m}$ are real numbers. Then:
        \begin{equation}
        \label{EqnB.10}
        {\sum}_{X} (c_{1}f_{1} + c_{2}f_{2} + \,{\ldots}\,+ c_{m}f_{m}) \,=\, 
    c_{1}{\cdot}{\sum}_{X} f_{1} + c_{2}{\cdot}{\sum}_{X} f_{2} + \,{\ldots}\,+ c_{m}{\cdot}{\sum}_{X} f_{m}
        \end{equation}
        In particular, one has as the following: ${\sum}_{X} (-f) \,=\, -{\sum}_{X} f$.

\V

        The simple proof is left as an exercise.

\V

        To see how this result generalizes the usual Distributive Law (see Axiom~A3),
    consider the special case in which $m \,=\, 1$ and the set $X$ has exactly two elements: $X \,=\, \{a,b\}$.
    Then on the left side of the equation one has
        \begin{displaymath}
        {\sum}_{X} c_{1}f_{1} \,=\, c_{1}f_{1}(a) + c_{1}f_{1}(b),
        \end{displaymath}
    while on the right side one has
        \begin{displaymath}
        c_{1}{\sum}_{X} f_{1} \,=\, c_{1}(f_{1}(a)+f_{1}(b)).
        \end{displaymath}
    The resulting  equality $c_{1}f_{1}(a) + c_{1}f_{1}(b) \,=\, c_{1}(f_{1}(a)+f_{1}(b))$ is the Distributive Law.

\V
\V

        The preceding calculations illustrate an important feature of proving well-known facts directly from the axioms:
    the process, although certainly instructive, can be quite tedious.
    It is also quite annoying, since we are proving results that we have been using, sometimes unconsciously, almost all our lives.
    From this point on, however, we shall follow the usual custom and normally leave such low-level proofs to the reader.
    Sometimes these proofs will be omitted without comment, sometimes with a blithe statement such as `Now simplify to get \,{\ldots}\, '.
    In any event, the reader is left the task of filling in the gaps in such calculations, often by invoking the Generalized Commutative, Associative and Distributive Laws.
    

\V
\V
\V


\begin{quotation}
{\footnotesize \underline{Pedagogical Comment}
    The axioms listed above all involve standard algebraic facts about real numbers that everyone has known -- and used, often without realizing it -- since grade school.
    Because of that familiarity, there is a tendency to think of these axioms as `obvious' or even `trivial';
    Chapter Quote~$1$ suggests a reason for this tendency.

        Nevertheless, the rather bland presentation of these axioms does hide some nonobvious facts.
    For example, consider the following equation:
        \begin{displaymath}
        25350{\times}47 \,=\, 325{\times}3666 \h ({\ast})
        \end{displaymath}
    The odds are high that most people would not find this equation as being `obviously true';
    indeed, most would have to verify it by simply performing the indicated multiplications and noting that each ends up equaling~$1191450$.
    However, one easily checks that $25350 \,=\, 325{\cdot}78$ and $3666 \,=\, 78{\cdot}47$,
     so that Equation~$({\ast})$ is just a rewriting of the following special case of the Associative Law for Multiplication:
        \begin{displaymath}
        (325{\cdot}78){\cdot}47 \,=\, 325{\cdot}(78{\cdot}47)
        \end{displaymath}
    If the special case $({\ast})$ of this Associative Law is not `obvious', then it would appear that the general Associative Law should be even less `obvious'.

        So why do we accept Axiom~A2 as `obvious'? Almost certainly {\em not} because we have tried out this axiom on dozens of triples $x$, $y$ and $z$ and verified that it works,
    much as we just did with Equation~$({\ast})$.
    The likely answer to this question is that we accept this axiom because in grade  school `Teacher said it is so',
    and we alwys believe our teachers (at least in grade school).
}%EndFootNoteSize
\end{quotation}

\V
\V


%--------------------------

\V
\V

                \section{{\bf Unordered Infinite Sums of Real Numbers}}\IndA{unordered infinite sums of real numbers}
                \label{SectG20}

\V

\begin{quotation}
{\footnotesize \underline{Preliminary Comments}

        We wish to extend the concept of `sum of the values of a function $f:X \,{\rightarrow}\, {\RR}$ over the set $X$',
    described for finite sets $X$ in Definition~\Ref{DefB10.50}, to the case in which $X$ is infinite.
    In symbolic terms: we wish to make sense of the notation ${\sum}_{X} f$ even when $X$ is infinite.

    Unfortunately, it seems to be impossible to formulate a `reasonable' definition of such a sum for {\em arbitrary}
    real-valued functions defined on an infinite nonempty set, even if one allows the possibility of the `values' $+{\infty}$ and $-{\infty}$.
    As will become evident, the main difficulty involves infinite sums in which infinitely many of the terms are positive and infinitely many are negative.
        Because of this, we start with the special case in which $f(x)\,\,{\geq}\,\,0$ for all $x$ in $X$.
    In symbols: Our first goal is to make sense, as best possible, of the expression ${\sum}_{X} f$ when $X$ is an arbitrary nonempty set
    and $f:X \,{\rightarrow}\, {\RR}$ is a {\em nonnegative} function on $X$.
    Once that is done, the next goal would then be to build on this to extend the ideas to as wide a class of functions as possible.

\V

        {\bf Remark} The approach to be followed below can be thought of as the obvious extension of the practice long used in doing finite sums `by hand.
    Indeed, when adding a {\em finite} list of numbers `by hand', the simplest way is to first compute the sum of the positive numbers in the list,
    then the sum the negative numbers, but with the minus signs ignored; finally, subtract the latter sum from the former to obtain the desired result.
    This reflects the reality that, in hand calculations at least, subtractions are more difficult to perform than are additions:
    this method requires only one subtraction.

        Note that even in the area of modern numerical analysis, in which the additions and subtractions are carried out by high-speed computers,
    so the difference in effort between subtraction and addition becomes negligible,
    the recommendation is still to add the positive and negative terms separately; but now the reason is to minimize the loss of significant digits.

\VV

        \underline{Temporary Notation} In order to minimize the possibilities for confusion between the `finite sum' and `infinite sum' cases,
    for the remainder of these `Preliminary Comments' we use the symbolism ${\displaystyle \overline{{\sum}}_{X}} f$ to denote the quantity already described in Definition~\Ref{DefB10.50}.
    That is, if $X$ is a finite nonempty set with exactly $k$ distinct elements, then in what follows we set
        \begin{displaymath}
        \overline{{\sum}}_{X} f \,=\, f(g(1)) + f(g(2)) + \,{\ldots}\,+ f(g(k)),
        \end{displaymath}
    where $g$ is any bijection of the set ${\NN}_{k} \,=\, \{1,2,\,{\ldots}\,k\}$ onto $X$.

\V

        Let us list some `guidelines' that will help us choose an appropriate definition.

\V

        \underline{Situation} $f:X \,{\rightarrow}\, {\RR}$ is a real-valued function, defined on a nonempty set $X$, such that $f(x)\,\,{\geq}\,\,0$ for all $x$ in $X$.
    Then any reasonable rule for assigning a definite `value' to the expression ${\sum}_{X} f$ ought to satisfy the following conditions:

\V

        \underline{Guideline 1} In the special case in which $X$ is a finite set, then the rule for ${\sum}_{X} f$ should assign same value $\overline{{\sum}}_{X} f$ given by Definition~\Ref{DefB10.50}.
    In terms of the `Temporary Notation' given above:
        \begin{displaymath}
        {\sum}_{X} f \,=\, \overline{{\sum}}_{X} f \mbox{when $X$ is a finite nonempty set}.
        \end{displaymath}

\V

        \underline{Guideline 2} The value assigned by the rule to the expression ${\sum}_{X} f$ should satisfy the inequality
        \begin{displaymath}
        0\,\,{\leq}\,\,{\sum}_{X} f\,\,{\leq}\,\,+{\infty}.
        \end{displaymath}

\V

        \underline{Guideline 3} If $Y$ is a nonempty subset of $X$, then the rule should imply that
        \begin{displaymath}
        {\sum}_{Y} f\,\,{\leq}\,\,{\sum}_{X} f.
        \end{displaymath}
    In particular, if the subset $Y$ satisfies ${\sum}_{Y} f \,=\, +{\infty}$, then one ought to have ${\sum}_{X} f \,=\, +{\infty}$.

\V

        \underline{Guideline 4} Let $S_{1}$, $S_{2}$,\,{\ldots}\,$S_{r}$ be nonempty subsets of $X$ which are mutually disjoint and whose union is $X$.
    Assume also that for each $j \,=\, 1,2,\,{\ldots}\,k$ the rule assigns a {\em finite} value to each expression ${\sum}_{S_{j}} f$, $1\,\,{\leq}\,\,j\,\,{\leq}\,\,r$, are all finite.
    Then the rule ought to assign to the expression ${\sum}_{X} f$ the value
        \begin{displaymath}
        {\sum}_{X} f \,=\, \left({\sum}_{S_{1}} f\right) + \left({\sum}_{S_{2}}\right) f + \,{\ldots}\, + \left({\sum}_{S_{r}} f\right)
        \end{displaymath}
    In particular, in this case ${\sum}_{X} f$ should also be finite.

\V

        {\bf Remarks on the Guidelines}

\V

        (1) Guideline 1 simply reaffirms that we are {\em extending} the definition of ${\sum}_{X} f$ from the case in which it was already defined to a wider class of situations.

        (2) The reason for including the possibility ${\sum}_{X} f \,=\, +{\infty}$ in Guideline~2 comes from examples such as $f:{\NN} \,{\rightarrow}\, {\RR}$ in which $f(k) \,=\, 1$ for all $k$ in ${\NN}$.
    Any reasonable definition of ${\sum}_{{\NN}} f$ for this $f$ ought to correspond intuitively to the equation $1+1+\,{\ldots}\,+1+\,{\ldots}\, \,=\, +{\infty}$.
    And of course the reason for {\em not} allowing ${\sum}_{X} f$ to be less than zero is because, in the situation under consideration we assume that $f(x)\,\,{\geq}\,\,0$ for all $x$.
    (However, the fact that the great Euler was willing to write the equation appearing in Chapter Quote~(1) might give one cause for worry about this guideline.
    More on that later.)

        (3) Guidelines 3 and 4 simply assert that some properties which are obvious for finite sums ought to extend to infinite sums,
    at least in the case of nonnegative summands.
    In particular, Guideline~4 corresponds to the Generalized Associative Law for Addition (see Theorem~\Ref{ThmB10.60}).

        (4) Note that in Guideline 4, only the {\em sums} ${\sum}_{S_{j}} f$, $1\,\,{\leq}\,\,j\,\,{\leq}\,\,r$, are asssumed to be finite.
    The {\em sets} $S_{j}$ are still allowed to have infinitely many elements.
    Indeed, that is the only case of real interest, since if all the sets $S_{j}$, $1\,\,{\leq}\,\,j\,\,{\leq}\,\,r$, are finite sets then so is $X$;
    then by Guideline~1, the result would revert to the finite case, which was already studied in Section~\Ref{SectB10}.

\V

        \underline{Note} One could of course come up with other Guidelines which any `reasonable' definition of ${\sum}_{X} f$ ought to satisfy.
    The four guidelines given here, however, are enough for our purposes.
    In particular, they suffice to point us towards the definition given below.

\V
\V

         The connecting link between the guidelines just given and the official definition given below is the following:

        \underline{Observation} Suppose that we have defined a notion of ${\sum}_{X} f$, at least for nonnegative functions $f$,
    which satisfies the preceding guidelines.
    Let $U_{X;f}$ denote the set of all real numbers of the form $\overline{{\sum}}_{W} f$, where $W$ is a nonempty {\em finite} subset of $X$.
    Then
        \begin{displaymath}
        {\sum}_{X} f \,\,{\geq}\,\,{\sup}\,U_{X;f}.
        \end{displaymath}

\V

        \underline{Proof of Observation} First note that the set $U_{X;f}$ is nonempty.
    Indeed, let $c$ be any element of $X$;
    such $c$ exists because $X$ itself is nonempty.
    Let $W \,=\, \{c\}$, and note that (by Definition~\Ref{DefB10.50}) one has $\overline{{\sum}}_{W} f\,=\, f(c)$, so the number $f(c)$ is an element of $U_{X;f}$.

    Next, it follows from Guidelines~1 and~3 that 
        \begin{displaymath}
        {\sum}_{X} f\,\,{\geq}\,\,{\sum}_{W} f \,=\, \overline{{\sum}}_{W} f
        \end{displaymath}
    for every finite nonempty subset $W$ of $X$.
    Thus, ${\sum}_{X} f$ must be an upper bound for the set $U_{X;f}$.
    But ${\sup}\,U_{X;f}$ is the {\em least} of such upper bounds, so it follows that ${\sum}_{X} f\,\,{\geq}\,\,{\sup}\,U_{X;f}$, as claimed.

\V
\V

        It follows from the preceding discussion that any `reasonable' definition of `${\sum}_{X} f$' for nonnegative functions $f$ defined on an nonempty set $X$ must satisfy
        \begin{displaymath}
        {\sup}\,U_{X;f}\,\,{\leq}\,\,{\sum}_{X} f\,\,{\leq}\,\,+{\infty}.
        \end{displaymath}
    These last inequalities  describe the extremes of the possible `reasonable' definitions of ${\sum}_{X} f$.
    It also suggests two obvious candidates for a `reasonable' definition of ${\sum}_{X}$.

        \underline{The Maximal Candidate} Suppose that $f:X \,{\rightarrow}\, {\RR}$ is a nonnegative function defined on an nonempty set $X$.
    Then define
        \begin{displaymath}
        {\sum}_{X}^{\mbox{max}} f \,=\, \left\{
        \begin{array}{cl}
        0 & \mbox{if $f(x) \,=\, 0$ for all $x$ in $X$} \\
          &                                             \\
        {\displaystyle \overline{{\sum}}_{W} f} & \mbox{if the set $W \,=\, \{x:f(x)\,>\,0\}$ is finite and nonempty} \\
          &                                             \\
        +{\infty} & \mbox{if $f(x)\,>\,0$ for infinitely many $x$}.
        \end{array}
                                    \right.
        \end{displaymath}

        \underline{The Minimal Candidate} Suppose that $f:X \,{\rightarrow}\, {\RR}$ is a nonnegative function defined on a nonempty set $X$.
    Let $U_{X;f}$ be as above, and define
        \begin{displaymath}
        {\sum}_{X}^{\mbox{min}} f \,=\, {\sup}\,U_{X;f}.
        \end{displaymath}

        It can be shown that both the `Maximal Candidate' ${\sum}_{X}^{\mbox{max}}$ and the `Minimal Candidate' ${\sum}_{X}^{\mbox{min}}$ satisfy the four guidelines.
    However, the `Maximal' case is of little interest;
    indeed, it assigns the {\em same} value, $+{\infty}$, to every sum of infinitely many positive numbers.

        In contrast, the `Minimal Candidate' has ${\sum}_{X} f$ being finite in as many situations as is possible.
    Thus, we use it in Section~\Ref{SectG20} below as our `official' definition of ${\sum}_{X} f$ for positive functions $f$.

\V

        It has been mentioned above that the concept of the `Unordered Sum' of the values of a positive function $f:X \,{\rightarrow}\, {\RR}$ corresponds intuitively to a process of `weighing', at least in the case of finite sums.
    More precisely, think of $X$ as being a collection of physical objects, and for each $x$ in $X$ let $f(x)$ denote the weight of the object $x$.
    Recall how the process of `weighing' works: place the collection $X$ on the left-hand tray of the scale,
    and then place on the right-hand tray a stack of standard weights that is at least as heavy $X$.
    Remove standard weights from the right tray until the two trays come into balance;
    at that point, the total weight in the right-hand tray equals the total weight of $X$.
    More precisely, let $V_{X;f}$ denote the set of numbers $v$ such that $v$ is larger than the total mass of $X$;
    in order for $X$ to have finite total mass, the set $V_{X;f}$ must be nonempty.
    The process of removing weights from the right-hand tray then corresponds to the mathematical process of determining the infimum of the set $V_{X;f}$.

        The key to this weighing process is to have a way of knowing that a given stack of standard weights is larger than the total mass of $X$.
    The basic assumption behind the `minimal candidate' is that any number which is at least as large as all the numbers in $U_{X;f}$ must be in $V_{X;f}$.
    In other words, $V_{X;f}$ is the set of all upper bounds of $U_{X;f}$.
    Since ${\sup}\,U_{X;f}$ is the {\em least} of the upper bounds of $U_{X;f}$, it follows that ${\sum}_{X}^{\mbox{min}} f \,=\, {\inf}\,V_{X;f}$; that is, ${\sum}_{X}^{\mbox{min}}$ is the result of removing the excess weights from the right-hand tray until balance is reached.

\V

        The obvious way to extend the concept of ${\sum}_{X}^{\mbox{min}} f$ from the case in which $f$ is nonnegative on~$X$
    to the case of arbitrary~$f$ is to use the results of Corollary~\Ref{CorB20.105}
    That is, set
        \begin{displaymath}
        {\sum}_{X}^{\mbox{min}} f \,=\,
    \left({\sum}_{X}^{\mbox{min}} f^{+}\right) - 
    \left({\sum}_{X}^{\mbox{min}} f^{-}\right) \h ({\ast})
        \end{displaymath}
    where the values ${\sum}_{X}^{\mbox{min}} f^{+}$ and ${\sum}_{X}^{\mbox{min}} f^{-}$ are defined as above for nonnegative functions.

        Unfortunately, there is a problem which can arise when $X$ is an infinite set:
    the quantities ${\sum}_{X}^{\mbox{min}} f^{+}$ and ${\sum}_{X}^{\mbox{min}} f^{-}$ might both equal~$+{\infty}$.
    That is, Equation~$({\ast})$ might take the form ${\sum}_{X}^{\mbox{min}} f \,=\, {\infty}-{\infty}$.
    However, as is  well known from elementary calculus, the expression ${\infty}-{\infty}$ is an `indeterminate form' which cannot be assigned a definite value as written.

        This difficulty is a real one, and cannot be completely resolved. However, if at least one of the quantities ${\sum}_{X}^{\mbox{min}} f^{+}$ or ${\sum}_{X}^{\mbox{min}} f^{-}$ is finite,
    then a definite answer can be given.
    For example, if ${\sum}_{X}^{\mbox{min}} f^{+} \,=\, +{\infty}$ but ${\sum}_{X}^{\mbox{min}} f^{-}$ is finite,
    then Equation~$({\ast})$ takes the form
        \begin{displaymath}
        {\infty} - \mbox{ a finite number}.
        \end{displaymath}
    It is natural to assign the value $+{\infty}$ to that expression.

\V

        The fruit of these considerations is summarized in the `official' definition given below.
    In it, we drop the provisional `min' from the notation $\sum_{X}^{\mbox{min}}$,
    since its only purpose was to distinguish itself from the other candidate, $\sum_{X}^{\mbox{max}}$.
}%EndFootNoteSize
\end{quotation}

\V
\V

            \subsection{\small{\bf Definition} (Unordered Infinite Sums)\IndB{unordered infinite sums of real numbers}{definition}}
            \label{DefG20.20}

\V

        Let $X$ be a nonempty set of real numbers.

\V

        (1) Suppose that $g:X \,{\rightarrow}\, {\RR}$ is a nonnegative function on $X$;
    that is, $g(x)\,\,{\geq}\,\,0$ for all $x$ in $X$.
    Let $U_{X;g}$ denote the set of all numbers of the form ${\sum}_{W} g$, where $W$ is a finite nonempty subset of $X$, and ${\sum}_{W} g$ is the corresponding unordered finite sum, as described in Definition~\Ref{DefB10.50}.
    Then
        \begin{displaymath}
        {\sum}_{X} g \,=\, {\sup}\,U_{X;g}.
        \end{displaymath}

\V

        (2) Suppose that $f:X \,{\rightarrow}\, {\RR}$ is a real-valued function defined on $X$,
    and let $f^{+}$ and $f^{-}$ be the corresponding positive and negative parts of $f$, as in Definition~\Ref{DefB20.70}.
    Let $A \,=\, {\sum}_{X} f^{+}$ and $B \,=\, {\sum}_{X} f^{-}$, as defined in Part~(1) above, with $g$ replaced by $f^{+}$ and $f^{-1}$, respectively.

        \h (i)\, If $A$ and $B$ are both finite, then ${\sum}_{X} f \,=\, A-B$.

        \h (ii) If $A \,=\, +{\infty}$ and $B$ is finite, then ${\sum}_{X} f \,=\, +{\infty}$.
    Likewise, if $A$ is finite and $B \,=\, +{\infty}$, then ${\sum}_{X} f \,=\, -{\infty}$.

\V

        (3) Suppose that $f:X \,{\rightarrow}\, {\RR}$ is a real-valued function, defined on a nonempty set $X$.
    Let $A \,=\, {\sum}_{X} f^{+}$, $B \,=\, {\sum}_{X} f^{-}$, as above.

        \h (i) If at least one of the quantities $A$ or $B$ is finite then one says that {\bf the unordered sum ${\sum}_{X} f$ is defined}, or that {\bf the sum exists}.
    In contrast, if $A \,=\, B \,=\, +{\infty}$, then one says that {\bf the unordered sum ${\sum}_{X} f$ is not defined}, or that it is the {\bf indeterminate form ${\infty}-{\infty}$}.

        \h (ii) Suppose, in addition, $X$ is an infinite set, and assume that the infinite sum ${\sum}_{X} f$ is defined, as described in~(i).
    If ${\sum}_{X} f \,=\, S$ for some \underline{finite} number $S$,
    then one says that the unordered sum ${\sum}_{X} f$ {\bf is convergent} and that ${\sum}_{X} f$ {\bf converges to $S$}.
    In contrast, if ${\sum}_{X} f \,=\, +{\infty}$ or ${\sum}_{X} f \,=\, -{\infty}$,
    then one says that the unordered sum ${\sum}_{X} f$ {\bf diverges to $+{\infty}$ or to $-{\infty}$}, respectively.

        Note: In ordinary mathematical usage, statements such as `${\sum}_{X} f$ is a finite sum', or `${\sum}_{X} f$ is an infinite sum', can be ambiguous.
    For example, the first statement might mean that the sum involves only a finite number of terms (i.e., $X$ is a finite set),
    while the second statement might mean that the sum involves infinitely many terms (i.e., $X$ is an infinite set).
    However, these statements could well be interpreted as referring to the {\em value} assigned to the sum,; that is, the first statement might mean that the value is finite (even though $X$ could be infinite), while the second could mean that the value is $+{\infty}$ or $-{\infty}$.
    In {\TheseNotes} we try avoid such ambiguity by using the phrases `finite sum' and `infinite sum' to indicate that the set $X$ is finite or infinite, respectively;
    and to use `convergent' and `divergent to $ \,{\pm}\, {\infty}$' for the alternate meanings.

\V
\V

            \subsection{\small{\bf Remarks}}
            \label{RemrkG20.30}

\hspace*{\parindent}(1) Note that we allow ourselves the freedom to write down expressions such as ${\sum}_{X} f$ {\em before} we know whether `the sum exists'.
    This is similar to the freedom we allowed ourselves with limits; see Remark~\Ref{RemrkC40.15}.

\V

        (2) The preceding definition provides a fairly conservative approach to infinite sums because it refuses to assign
    a value to the expression ${\sum}_{X} f$ when both of the quantities $A$ and $B$ in Part~(2) are infinite.
    There are other, less conservative, approaches which sometimes assign a definite value in that case as well.
    Indeed, the `Ordered Sum' approach to infinite sums, which is discussed in the next section, is an example of such a less conservative approach.
    There is an entire theory of such `summation methods'.

\V

        (3) The preceding definition is said to be only `fairly' conservative, because one could have chosen to use an approach that is drastically more so.
    For example, in his famous `Achilles and the Tortoise' paradox, the ancient Greek philosopher Zeno of Elea
    appears to take the viewpoint that every infinite sum of positive terms should be treated as having value $+{\infty}$;
    that is, he chose the `Maximal Candidate'.

\V

        (3) Definition~\Ref{DefG20.20} allows us the flexibility of saying that certain unordered infinite sums are defined, but diverge (whether to $+{\infty}$ or $-{\infty}$).
    In practice, however, most of the interesting results involve unordered sums which are convergent to some (finite) value.

\V
\V

            \subsection{\small{\bf Examples}}
            \label{ExampG20.35}

\hspace*{\parindent}(1) Let $X$ be an infinite set and let $c$ be a real number.
    Define $f:X \,{\rightarrow}\, {\RR}$ by the rule $f(x) \,=\, c$ for all $x$ in $X$.
    It is easy to show that ${\sum}_{X} f$ is defined, and that
        \begin{displaymath}
        {\sum}_{X} f \,=\, \left\{
        \begin{array}{cl}
        +{\infty} & \mbox{if $c\,>\,0$}   \\
            0     & \mbox{if $c \,=\, 0$} \\
        -{\infty} & \mbox{if $c\,<\,0$}.
        \end{array}
                                    \right.
        \end{displaymath}

        \underline{Note} The main point of this example is that the quantity ${\sum}_{X} f$ can be defined even if the set $X$ is quite large;
    in particular, $X$ is allowed to be uncountable.

\V

        (2) Let $X \,=\, {\NN}$ and let $f:{\NN} \,{\rightarrow}\, {\RR}$ be given by the rule
        \begin{displaymath}
        f(k) \,=\, \frac{1}{k^{2}} \mbox{ for each $k$ in ${\NN}$}.
        \end{displaymath}
    Since $f(k)\,>\,0$ for all $k$ in ${\NN}$, it follows that the infinite sum ${\sum}_{{\NN}} f$ is certainly defined.
    The real issue is whether this sum is finite or infinite.

        Let $W$ be a finite nonempty subset of ${\NN}$, and let $k$ be the largest element of $W$.
    Then certainly one has $W \,{\subseteq}\, {\NN}_{k}$,
    where ${\NN}_{k} \,=\, \{m{\in}{\NN}:1\,\,{\leq}\,\,m\,\,{\leq}\,\,k\}$ (see Definition~\Ref{DefA10.05}).
    Let $x_{k} \,=\, \sum_{{\NN}_{k}} f$, so by Definition~\Ref{DefB10.50} one can write
        \begin{displaymath}
        x_{k} \,=\, f(1) + f(2) + \,{\ldots}\,+ f(k) \,=\, 
    1 + \frac{1}{4} + \frac{1}{9} + \,{\ldots}\,+\frac{1}{k^{2}}.
        \end{displaymath}
    Clearly the sequence ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,)$ satisfies $x_{1} \,=\, 1$ and $x_{k+1} \,=\, x_{k}+1/(k+1)^{2}$ for each $k$ in ${\NN}$.
    In other words,  ${\xi}$ satisfies the conditions of the sequence studied in Example~\Ref{ExampC20.20}~(1).
    It follows from the results of that example that $x_{k}\,<\,2$ for all $k$.
    Thus one has
        \begin{displaymath}
        {\sum}_{W} f\,\,{\leq}\,\,{\sum}_{{\NN}_{k}} f\,<\, 2,
        \end{displaymath}
    so that $2$ is an upper bound for the set $U_{{\NN};f}$.
    Thus, by basic properties of `supremum', it follows that
        \begin{displaymath}
        {\sum}_{{\NN}} f \,=\, {\sup}\,U_{{\NN};f}\,\,{\leq}\,\,2.
        \end{displaymath}
    In particular, ${\sum}_{{\NN}} f$ is finite.

        The logical next question would be this: since we now know that ${\sum}_{{\NN}} f$ is finite, what is its exact value?
    It turns out that this question can be answered, but only with considerable difficulty;
    we do not address this question here.

\V

        (3) Let $X \,=\, {\NN}$ and let $f:{\NN} \,{\rightarrow}\, {\RR}$ be given by the rule
        \begin{displaymath}
        f(k) \,=\, \frac{1}{k} \mbox{ for each $k$ in ${\NN}$}.
        \end{displaymath}
    An analysis similar to that used in Example~(2) above, but this time based on Example~\Ref{ExampC20.20}~(2), shows that ${\sum}_{X} f \,=\, +{\infty}$.
    Indeed, let $k$ be any element of ${\NN}$, and let $W_{k} \,=\, {\NN}_{2^{k}} \,=\, \{1,2,\,{\ldots}\,2^{k}\}$.
    The results of Example~\Ref{ExampC20.20}~(2) tell us that
        \begin{displaymath}
        {\sum}_{W_{k}} f \,=\, {\sum}_{{\NN}_{2^{k}}}  f\,\,{\geq}\,\,\frac{k}{2}.
        \end{displaymath}
    By the Archimedean Property, the fraction $k/2$ can become arbitrarily large;
    and it is clear that $U_{{\NN};f}$ contains all the numbers of the form ${\sum}_{W_{k}} f$.
    Thus, it follows that ${\sup}\,U_{{\NN};f} \,=\, +{\infty}$. That is, ${\sum}_{{\NN}} f \,=\, +{\infty}$, as claimed.

\V

        (4) Let $X \,=\, {\NN}$ and let $r$ be a number such that $-1\,<\,r\,<\,1$.
    Define $g:{\NN} \,{\rightarrow}\, {\RR}$ by the rule $g(i) \,=\, r^{i-1}$.
    (Note that if $r \,=\, 0$ and $i \,=\, 1$, then the expression $r^{i-1}$ becomes the indeterminate form~$0^{0}$; see the discussion after Lemma~\Ref{LemmaE20.75}.
    As is stated in that discussion, we set this expression equal to~$1$.)


    \h\underline{Case 1} Suppose that $r \,=\, 0$. Then $g(1) \,=\, 1$ while $g(i) \,=\, 0$ if $i\,\,{\geq}\,\,2$;
    in particular, $g(i)\,\,{\geq}\,\,0$ for all $i$ in ${\NN}$. It is clear that if $W$ is any finite nonempty subset of ${\NN}$,
    then either $\sum_{W} g \,=\, 0$ (namely, when $1\not \in W$), or $\sum_{W} g \,=\, 1$ (when $1{\in}W$).
    It is clear from this that $\sum_{{\NN}} g \,=\, 1$.

       \h \underline{Case 2} Suppose that $0\,<\,r\,<\,1$, so that $g(i)\,>\,0$ for all $i$ in $X$.
    Let $W$ be a finite nonempty subset of ${\NN}$, and let $k$ be the largest of the elements of $W$, so that $W \,{\subseteq}\, {\NN}_{k}$.
    One can then use Part~(a) of Theorem~\Ref{ThmB25.80} to conclude that
        \begin{displaymath}
        {\sum}_{W} g\,\,{\leq}\,\,{\sum}_{{\NN}_{k}} g \,=\, 
    1 + r + \,{\ldots}\,+ r^{k-1} \,=\, \frac{1-r^{k}}{1-r}\,<\,\frac{1}{1-r}.
        \end{displaymath}
    In particular, $1/(1-r)$ is an upper bound for the set $U_{{\NN};g}$, so that ${\displaystyle {\sup}\,U_{{\NN};g}\,\,{\leq}\,\,\frac{1}{1-r}}$.
    Likewise, one knows that for every ${\varepsilon}\,>\,0$ there exists $k$ such that $0\,<\,r^{k}\,<\,{\varepsilon}(1-r)$.
    For such $k$ the set $W \,=\, \{1,2,\,{\ldots}\,k\}$ satisfies
        \begin{displaymath}
        {\sum}_{W} g \,=\, \frac{1-r^{k}}{1-r}\,>\,\frac{1-{\varepsilon}(1-r)}{1-r} \,=\, \frac{1}{1-r} - {\varepsilon}
        \end{displaymath}
    and thus ${\displaystyle {\sup}\,U_{{\NN};g}\,>\,\frac{1}{1-r} - {\varepsilon}}$ for every ${\varepsilon}\,>\,0$.
    It now follows that ${\displaystyle \sum_{{\NN}} g \,=\, \frac{1}{1-r}}$.

        \h \underline{Case 3} Suppose that that $-1\,<\,r\,<\,0$, so that $r^{i-1}\,>\,0$ if $i$ is odd, and $r^{i-1}\,<\,0$ if $i$ is even.
    It follows that $g^{+}\left(2j+1\right) \,=\, |r|^{j}$, and $g^{+}\left(2j\right) \,=\, 0$, for each $j$ in ${\NN}$.
    It then follows from the results of Case~2 above -- but with $r$ in that case replaced by $r^{2}$ --
    that the infinite unordered sum $\sum_{{\NN}} g^{+}$ is convergent and has value ${\displaystyle \frac{1}{1-r^{2}}}$.
    In a similar manner one can easily show that ${\displaystyle {\sum}_{{\NN}} g^{-} \,=\, \frac{(-r)}{1-r^{2}}}$.
    Thus, by Definition~\Ref{DefG20.20} the unordered sum ${\displaystyle {\sum}_{{\NN}} g}$ is convergent, and one has
        \begin{displaymath}
        {\sum}_{{\NN}} g  \,=\, {\sum}_{{\NN}} g^{+} - {\sum}_{{\NN}} g^{-} \,=\, 
    \left(\frac{1}{1-r^{2}}\right) - \left(-\frac{r}{1-r^{2}}\right) \,=\, \frac{1+r}{1-r^{2}} \,=\, \frac{1}{1-r}.
        \end{displaymath}

        \underline{Summary}: If $-1\,<\,r\,<\,1$, and $g(i) \,=\, r^{i-1}$ for each $i$ in ${\NN}$,
    then the unordered sum ${\displaystyle {\sum}_{{\NN}} g}$ is convergent, and its value is $1/(1-r)$.

\V
\V

The next result summarizes some basic properties of unordered sums.

\V

            \subsection{\small{\bf Theorem}}
            \label{ThmG20.40}

        Let $g:X \,{\rightarrow}\, {\RR}$ be a real-valued function defined on a nonempty set $X$ such that $g(x)\,\,{\geq}\,\,0$ for all $x$ in $X$.
    Then:

\V

        (a) The quantity ${\sum}_{X} g$ is defined (see Part~(2) of Definition~\Ref{DefG20.20}), and ${\sum}_{X} g\,\,{\geq}\,\,0$.
    Furthermore, one gets ${\sum}_{X} g \,=\, 0$ if, and only if, $g(x) \,=\, 0$ for all $x$ in $X$.


\V

        (b) Suppose that $Y$ is a nonempty subset of $X$. Then ${\sum}_{Y} g\,\,{\leq}\,\,{\sum}_{X} g$.
    In particular, if ${\sum}_{Y} g \,=\, +{\infty}$ then ${\sum}_{X} g \,=\, +{\infty}$.

        If, instead, the quantity ${\sum}_{Y} g$ is finite, and ${\sum}_{Y} g \,=\, {\sum}_{X} g$,
    then either $Y \,=\, X$ or, when $Y \,\,{\neq}\,\, X$,
    one has $g(x) \,=\, 0$ for all $x$ in $X{\setminus}Y$.
    Conversely, if either $Y \,=\, X$ or $g(x) \,=\, 0$ for all $x$ in $X{\setminus}Y$, then ${\sum}_{Y} g \,=\, {\sum}_{X} f$.

\V

        (c) A similar result holds if $g(x)\,\,{\leq}\,\,0$ for all $x$ in $X$, except that now one has ${\sum}_{Y} g\,\,{\geq}\,\,{\sum}_{X} g$.
    In particular, if ${\sum}_{Y} g \,=\, -{\infty}$ then ${\sum}_{X} g \,=\, -{\infty}$.


        The simple proof is left as an exercise.

\V
\V


        The next result allows one to determine directly, in terms of $f$, whether an unordered infinite sum ${\sum}_{X} f$ is convergent,
    without needing to first reduce everything to $f^{+}$ and $f^{-}$.

\V

            \subsection{\small{\bf Theorem}}
            \label{ThmG20.50}

\V

        Let $f:X \,{\rightarrow}\, {\RR}$ be a real-valued function defined on a infinite set $X$.

\V

        Let $L$ be a real number. Then the following statements are equivalent:

        \h (i)\, The unordered sum ${\sum}_{X} f$ is convergent and has the value $L$ in ${\RR}$.

        \h (ii) For every ${\varepsilon}\,>\,0$ there exists a finite nonempty subset $W_{{\varepsilon}}$ of $X$ such that if $W$ is any finite subset of $X$ with $W \,{\supseteq}\, W_{{\varepsilon}}$,
then
        \begin{displaymath}
        \left|L-\left({\sum}_{W} f\right)\right|\,<\,{\varepsilon}.
        \end{displaymath}
    \underline{Alternate Formulation of (ii)}: For every choice of numbers $y$ and $z$ such that $y\,<\,L\,<\,z$,
    there exists a finite subset $W_{y;z}$ of $X$ such that if $W$ is any finite subset of $X$ with $W \,{\supseteq}\, W_{y;z}$, then $y\,<\,\sum_{W} f\,<\,z$.

\V

        {\bf Proof} First note that it is obvious that the two formulations of Statement~(ii) are equivalent.

\V

        Suppose that Statement~(i) is true. Then, by definition, the quantities $A \,=\, {\sum}_{X} f^{+}$ and $B \,=\, {\sum}_{X} f^{-}$ are finite.
    It follows, from the definitions of these quantities as ${\sup}\,U_{X;f^{+}}$ and ${\sup}\,U_{X;f^{-}}$ and the properties of `sup',
    that there exist finite nonempty subsets $W_{1}$ and $W_{2}$ of $X$ such that
        \begin{displaymath}
        A-\frac{{\varepsilon}}{2}\,<\,{\sum}_{W_{1}} f^{+}\,\,{\leq}\,\,A\,<\,A+\frac{{\varepsilon}}{2}
        \end{displaymath}
    and
        \begin{displaymath}
        B-\frac{{\varepsilon}}{2}\,<\,{\sum}_{W_{2}} f^{-}\,\,{\leq}\,\,A\,<\,A+\frac{{\varepsilon}}{2}.
        \end{displaymath}
    Let $W_{{\varepsilon}} \,=\, W_{1}\,{\cup}\,W_{2}$, and suppose that $W$ is a finite subset of $X$ such that $W \,{\supseteq}\, W_{{\varepsilon}}$.
    Then one has $W \,{\supseteq}\, W_{1}$, hence
        \begin{displaymath}
        A-\frac{{\varepsilon}}{2}\,<\,{\sum}_{W_{1}} f^{+} \,\,{\leq}\,\,{\sum}_{W} f^{+}\,\,{\leq}\,\,A\,<\,A+\frac{{\varepsilon}}{2};
        \end{displaymath}
    the inequality ${\sum}_{W_{1}} f^{+}\,\,{\leq}\,\,{\sum}_{W} f^{+}$ holding because $f^{+}(x)\,\,{\geq}\,\,0$ for all $x$ and $W \,{\supseteq}\, W_{1}$.
    Likewise, one has
        \begin{displaymath}
        B-\frac{{\varepsilon}}{2}\,<\,{\sum}_{W_{2}} f^{-}\,\,{\leq}\,\,{\sum}_{W} f^{-}\,\,{\leq}\,\,B\,<\,B+\frac{{\varepsilon}}{2}.
        \end{displaymath}
    Multiply each term in this string of inequalities by $(-1)$ to get
        \begin{displaymath}
        -B-\frac{{\varepsilon}}{2}\,<\,-B\,\,{\leq}\,\,-\left({\sum}_{W} f^{-}\right)\,<\,-B+\frac{{\varepsilon}}{2}
        \end{displaymath}
    Combining these inequalities then yields
        \begin{displaymath}
        A-B-{\varepsilon}\,<\,{\sum}_{W} f^{+} - {\sum}_{W} f^{-}\,<\,A-B + {\varepsilon}.
        \end{displaymath}
    In light of Theorem~\Ref{ThmB10.100}, combined with the fact that $f \,=\, f^{+} - f^{-}$, one can rewrite the preceding as
        \begin{displaymath}
        A-B-{\varepsilon}\,<\,{\sum}_{W} f\,<\,A-B+{\varepsilon}.
        \end{displaymath}
    Since $A-B \,=\, L \,=\, {\sum}_{X} f$, this last result can be written
        \begin{displaymath}
        \left|{\sum}_{X} f - {\sum}_{W} f\right|\,<\,{\varepsilon},
        \end{displaymath}
    as required.
    Thus, Statement~(i) implies Statement~(ii).

\V

        Suppose, instead, that Statement (ii) is true.

        \underline{Claim 1} The unordered sum ${\sum}_{X} f$ is convergent.

        \underline{Proof of Claim 1} Choose ${\varepsilon} \,=\, 1$ in Statement~(ii),
    and let $W_{1}$ be a finite nonempty subset of $X$ such that if $W$ is a finite subset of $X$ satisfying $W \,{\supseteq}\, W_{{1}}$,
    then $\left|L-{\sum}_{W} f\right|\,<\,1$.

        Suppose ${\sum}_{X} f^{+} \,=\, +{\infty}$. Let $D \,=\, {\sum}_{W_{1}} f^{-}$, so that $D$ is a fixed nonegative real number.
    Then, by the hypothesis that
        %\begin{displaymath}
        ${\displaystyle {\sum}_{X} f^{+} \,=\, {\sup}\,U_{X;f^{+}}}$,
        %\end{displaymath}
 there must exist a finite nonempty subset $W'$ of $X$ such that
        \begin{displaymath}
        {\sum}_{W'} f^{+}\,\,{\geq}\,\,1+|L| + D
        \end{displaymath}
    By properties of finite sums, we may ignore any terms in the sum on the left side of this inequality equal to $0$,
    since such terms do not change the value of that sum.
    That is, we may assume, without loss of generality, that $W'$ is chosen so that $f^{+}(x)\,>\,0$ for all $x$ in $W'$.
    It then follows that $f^{-}(x) \,=\, 0$ for all $x$ in $W'$.
    Now let $W \,=\, W'\,{\cup}\,W_{1}$.
    Since $W \,{\supseteq}\, W'$, it is clear that
        \begin{displaymath}
        {\sum}_{W} f^{+}\,\,{\geq}\,\,{\sum}_{W'} f^{+}\,\,{\geq}\,\,1+|L|+D
        \end{displaymath}
    Also, since $W \,{\supseteq}\, W_{1}$, it follows by Statement~(ii) that
        \begin{displaymath}
        \left|L-{\sum}_{W} f\right|\,<\,1
        \end{displaymath}
    Finally, because $f^{-}(x) \,=\, 0$ for all $x$ in $W'$, it follows from basic properties of finite sums that
        \begin{displaymath}
        {\sum}_{W} f^{-} \,=\, {\sum}_{W_{1}} f^{-} \,=\, D.
        \end{displaymath}
    Next, use the fact that $f \,=\, f^{+}-f^{-}$ and Theorem~\Ref{ThmB10.100} to see that
        \begin{displaymath}
        {\sum}_{W} f^{+} \,=\, {\sum}_{W} f + {\sum}_{W} f^{-} \,=\, 
    \left({\sum}_{W} f\, -L\right) + L + {\sum}_{W} f^{-} \,=\, 
    \left({\sum}_{W} f\, -L\right) + L + D.
        \end{displaymath}
    Apply the Triangle Inequality to this last equation, together with the inequalities obtained above, to get
        \begin{displaymath}
        1+|L| + D\,\,{\leq}\,\,{\sum}_{W} f^{+}\,\,{\leq}\,\,\left|L-{\sum}_{W} f\right| + |L| + D\,<\,1 + |L| + D.
        \end{displaymath}
    This implies the impossible inequality $1+|L|+D\,<\,1+|L|+D$.
    That is, assuming that ${\sum}_{X} f^{+} \,=\, +{\infty}$ leads to a contradiction.
    Likewise, assuming that ${\sum}_{X} f^{-} \,=\, +{\infty}$ would also lead to a similar contradiction.
    Thus, both of these unordered sums must be finite, hence ${\sum}_{X} f$ is defined and is finite, as claimed.

        \underline{Claim 2} Let $A \,=\, {\sum}_{X} f^{+}$ and $B \,=\, {\sum}_{X} f^{-}$.
    Then $L \,=\, A-B$.

        \underline{Proof of Claim 2} Let ${\varepsilon}\,>\,0$ be given, and let $W_{{\varepsilon}}$ be a finite nonempty subset of $X$ such that
    if $W$ is any subset of $X$ satisfying $W \,{\supseteq}\, W_{{\varepsilon}}$ then $|L-{\sum}_{W} f|\,<\,{\varepsilon}/3$.
    Now let $W_{1}$ and $W_{2}$ be a finite subsets of $X$ such that $0\,\,{\leq}\,\,A-{\sum}_{W_{1}} f^{+}\,<\,{\varepsilon}/3$,
    and $0\,\,{\leq}\,\,B-{\sum}_{W_{2}} f^{-}\,<\,{\varepsilon}/3$.
    By an argument similar to that used in Part~(a) of this proof, together with the hypothesis that Statement~(ii) holds, one sees that
        \begin{displaymath}
        |A-{\sum}_{W} f^{+}|\,<\,\frac{{\varepsilon}}{3}, \,
        |B-{\sum}_{W} f^{-}|\,<\,\frac{{\varepsilon}}{3}, \mbox{ and }
        |L-{\sum}_{W} f|\,<\,\frac{{\varepsilon}}{3}.
        \end{displaymath}
    Next, notice that from the equation $f \,=\, f^{+} - f^{-}$ one gets ${\displaystyle \sum_{W} f \,=\, \left(\sum_{W} f^{+}\right) - \left(\sum_{W} f^{-}\right)}$, hence
        \begin{displaymath}
        (A-B)-L \,=\, \left(A-{\sum}_{W} f^{+}\right) - \left(B- {\sum}_{W} f^{-}\right) - \left(L-{\sum}_{W} f\right).
        \end{displaymath}
    Now use the Triangle Inequality, together with the properties obtained above for the set $W$, to get
        \begin{displaymath}
        |(A-B)-L|\,\,{\leq}\,\,\left|A-{\sum}_{W} f^{+}\right| + \left|B- {\sum}_{W} f^{-}\right| + \left|L-{\sum}_{W} f\right|\,<\,
    \frac{{\varepsilon}}{3} + \frac{{\varepsilon}}{3} + \frac{{\varepsilon}}{3}.
        \end{displaymath}
    In summary: For every ${\varepsilon}\,>\,0$ one has $|(A-B)-L|\,<\,{\varepsilon}$.
    The only way this can happen is if $L \,=\, A-B$.
    In light of the definition of $A$ and $B$, this can be written
        \begin{displaymath}
        L \,=\, {\sum}_{X} f^{+} - {\sum}_{X} f^{-} \,=\, {\sum}_{X} f.
        \end{displaymath}
    That is, Statement (ii) implies Statement (i), as claimed.

\V
\V

            \subsection{\small{\bf Remarks}}
            \label{RemrkG20.55}

\V

\hspace*{\parindent}(1) Many authors use the property described in Statement~(ii) above as the {\em definition} of what it means for the unordered sum $\sum_{X} f$ to converge to~$L$;
    our definition would, for them, be a theorem to be proved.
    On the whole, the approach to `unordered sums' based on the property in Staement~(ii) seems fussier than the `$f \,=\, f^{+}-f^{-}$' approach used in {\TheseNotes}.
    The `Statement~(ii)' approach does have one major advantage: it can be generalized to situations involving functions whose values are {\em not} real numbers,
    and thus for which the decomposition $f \,=\, f^{+}-f^{-}$ may not make sense.
    (For those in the know: Think of functions $f:X \,{\rightarrow}\, B$, where $B$ is a Banach space.)

\V

        (2) The equations `$\sum_{X} f \,=\, +{\infty}$' and `$\sum_{X} f \,=\, -{\infty}$' have similar characterizations in terms of finite sums of the form $\sum_{W} f$.
    The precise statements of these characterizations, and their proofs, are left as an exercise.
    (Hint: Use the `Alternate Formulation of (ii)' as a guide.)

\V

            \subsection{\small{\bf Corollary}}
            \label{CorG20.60}

        Let $X$  be a nonempty set.

\V

        (a) Suppose that $f$ and $g$ are real-valued functions on $X$ such that the unordered sums ${\sum}_{X} f$ and ${\sum}_{X} g$ are convergent.
    Then the sum ${\sum}_{X} (f+g)$ is also convergent, and one has
        \begin{displaymath}
        {\sum}_{X} (f+g) \,=\, {\sum}_{X} f + {\sum}_{X} g.
        \end{displaymath}

\V

        (b) Suppose that $f:X \,{\rightarrow}\, {\RR}$ is a function such that ${\sum}_{X} f$ is convergent.
    Then for every real number $c$ the sum ${\sum}_{X} c{\cdot}f$ is also convergent, and one has
        \begin{displaymath}
        {\sum}_{X} c{\cdot}f \,=\, c{\sum}_{X} f.
        \end{displaymath}

\V

        (c) More generally, suppose that $f_{1}$, $f_{2}$,\,{\ldots}\,$f_{k}$ are functions such that each of the ordered sums ${\sum}_{X} f_{j}$, $1\,\,{\leq}\,\,j\,\,{\leq}\,\,k$, is convergent.
    Also, let $c_{1}$, \,{\ldots}\,$c_{k}$ be real numbers. Then the unordered sum
    ${\sum}_{X} (c_{1}{\cdot}f_{1} + \,{\ldots}\,+ c_{k}{\cdot}f_{k})$ is convergent, and one has
        \begin{displaymath}
        {\sum}_{X} (c_{1}{\cdot}f_{1} + \,{\ldots}\,+ c_{k}{\cdot}f_{k}) \,=\, 
    c_{1}{\sum}_{X} f_{1} + \,{\ldots}\,+ c_{k}{\sum}_{X} f_{k}.
        \end{displaymath}

\V

        {\bf Proof} 

\V

        (a) Set $A \,=\, {\sum}_{X} f$, $B \,=\, {\sum}_{X} g$, and $C \,=\, A+B$.
    Let ${\varepsilon}\,>\,0$ be a positive number. By Theorem~\Ref{ThmG20.40},
there exist finite nonempty subsets $Y_{{\varepsilon}}$ and $Z_{{\varepsilon}}$ of $X$ such that if $Y$ and $Z$ are finite subsets of $X$ such that $Y \,{\supseteq}\, Y_{{\varepsilon}}$ and $Z \,{\supseteq}\, Z_{{\varepsilon}}$,
    then
        \begin{displaymath}
        \left|A-{\sum}_{Y} f\right|\,<\,\frac{{\varepsilon}}{2} \mbox{ and }
        \left|B-{\sum}_{Z} g\right|\,<\,\frac{{\varepsilon}}{2} \h ({\ast})
        \end{displaymath}
    Now set $W_{{\varepsilon}} \,=\, Y_{{\varepsilon}}\,{\cup}\,Z_{{\varepsilon}}$, so that $W_{{\varepsilon}}$ is a finite nonempty subset of $X$.
    Suppose that $W$ is a finite subset of $X$ such that $W \,{\supseteq}\, W_{{\varepsilon}}$.
    Since the set $W$ is finite, it follows from Theorem~\Ref{ThmB10.60} that ${\sum}_{W} (f+g) \,=\, {\sum}_{W} f + {\sum}_{X} g$.
    Using this, and the Triangle Inequality, one gets
        \begin{displaymath}
        |C-{\sum}_{W} (f+g)| \,=\, |(A+B)-{\sum}_{W} (f+g)| \,=\, 
    \left|\left(A-{\sum}_{W} f\right) + \left(B - {\sum}_{W} g\right)\right|
    \,\,{\leq}\,\,\left|A-{\sum}_{W} f\right| + \left|B - {\sum}_{W} g\right|
        \end{displaymath}
    Now apply Inequality~$({\ast})$ to the right-most terms above to get
        \begin{displaymath}
        |C-{\sum}_{W} (f+g)|\,\,{\leq}\,\, \frac{{\varepsilon}}{2} + \frac{{\varepsilon}}{2} \,=\, {\varepsilon}.
        \end{displaymath}
    Theorem~\Ref{ThmG20.50} now implies that $C \,=\, {\sum}_{X} (f+g)$.

\V

        (b) and (c): The simple proofs are left to the reader.

\V
\V

        The unordered infinite sums of greatest interest in analysis are the {\em convergent} sums.
    The next result provides the most common tool for determining that a sum is convergent without necessarily determining the actual value of that sum.

\V

            \subsection{\small{\bf Theorem}}
            \label{ThmG20.65}

        Let $f:X \,{\rightarrow}\, {\RR}$ be a real-valued function defined on an infinite set $X$.
    Then the following statements are equivalent:

\V

        \h (i)\,\, The unordered infinite sum ${\sum}_{X} f$ is convergent.

        \h (ii)\, The unordered infinite sum ${\sum}_{X} |f|$ is convergent.

        \h (iii) There exists a number $M\,\,{\geq}\,\,0$ such that
        \begin{displaymath}
        {\sum}_{W} |f|\,\,{\leq}\,\,M \mbox{ for every finite nonempty subset $W$ of $X$}.
        \end{displaymath}

\V

        {\bf Proof} Let $A \,=\, {\sum}_{X} f^{+}$ and $B \,=\, {\sum}_{X} f^{-}$.

\V

        Suppose that Statement (i) is true. Then, by definition, the quantities $A$ and $B$ are numbers, not $+{\infty}$.
    Since $|f| \,=\, f^{+}+f^{-}$, Part~(a) of Corollary~\Ref{CorG20.60} implies that ${\sum}_{X} |f|$ is a convergent and that
        \begin{displaymath}
       {\sum}_{X} |f| \,=\, {\sum}_{X} f^{+} + {\sum}_{X} f^{-} \,=\, A+B.
        \end{displaymath}
    Thus, ${\sum}_{X} |f|$ is convergent; that is, Statement~(ii) is true.

        Suppose that Statement (ii) is true, and let $M \,=\, {\sum}_{X} |f|$.
    Then $M \,=\, {\sup}\,U_{X;|f|}$.
    In particular, $M$ is an upper bound for the set of all numbers of the form ${\sum}_{W} |f|$, where $W$ is any finite nonempty subset of $X$.
    That is, Statement~(iii) is true.

        Finally, suppose that Statement~(iii) is true. Then the number $M$ is an upper bound of the set $U_{X;|f|}$. Since ${\sup}\,U_{X;|f|}$ is the least of such upper bounds, it follows that ${\sum}_{X} |f|\,\,{\leq}\,\,M$.
    Also $|f| \,=\, f^{+} + f^{-}$ and $f^{+}(x)\,\,{\geq}\,\,0$ and $f^{-}(x)\,\,{\geq}\,\,0$ for all $x$ in $x$.
    Thus it follows that $f^{+}(x)\,\,{\leq}\,\,|f|(x)$ and $f^{-}(x)\,\,{\leq}\,\,|f|(x)$ for all $x$ in $X$.
    In particular, for every finite nonempty subset $W$ of $X$ one has
        \begin{displaymath}
        {\sum}_{W} f^{+}\,\,{\leq}\,\,{\sum}_{W} |f|\,\,{\leq}\,\,M
    \mbox{ and }
        {\sum}_{W} f^{-}\,\,{\leq}\,\,{\sum}_{W} |f|\,\,{\leq}\,\,M.
        \end{displaymath}
    Thus, $M$ is an upper bound for both of the sets $U_{X;f^{+}}$ and $U_{X;f^{-}}$, which implies that ${\sum}_{X} f^{+}\,\,{\leq}\,\,M$ and ${\sum}_{X} f^{-}\,\,{\leq}\,\,M$.
    In particular, ${\sum}_{X} f$ is convergent; that is, Statement~(i) is true.

\V

            \subsection{\small{\bf Corollary}}
            \label{CorG20.67}


        Let $f:X \,{\rightarrow}\, {\RR}$ be a real-valued function on an infinite set $X$,
    and assume that the unordered sum ${\sum}_{X} f$ is defined (see Part~(3) of Definition~\Ref{DefG20.20}).
    If $Y$ is a nonempty subset of $X$, then the unordered sum ${\sum}_{Y} f$ is also defined. Furthermore:

        \h \, (i)\,\, If ${\sum}_{Y} f \,=\, +{\infty}$ then ${\sum}_{X} f \,=\, +{\infty}$.

        \h (ii)\, If ${\sum}_{Y} f \,=\, -{\infty}$ then ${\sum}_{X} f \,=\, -{\infty}$.

        \h (iii) If the sum ${\sum}_{X} f$ is convergent, then so is ${\sum}_{Y} f$.

\V
        
        The simple proof is left as an exercise.

\V
\V


        The next result is the generalization, to infinite unordered sums, of Theorem~\Ref{ThmB10.90}, the `Alternate Formulation of the Generalized Associative Law (for finite sums)'.

\V

            \subsection{\small{\bf Theorem} (Generalized Associative Law for Infinite Unordered Sums)}\IndDD{generalized associative law for infinite unordered sums}{unordered infinite sums of real numbers}\IndB{unordered infinite sums of real numbers}{generalized associative law for infinite unordered sums}
            \label{ThmG20.70}

\V

        Let $f:X \,{\rightarrow}\, {\RR}$ be a real-valued function on an infinite set $X$, and assume that the unordered sum $\sum_{X} f$ is defined.
    Let ${\cal F}$ be a partition of $X$ into a family of disjoint nonempty subsets (see Definition~\Ref{DefA50.85}).
        Suppose that for every $S$ in ${\cal F}$ the unordered sum ${\sum}_{S} f$ is convergent.
    (The fact that ${\sum}_{S} f$ is defined for each $S$ in ${\cal F}$ follows from Corollary~\Ref{CorG20.67} above.)
    Define a function $\hat{f}:{\cal F} \,{\rightarrow}\, {\RR}$ by the rule
        \begin{displaymath}
        \hat{f}(S) \,=\, {\sum}_{S} f \mbox{ for each $S$ in ${\cal F}$}.
        \end{displaymath}
    Then the unordered sum ${\sum}_{{\cal F}} \hat{f}$ is defined.
    Furthermore, one has
        \begin{equation}
        \label{EqnG.20}
        {\sum}_{X} f \,=\, {\sum}_{{\cal F}} \hat{f}; \mbox{ that is, }
        \sum_{X} f  \,=\, \sum_{S{\in}{\cal F}} \sum_{S} f.
        \end{equation}

\V
        
        {\bf Proof}

\V

        The proof breaks naturally into two cases.

        \underline{Case 1} Suppose that ${\sum}_{{\cal F}} \hat{f}^{+} \,=\, +{\infty}$.
    Then for every real number $M\,>\,0$ there exists a finite nonempty subset ${\cal W}$ of ${\cal F}$ such that ${\sum}_{{\cal W}} \hat{f}^{+}\,\,{\geq}\,\,2M$.
    Suppose that ${\cal W}$ has exactly $r$ distinct elements $S_{1}$,\,{\ldots}\,$S_{r}$, so that
        \begin{displaymath}
        {\sum}_{{\cal W}} \hat{f}^{+} \,=\, \hat{f}^{+}(S_{1}) + \,{\ldots}\,+\hat{f}^{+}(S_{r}).
        \end{displaymath}
    By definition of the `positive part' of a function, for each $j \,=\, 1,2,\,{\ldots}\,r$ one has
        \begin{displaymath}
        \hat{f}^{+}(S_{j}) \,=\, \left\{
        \begin{array}{cl}
        \hat{f}(S_{j}) & \mbox{if $\hat{f}(S_{j})\,\,{\geq}\,\,0$} \\
            0          & \mbox{if $\hat{f}(S_{j})\,<\,0$}
        \end{array}
                                            \right.
        \end{displaymath}
    Since, by definition, $\hat{f}(S_{j}) \,=\, {\sum}_{S_{j}} f$, and since $ {\sum}_{S_{j}} f\,\,{\leq}\,\,{\sum}_{S_{j}} f^{+}$,
    it follows easily that $\sum_{S_{j}} f^{+}\,\,{\geq}\,\,\hat{f}^{+}(S_{j})$.
    Thus one has
        \begin{displaymath}
        {\sum}_{S_{1}} f^{+} + \,{\ldots}\,+ {\sum}_{S_{r}} f^{+}\,\,{\geq}\,\,
    \hat{f}(S_{1}) + \,{\ldots}\, + \hat{f}(S_{r}) \,\,{\geq}\,\,2M.
        \end{displaymath}
    From the definition of ${\sum}_{S_{j}} f^{+}$ there exists a finite nonempty subset $W_{j}$ of $S_{j}$ such that
        \begin{displaymath}
        {\sum}_{W_{j}} f^{+}\,\,{\geq}\,\,\frac{1}{2}{\sum}_{S_{j}} f^{+}.
        \end{displaymath}
    Let $W \,=\, W_{1}\,{\cup}\,\,{\ldots}\,\,{\cup}\,W_{r}$, so that $W$ is a finite nonempty subset of $X$.
    Note that the sets $W_{1}$,\,{\ldots}\,$W_{r}$ in this union are mutually disjoint, since $W_{j} \,{\subseteq}\, S_{j}$ and the sets $S_{1}$,\,{\ldots}\,$S_{r}$ are distinct elements of ${\cal F}$ and ${\cal F}$ satisfies the `Disjointness Property' for partitions.
    Thus, Theorem~\Ref{ThmB10.60}, the `Generalized Associative Law' (for finite unordered sums) can be combined with the preceding results to yield
        \begin{displaymath}
        {\sum}_{W} f^{+} \,=\, {\sum}_{W_{1}} f^{+} + \,{\ldots}\,+ {\sum}_{W_{r}} f^{+}
        \,\,{\geq}\,\,\frac{1}{2}\left({\sum}_{S_{1}} f^{+} + \,{\ldots}\,+ {\sum}_{S_{r}} f^{+}\right)\,\,{\geq}\,\,\frac{1}{2}{\cdot}(2M) \,=\, M.
        \end{displaymath}
    Since $M$ can be any positive real number, it follows that ${\sum}_{W} f^{+} \,=\, +{\infty}$.
    A similar argument shows that if ${\sum}_{{\cal F}} \hat{f}^{-} \,=\, +{\infty}$ then ${\sum}_{X} f^{-} \,=\, +{\infty}$.
    Since, by hypothesis, ${\sum}_{X} f$ is defined, it cannot be the case that {\em both} ${\sum}_{X} f^{+}$ and ${\sum}_{X} f^{-}$ equal $+{\infty}$.
    Thus, by what has just been  proved, it cannot happen that both ${\sum}_{{\cal F}} \hat{f}^{+}$ and ${\sum}_{{\cal F}} \hat{f}^{-}$ equal $+{\infty}$.
    In other words, the unordered sum ${\sum}_{{\cal F}} \hat{f}$ is defined.
    Moreover, the preceding also shows that if ${\sum}_{{\cal F}} \hat{f}$  diverges to either $+{\infty}$ or $-{\infty}$,
    then the same holds for ${\sum}_{X} f$, and one has ${\sum}_{X} f \,=\, {\sum}_{{\cal F}} \hat{f}$, as required.

        \underline{Case 2} Assume that ${\sum}_{{\cal F}} \hat{f}$ is convergent, and set $C \,=\, {\sum}_{{\cal F}} \hat{f}$.
    Let ${\varepsilon}\,>\,0$ be given. Then, by Theorem~\Ref{ThmG20.50}, there exists a finite nonempty subset ${\cal W}_{{\varepsilon}}$ of ${\cal F}$ such that if ${\cal W}$ is any finite subset of ${\cal F}$ which satisfies ${\cal W} \,{\supseteq}\, {\cal W}_{{\varepsilon}}$, then
        \begin{displaymath}
        \left|C-{\sum}_{{\cal W}} \hat{f}\right|\,<\,\frac{{\varepsilon}}{2}.
        \end{displaymath}
    Suppose that the finite subset ${\cal W}_{{\varepsilon}}$ has exactly $r$ members, namely $S_{1}$, \,{\ldots}\,$S_{r}$.
    By Theorem~\Ref{ThmG20.50} again, for each $j \,=\, 1,2,\,{\ldots}\,r$ there exists a finite nonempty subset $W_{j{\varepsilon}}$ of $S_{j}$ such that if $W_{j}$ is any finite nonempty subset of $S_{j}$ satisfying $W_{j} \,{\supseteq}\, W_{j{\varepsilon}}$, then
        \begin{displaymath}
        \left|\left({\sum}_{S_{j}} f\right) - \left({\sum}_{W_{j}} f\right)\right|\,<\,\frac{{\varepsilon}}{2r}.
        \end{displaymath}
    Now let $W_{{\varepsilon}} \,=\, W_{1{\varepsilon}}\,{\cup}\,\,{\ldots}\,\,{\cup}\,W_{r{\varepsilon}}$, and let $W$ be any finite subset of $X$ such that $W \,{\supseteq}\, W_{{\varepsilon}}$.
For each $j \,=\, 1,\,{\ldots}\,r$ let $W_{j} \,=\, W\,{\cap}\,S_{j}$. Then the sets $W_{1}$, \,{\ldots}\, $W_{r}$ are mutually disjoint
    (because the sets $S_{1}$,\,{\ldots}\,$S_{r}$ are mutually disjoint) and $W_{j} \,{\supseteq}\, W_{j{\varepsilon}} \,\,{\neq}\,\, {\emptyset}$ (because $W \,{\supseteq}\, W_{{\varepsilon}}$).
    Note that, by the usual `cancellation' properties of finite sums, one has ${\sum}_{W} f - C \,=\, \left({\sum}_{W_{1}} f + \,{\ldots}\,+{\sum}_{W_{r}} f\right) - C  \,=\,$
        \begin{displaymath}
    \left({\sum}_{W_{1}} f - {\sum}_{S_{1}} f\right) + \left({\sum}_{W_{2}} f - {\sum}_{S_{2}} f\right) + \,{\ldots}\,+ \left({\sum}_{W_{r}} f - {\sum}_{S_{r}} f\right) + \left({\sum}_{S_{1}} f + \,{\ldots}\,+ {\sum}_{S_{r}} f - C\right).
        \end{displaymath}
    Now apply the `Extended Triangle Inequality' (see Theorem~\Ref{ThmB20.65}) to get
        \begin{displaymath}
        |{\sum}_{W} f - C|\,\,{\leq}\,\,
    \left|{\sum}_{W_{1}} f - {\sum}_{S_{1}} f\right| + \left|{\sum}_{W_{2}} f - {\sum}_{S_{2}} f\right| + \,{\ldots}\,+ \left|{\sum}_{W_{r}} f - {\sum}_{S_{r}} f\right| + \left|{\sum}_{S_{1}} f + \,{\ldots}\,+ {\sum}_{S_{r}} f - C\right|
    \]
    \[
\,<\,\frac{{\varepsilon}}{2r} + \,{\ldots}\,+\frac{{\varepsilon}}{2r} + \frac{{\varepsilon}}{2} \,=\, {\varepsilon}.
        \end{displaymath}
    The desired result now follows from Theorem~\Ref{ThmG20.50}.

\V
\V

            \subsection{\small{\bf Examples}}
            \label{ExampG20.80}

    \hspace*{\parindent}(1) Let $X \,=\, {\NN}{\times}{\NN}$, and define $f:X \,{\rightarrow}\, {\RR}$ by the rule
        \begin{displaymath}
        f(i,j) \,=\, \frac{1}{2^{i+j}} \mbox{ for all $(i,j)$ in $X$}
        \end{displaymath}
    It is useful to think of the set $X$ and the function $f$ in terms
    of the following `infinite matrix':
        \begin{displaymath}
        \left[
        \begin{array}{lllclc}
        {\displaystyle \frac{1}{2^{2}}} & {\displaystyle \frac{1}{2^{3}}} & {\displaystyle \frac{1}{2^{4}}} & \,{\ldots}\, & {\displaystyle \frac{1}{2^{1+k}}} & \,{\ldots}\, \\
    &  &  &  &  &  \\
        {\displaystyle \frac{1}{2^{3}}} & {\displaystyle \frac{1}{2^{4}}} & {\displaystyle \frac{1}{2^{5}}} & \,{\ldots}\, & {\displaystyle \frac{1}{2^{2+k}}} & \,{\ldots}\, \\
    &  &  &  &  &  \\
        {\displaystyle \frac{1}{2^{4}}} & {\displaystyle \frac{1}{2^{5}}} & {\displaystyle \frac{1}{2^{6}}} & \,{\ldots}\, & {\displaystyle \frac{1}{2^{3+k}}} & \,{\ldots}\, \\
    &  &  &  &  &  \\
{\vdots} &  &  &  &  & {\vdots}  \\
        \end{array}
                \right.
        \end{displaymath}
    More precisely, the entry in this matrix in the $(i,j)$-location (i.e., Row~\#$i$ and Column~\#$j$) is the number $f(i,j) \,=\, 1/2^{i+j}$.
    Then the task of computing ${\sum}_{X} f$ can be interpreted as `to add up all the entries of this matrix'.

        Suppose that $W$ is a finite nonempty subset of $X$; thus, it is possible to list the distinct elements of $W$ as
        \begin{displaymath}
        W \,=\, \{(i_{1},j_{1}), (i_{2},j_{2}),\,{\ldots}\,(i_{m},j_{m})\}
        \end{displaymath}
    where $m$ is the exact number of elements of the set $W$.
    Let $k \,=\, \max\,\{i_{1},i_{2},\,{\ldots}\,i_{m},j_{1},j_{2},\,{\ldots}\,j_{m}\}$;
    that is, $k$ is the largest of all the numbers which appear in the ordered pairs $(i,j)$ in $W$.
    Let $Y \,=\, {\NN}_{k}{\times}{\NN}_{k}$.
    Clearly $W \,{\subseteq}\, Y$, and thus one has
        \begin{displaymath}
        {\sum}_{W} f\,\,{\leq}\,\,{\sum}_{Y} f.
        \end{displaymath}
    By definition, the number ${\sum}_{Y} f$ is the sum of the numbers $1/2^{i+j}$ with $1\,\,{\leq}\,\,i,j\,\,{\leq}\,\,k$.
    By the Generalized Commutative and Associative laws for Addition (see Theorems~\Ref{ThmB10.32} and~\Ref{ThmB10.60}),
    one can obtain this latter sum by grouping together the elements of ${\NN}_{k}{\times}{\NN}_{k}$ `row-by-row', as follows:
        \begin{displaymath}
        {\sum}_{Y} f \,=\, \left(\frac{1}{2^{2}}+\frac{1}{2^{3}} + \frac{1}{2^{4}} + \,{\ldots}\,+\frac{1}{2^{1+k}}\right) +
    \left(\frac{1}{2^{3}}+\frac{1}{2^{4}} + \frac{1}{2^{5}} + \,{\ldots}\,+\frac{1}{2^{2+k}}\right) +
    \left(\frac{1}{2^{4}}+\frac{1}{2^{5}} + \frac{1}{2^{6}} + \,{\ldots}\,+\frac{1}{2^{3+k}}\right) +
        \end{displaymath}
        \begin{displaymath}
\,{\ldots}\, + \left(\frac{1}{2^{k+1}}+\frac{1}{2^{k+2}} + \frac{1}{2^{k+3}} + \,{\ldots}\,+\frac{1}{2^{2k}}\right).
        \end{displaymath}
    Note that
        \begin{displaymath}
        \frac{1}{2^{j+1}}+\frac{1}{2^{j+2}} + \frac{1}{2^{j+3}} + \,{\ldots}\,+\frac{1}{2^{j+k}} \,=\, \left(\frac{1}{2^{j}}\right)\left(\frac{1}{2^{1}}+\frac{1}{2^{2}} + \frac{1}{2^{3}} + \,{\ldots}\,+\frac{1}{2^{k}}\right)
        \end{displaymath}
    It follows from the results of Example~\Ref{ExampC20.20}~(4), with $r$ in that example equalling $1/2$, that
        \begin{displaymath}
        \frac{1}{2^{1}}+\frac{1}{2^{2}} + \frac{1}{2^{3}} + \,{\ldots}\,+\frac{1}{2^{k}}\,<\,1, \mbox{ hence } \left(\frac{1}{2^{j}}\right)\left(\frac{1}{2^{1}}+\frac{1}{2^{2}} + \frac{1}{2^{3}} + \,{\ldots}\,+\frac{1}{2^{k}}\right)\,<\,1.
        \end{displaymath}
    Thus one gets
        \begin{displaymath}
        {\sum}_{Y} f\,\,{\leq}\,\,\frac{1}{2} + \frac{1}{2^{2}} + \,{\ldots}\,+\frac{1}{2^{k}}\,<\,1
        \end{displaymath}
    for all $k$ in ${\NN}$.
Hence,
        \begin{displaymath}
        {\sum}_{W} f\,<\,1 \mbox{ for every finite nonempty subset $W$ of $X$}.
        \end{displaymath}
    In particular, ${\sum}_{X} f\,\,{\leq}\,\,1$.

        To compute the actual value of ${\sum}_{X} f$, consider the partition of $X$ `by rows'.
    That is, let ${\cal F}$ denote the infinite family of nonempty subsets $S_{1}$, $S_{2}$,\,{\ldots}\,of $W$ given by the rule
        \begin{displaymath}
        S_{i} \,=\, \{(i,1), (i,2),\,{\ldots}\,(i,k),\,{\ldots}\,\} \mbox{ for each $i$ in ${\NN}$}.
        \end{displaymath}
    Note that for each such $i$ one has
        \begin{displaymath}
        f(i,j) \,=\, \frac{1}{2^{i+j}} \,=\, \frac{1}{2^{i+1}}\frac{1}{2^{j-1}} \,=\, 
    \frac{1}{2^{i+1}}g(j),
        \end{displaymath}
    where $g:{\NN} \,{\rightarrow}\, {\RR}$ is the function discussed in Example~\Ref{ExampG20.35}~(4) above, with $r$ in that example equalling~$1/2$.
    It is clear that the family ${\cal F}$ is a partition of $X$, and 
        \begin{displaymath}
        {\sum}_{S_{i}} f \,=\, \frac{1}{2^{i+1}}{\sum}_{{\NN}} g \,=\, 
    \frac{1}{2^{i}} \,=\, \frac{1}{2}g(i).
        \end{displaymath}
    Now use Theorem~\Ref{ThmG20.70}, the Generalized Associative Law for Unordered Sums, to conclude
        \begin{displaymath}
        {\sum}_{X} f \,=\, {\sum}_{{\cal F}} \hat{f} \,=\, \frac{1}{2}{\sum}_{{\NN}} g \,=\, \frac{2}{2} \,=\, 1.
        \end{displaymath}

\V

        (2) Let $X \,=\, {\NN}{\times}{\NN}$, as in the preceding example, but now define $f:X \,{\rightarrow}\, {\RR}$ by the rule
        \begin{displaymath}
        f(i,j) \,=\, \left\{
                \begin{array}{rl}
        1 & \mbox{if $j \,=\, i$}   \\
       -1 & \mbox{if $j \,=\, i+1$} \\
        0 & \mbox{in all other cases}
        \end{array}
                                \right.
        \end{displaymath}
    As in the preceding example, it is possible to visualize the set $X$ and the function $f$ as an infinite matrix:
        \begin{equation}
        \label{MatrixG.40}
        \left[
        \begin{array}{rrrrrrc}
        1 & -1 &  0 &  0 &  0 & 0 & \,{\ldots}\, \\
        0 &  1 & -1 &  0 &  0 & 0 & \,{\ldots}\, \\
        0 &  0 &  1 & -1 &  0 & 0 & \,{\ldots}\, \\
        0 &  0 &  0 &  1 & -1 & 0 & \,{\ldots}\, \\
 {\vdots} &    &    &    &    &   & {\vdots}
        \end{array}
                \right.
        \end{equation}
    It is clear that there are infinitely many points $(i,j)$ in $X$ at which $f(i,j) \,=\, +1$; namely, the points corresponding to the main diagonal of this matrix.
    In particular, one has ${\sum}_{X_{f;+}} f \,=\, +{\infty}$.
    A similar argument shows that ${\sum}_{X_{f;-}} f \,=\, -{\infty}$.
    In particular, the unordered sum ${\sum}_{X} f$ is {\em not} defined.

        Nevertheless, it is possible to find a partition ${\cal F}$ of $X$ such that ${\sum}_{Y} f$ {\em is} defined, and even convergent, for each $Y$ in ${\cal F}$, and so that ${\sum}_{{\cal F}} \hat{f}$ is also converent.
    For instance, if $i{\in}{\NN}$, let $Y_{i} \,=\, \{(i,1), (i,2),\,{\ldots}\,(i,k),\,{\ldots}\,\}$.
    Thus, $Y_{i}$ corresponds to the $i$-th row of the matrix above.
    It is clear that $X$ is the disjoint union of the nonempty subsets $Y_{1}$, $Y_{2}$,\,{\ldots}\,, so that the collection ${\cal F} \,=\, \{Y_{1},Y_{2},\,{\ldots}\,\}$ is a partition of $X$.
    It is also clear that each row of Matrix~\Ref{MatrixG.40} has exactly one entry equal to $+1$, exactly one term equal to $-1$, and the remaining entries equal to $0$.
    From this one concludes that
        \begin{displaymath}
        {\sum}_{Y_{i}} f \,=\, 0 \mbox{ for each $i$ in ${\NN}$}
        \end{displaymath}
    In particular, for each $i$ the unordered sum ${\sum}_{Y_{i}} f$ is convergent; in fact it equals~$0$.
    Thus one can write
        \begin{displaymath}
        \hat{f}(Y_{i}) \,=\, {\sum}_{Y_{i}} f \,=\, 0 \mbox{ for all $i$ in ${\NN}$}.
        \end{displaymath}
    By Theorem~\Ref{ThmG20.40} it follows that
        \begin{displaymath}
        {\sum}_{{\cal F}} \hat{f} \,=\, 0.
        \end{displaymath}

        Now  consider a second partition, ${\cal G}$, of $X$, obtained from the {\em columns} of Matrix~\Ref{MatrixG.40} above.
    More precisely, for each $j$ in ${\NN}$ let $Z_{j} \,=\, \{(1,j), (2,j),\,{\ldots}\,(k,j),\,{\ldots}\,\}$.
    Clearly the sets $Z_{j}$, $j{\in}{\NN}$, are mutually disjoint and ${\cal G} \,=\, \{Z_{1},Z_{2},\,{\ldots}\,\}$ is a partition of $X$.
    Note that, with one exception, every column of the matrix has exactly one entry equal to~$1$, exactly one entry equal to~$-1$, and all other entries equal to~$0$.
    The one exception is the first column, which has exactly one entry equal to $1$ and all others equal to $0$.
    It follows easily that for each $j$ the unordered sum ${\sum}_{Z_{j}} f$ is convergent.
    More precisely,
        \begin{displaymath}
        {\sum}_{Z_{j}} f \,=\, \left\{
        \begin{array}{rl}
        1 & \mbox{if $j \,=\, 1$} \\
        0 & \mbox{if $j\,\,{\geq}\,\,2$}.
        \end{array}
                                            \right.
        \end{displaymath}
    Now set $f^{\#}:{\cal G} \,{\rightarrow}\, {\RR}$ by the rule
        \begin{displaymath}
        f^{\#}(Z_{j}) \,=\, {\sum}_{Z_{j}} f.
        \end{displaymath}
    Clearly $f^{\#}(j) \,=\, 1$ if $j \,=\, 1$, $f^{\#}(j) \,=\, 0$ if $j\,\,{\geq}\,\,2$.
    Thus one has
        \begin{displaymath}
        {\sum}_{{\cal G}} f^{\#} \,=\, 1.
        \end{displaymath}

\V

        (3) Let $f:{\NN} \,{\rightarrow}\, {\RR}$ and $g:{\NN} \,{\rightarrow}\, {\RR}$ be real-valued functions defined on ${\NN}$,
    and suppose that both of the infinite unordered sums $\sum_{{\NN}} f$ and $\sum_{{\NN}} g$ are convergent, with (finite) values $A$ and $B$, respectively.
    Define $h:{\NN}{\times}{\NN} \,{\rightarrow}\, {\RR}$ by the rule $h(i,j) \,=\, f(i)g(j)$ for each $(i,j)$ in ${\NN}{\times}{\NN}$.
    Then it is easy to show that $\sum_{{\NN}{\times}{\NN}} h$ is convergent, and that its value is $AB$.


\V
\V


        It was pointed out after Example~(1) above that the quantity ${\sum}_{X} f$ can sometimes be defined even when the set $X$ is uncountable.
    In contrast, the next result explains why we normally restrict our attention to the case in which $X$ is countable.

\V
\V

            \subsection{\small{\bf Theorem}}
            \label{ThmG20.100}

        Suppose that $f:X \,{\rightarrow}\, {\RR}$ is a real-valued function whose domain is an uncountable set $X$.
    Then there exists a countable nonempty subset $Y$ of $X$ such that the behavior of the unordered sum ${\sum}_{X} f$ is the same as the behavior of the unordered sum ${\sum}_{Y} f$.

        More precisely, there exists a countable nonempty subset $Y$ of $X$ such that
        \begin{displaymath}
        {\sum}_{Y} f^{+} \,=\, {\sum}_{X} f^{+} \mbox{ and }
        {\sum}_{Y} f^{-} \,=\, {\sum}_{X} f^{-}
        \end{displaymath}

\V

        {\bf Proof}

\V

        For convenience set $A \,=\, {\sum}_{X} f^{+}$ and $B \,=\, {\sum}_{X} f^{-}$.

        \h Case (i)\,\, Suppose that $A \,=\, 0$. Then, by Part~(a) of Theorem~\Ref{ThmG20.40}, one has $f(x) \,=\, 0$ for all $x$ in $X$.
    In this case, let $Y^{+}$ be any nonempty countable subset of $X$; clearly ${\sum}_{Y^{+}} f^{+} \,=\, 0 \,=\, A$.

        \h Case (ii)\, Suppose $0\,<\,A\,<\,+{\infty}$.
    For each $m$ in ${\NN}$ let $X^{+}_{m}$ denote the set of all $x$ in $X$ such that $f^{+}(x)\,\,{\geq}\,\,1/m$.

        \underline{Claim} The set $X^{+}_{m}$ is finite. More precisely, if $k$ is a positive integer such that $k\,>\,mA$, then $X^{+}_{m}$ has fewer than $k$ elements.

        \underline{Proof of Claim} Suppose that $X^{+}_{m}$ has at least $k$ elements.
    Then there must exist a finite subset $W \,=\, \{x_{1},x_{2},\,{\ldots}\,x_{k}\}$ of $X^{+}_{m}$ with exactly $k$ elements.
    Clearly $f^{+}(x_{j})\,\,{\geq}\,\,1/m$ for each $j \,=\, 1,2,\,{\ldots}\,k$, so
        \begin{displaymath}
        {\sum}_{W} f^{+} \,=\, f(x_{1}) + f(x_{2}) + \,{\ldots}\,+ f(x_{k})\,\,{\geq}\,\, \frac{k}{m}\,>\,\frac{mA}{m} \,=\, A.
        \end{displaymath}
    By Part~(b) of Theorem~\Ref{ThmG20.40} one then has ${\sum}_{X} f^{+}\,\,{\geq}\,\,{\sum}_{W} f^{+}\,>\,A$, contrary to the definition of $A$.
    That is, assuming that $X^{+}_{m}$ is an infinite set leads to a contradiction; thus, $X_{m}$ is a finite set, as claimed.

        Now let $Y^{+} \,=\, {\bigcup}_{m=1}^{{\infty}} X^{+}_{m}$. Note that $Y^{+}$, being the countable union of finite sets,
    is a countable set; see Theorem~\Ref{ThmA20.70}.
    In addition, since $A\,>\,0$ it is clear that there exist at least one $x$ in $X$ such that $f^{+}(x)\,>\,0$. For each such $x$ there exists $m$ in ${\NN}$ such that $x{\in}X^{+}_{m}$, and thus $Y^{+}$ is nonempty.
    Indeed, the Archimedean Principle guarantees that there exists $m$ such that $m\,>\,1/f^{+}(x)$, and thus $f^{+}(x)\,>\,1/m$; that is, $x{\in}X^{+}_{m}$.
    Finally, it follows from Part~(b) of Theorem~\Ref{ThmG20.40} (and the definition of $f^{+}$) that ${\sum}_{Y^{+}} f^{+} \,=\, {\sum}_{X} f^{+} \,=\, A$, as required.

        \h Case (iii) Suppose that $A \,=\, +{\infty}$. Then for every $m$ in ${\NN}$ there exists a finite nonempty subset $W_{m}$ of $X$  such that ${\sum}_{W_{m}} f^{+}\,\,{\geq}\,\,m$.
    Let $Y^{+} \,=\, {\bigcup}_{m=1}^{{\infty}} W_{m}$. The set $Y^{+}$ is the countable union of finite sets, hence it is a countable nonempty set.
    It is clear that ${\sum}_{Y^{+}} f^{+} \,=\, +{\infty} \,=\, A$.

        Conclusion: In all cases, there exists a countable nonempty set $Y^{+}$ such that ${\sum}_{Y^{+}} f^{+} \,=\, {\sum}_{X} f^{+}$.

        In a similar manner one can prove that there exists a countable nonempty subset $Y^{-}$ of $X$ such that ${\sum}_{Y^{-}} f^{-} \,=\, B \,=\, {\sum}_{X} f^{-}$.
    (Alternatively, simply apply what was just proved to the function $g^{+}$ when $g \,=\, -f$.)

        Finally, let $Y \,=\, Y^{+}\,{\cup}\,Y^{-}$. Clearly ${\sum}_{Y} f^{+} \,=\, {\sum}_{Y^{+}} f^{+}$,
    since if $x{\in}Y{\setminus}Y^{+}$, then $f(x)\,\,{\leq}\,\,0$, hence $f^{+}(x) \,=\, 0$.
    Thus, ${\sum}_{Y} f^{+} \,=\, A \,=\, {\sum}_{X} f^{+}$, as required.
    Likewise, ${\sum}_{Y} f^{-} \,=\, B \,=\, {\sum}_{X} f^{-}$.

\V

            \subsection{\small{\bf Remarks}}
            \label{RemrkG20.110}

\V

\hspace*{\parindent}
        (1) The sense in which `the behavior of the unordered sum ${\sum}_{X} f$ is the same as the behavior of the unordered sum ${\sum}_{Y} f$' is this:
    the sum ${\sum}_{X} f$ exists if, and only if, the sum ${\sum}_{Y} f$ exists;
    and if these sums exist, then they are equal.

\V

        (2) If ${\sum}_{X} f$ is convergent, then the proof above shows that the set $Z \,=\, \{x{\in}X: f(x) \,\,{\neq}\,\, 0\}$ must be countable.


\V
\V


    In light of the preceding results, it makes sense to narrow our inquiry to the case in which $X$ is countably infinite.
    The next result shows how to relate unordered sums in that situation to the theory of limits of sequences studied in Chapter~\Ref{ChaptC}.

\V
\V

            \subsection{\small{\bf Theorem}}
            \label{ThmG20.120}

        Let $f:X \,{\rightarrow}\, {\RR}$ be a real-valued function defined on a \underline{countably} infinite set $X$.

\V


    (a) Suppose that the unordered sum $\sum_{X} f$ is defined.
    Let ${\varphi}:{\NN} \,{\rightarrow}\, X$ be a bijection of ${\NN}$ onto $X$;
    and associate with $f$ and ${\varphi}$ the real-valued sequence ${\sigma}_{f;{\varphi}} \,=\, (s_{1},s_{2},\,{\ldots}\,)$ given by the rule
        \begin{displaymath}
        s_{k} \,=\, f({\varphi}(1)) + f({\varphi}(2)) + \,{\ldots}\,+ f({\varphi}(k)) \mbox{ for each $k$ in ${\NN}$} \h ({\ast})
        \end{displaymath}
    Then the sequence ${\sigma}_{f;{\varphi}}$ has a limit, and one has
        \begin{equation}
        \label{EqnG.50}
        {\sum}_{X} f \,=\, \lim_{k \,{\rightarrow}\, {\infty}} s_{k}
        \end{equation}

\V

        (b) Conversely, for each bijection ${\varphi}:{\NN} \,{\rightarrow}\, X$,
    define a corresponding sequence ${\sigma}_{f;{\varphi}} \,=\, (s_{1},\,{\ldots}\,s_{k},\,{\ldots}\,)$ by means of Equation~$({\ast})$. 
    Assume that there is a quantity $L$, where $L$ is either a real number or one of the infinities, 
    such that $\lim\, {\sigma}_{f;{\varphi}} \,=\, L$ for each such bijection ${\varphi}$.
    Then the unordered sum $\sum_{X} f$ exists and equals $L$.

\V

        {\bf Proof} (a) For convenience let us set $u_{i} \,=\, f({\varphi}(i))$ for each $i$ in ${\NN}$.
    Then one can write $s_{k} \,=\, u_{1}+u_{2}+\,{\ldots}\,+u_{k}$ for each index $k$.

        \underline{Special Case} Suppose that $f(x)\,\,{\geq}\,\,0$ for each $x$ in $X$.
    It follows that each of the numbers of the form $u_{i}$ is nonnegative and thus the sequence ${\sigma}_{f;{\varphi}}$ is monotonic up.
    In particular, the quantity $\lim_{k \,{\rightarrow}\, {\infty}} s_{k}$ exists. For convenience, we denote this limit by $L$; of course it is possible that $L \,=\, +{\infty}$.
    In any event, one certainly has $s_{m}\,\,{\leq}\,\,L$ for all indices $m$.

    By Definition~\Ref{DefG20.20}, ${\sum}_{X} f \,=\, {\sup}\,U_{X;f}$, with $U_{X;f}$ being the set of numbers of the form $f(x_{1})+f(x_{2})+ \,{\cdots}\, +f(x_{m})$,     
    and  $\{x_{1},x_{2},\,{\ldots}\,,x_{m}\}$ ranging over the finite subsets of $X$.
    (Of course the notation assumes that the $x$'s are distinct.)
    For such a set one can write $x_{i} \,=\, {\varphi}(j_{i})$ for certain (distinct) elements $j_{1},j_{2},\,{\ldots}\,j_{m}$ in ${\NN}$.
    Let $p$ be the largest of the indices $j_{1}$, $j_{2}$,\,{\ldots}\,$j_{m}$. Then clearly
        \begin{displaymath}
        f(x_{1})+f(x_{2})+ \,{\cdots}\, +f(x_{m}) \,=\, u_{j_{1}}+u_{j_{2}}+\,{\ldots}\,+u_{j_{m}}\,\,{\leq}\,\,u_{1}+u_{2}+\,{\ldots}\,+u_{p},
        \end{displaymath}
    since the sum $u_{1}+u_{2}+\,{\ldots}\,+u_{p}$, which has only nonnegative terms, includes all the terms which appear in the sum $u_{j_{1}}+u_{j_{2}}+\,{\ldots}\,+u_{j_{m}}$.
    It follows that $L$ is an upper bound for the set $U_{X;f}$, and thus
        \begin{displaymath}
        {\sum}_{X} f\,\,{\leq}\,\,L
        \end{displaymath}
    However, if $z$ is any number such that $z\,<\,L$, then there exists $k$ such that $z\,<\,s_{k}\,\,{\leq}\,\,L$.
    Since $s_{k}$ is certainly an element of $U_{X;f}$, it follows that for every such $z$ one has $z\,<\,{\sum}_{X} f$.
    This implies that ${\sum}_{X} f\,\,{\geq}\,\,L$ as well. Thus, ${\sum}_{X} f \,=\, L$.

        The general case now follows by applying the results of the Special Case above to the functions $f^{+}$ and $f^{-}$, and using the equation $\sum_{X} f \,=\, \sum_{X} f^{+} - \sum_{X} f^{-}$.

\V

        (b) Assume that the desired conclusion, namely that $\sum_{X} f \,=\, L$, is {\em not} true.
    We shall show that this implies the existence of a bijection ${\varphi}:{\NN} \,{\rightarrow}\, X$
    for which it is {\em not} the case that $\lim\, {\sigma}_{f;{\varphi}} \,=\, L$, in contradiction to the hypotheses.

    Consider first the case in which $L$ is finite.
    Then, by Theorem~\Ref{ThmG20.50}, there would exist ${\varepsilon}_{0}\,>\,0$ such that for every finite subset $W'$ of $X$
    there must exist a finite subset $W$ of $X$ such that $W \,{\supseteq}\, W'$ and $\left|L-\sum_{W} f\right|\,\,{\geq}\,\,{\varepsilon}_{0}$.
    It is clear that one can choose this $W$ so that $W$ is a {\em proper} superset of $W'$; that is, $W{\setminus}W' \,\,{\neq}\,\, {\emptyset}$.

        Express the countably infinite set $X$ as $X \,=\, \{x_{1},x_{2},\,{\ldots}\,\}$,
    and recursively define a sequence of finite subsets $W_{1}$, $W_{2}$,\,{\ldots}\, of $X$ as follows:

       \h (1) $W_{1}$ is a finite subset of $X$ such that $W_{1} \,{\supseteq}\, \{x_{1}\}$ and $\left|L-\sum_{W_{1}} f\right|\,\,{\geq}\,\,{\varepsilon}_{0}$.

       \h (2) If $W_{k}$ has been defined, let $W_{k+1}$ be a finite subset of $X$ such that $W_{k+1}$ is a {\em proper} superset of $W_{k}\,{\cup}\,\{x_{k+1}\}$, and $\left|L-\sum_{W_{k+1}} f\right|\,\,{\geq}\,\,{\varepsilon}_{0}$
    
    It is clear that ${\bigcup}_{k=1}^{{\infty}} W_{k} \,=\, X$; indeed, by construction one has that $x_{k}{\in}W_{k}$ for each $k$.
    (Of course it is possible that a given $x_{k}$ might well be in $W_{j}$ for some $j\,<\,k$.)
    It then follows that one can write
        \begin{displaymath}
        X \,=\, W_{1}\,{\cup}\,\left(W_{2}{\setminus}W_{1}\right) \,{\cup}\,
    \,{\ldots}\, \,{\cup}\, \left(W_{k}{\setminus}W_{k-1}\right) \,{\cup}\, \,{\ldots}\,,
        \end{displaymath}
and that this is a {\em disjoint} union; that is, it is a partition of $X$. Let $m_{1} \,=\, \#\,(W_{1})$ and for $k\,\,{\geq}\,\,2$ let $m_{k} \,=\, \#\,(W_{k}{\setminus}W_{k-1})$.
    Note that $m_{k}\,\,{\geq}\,\,1$ for each $k$, since the sets $W_{1}$ and (if $k\,\,{\geq}\,\,2$) $W_{k}{\setminus}W_{k-1}$ have been constructed to be nonempty.
    Then these numbers determine a corresponding partition of ${\NN}$. Indeed, let $n_{1} \,=\, m_{1}$, $n_{2} \,=\, m_{1}+m_{2}$, \,{\ldots}\,$n_{k} \,=\, m_{1} + \,{\ldots}\,+ m_{k}$. Then
        \begin{displaymath}
        {\NN} \,=\, {\NN}_{n_{1}}\,{\cup}\,\left({\NN}_{n_{2}}{\setminus}{\NN}_{n_{1}}\right)\,{\cup}\,\,{\ldots}\,\left({\NN}_{n_{k}}{\setminus}{\NN}_{n_{k-1}}\right)\,{\cup}\,\,{\ldots}\,
        \end{displaymath}
    It follows from Theorem~\Ref{ThmA50.88A} that there is a bijection ${\varphi}:{\NN} \,{\rightarrow}\, X$ such that ${\varphi}$ maps ${\NN}_{n_{1}}$ bijectively onto $W_{1}$,
    and if $k\,\,{\geq}\,\,2$ then ${\varphi}$ maps ${\NN}_{n_{k}}{\setminus}{\NN}_{n_{k-1}}$ bijectively onto $W_{k}{\setminus}W_{k-1}$.
    It is clear that for each $k$ one has
        \begin{displaymath}
        f({\varphi}(1)) + f({\varphi}(2)) + \,{\ldots}\, + f({\varphi}(n_{k})) \,=\, \sum_{W_{k}} f
        \end{displaymath}
    and thus
        \begin{displaymath}
        \left|L - \left(f({\varphi}(1)) + f({\varphi}(2)) + \,{\ldots}\, + f({\varphi}(n_{k}))\right)\right|\,\,{\geq}\,\,{\varepsilon}_{0}.
        \end{displaymath}
    In particular, the sequence ${\sigma}_{f;{\varphi}}$ has a subsequence which does not converge to $L$, namely $(s_{n_{1}}, s_{n_{2}},\,{\ldots}\,)$
    so it follows that the equation $\lim\,{\sigma}_{f;{\varphi}} \,=\, L$ cannot hold, contrary to the hypotheses.

        A similar argument works when $L$ is one of the infinities; the details are left as an exercise.

\V
\V

            \subsection{\small{\bf Remark}}
            \label{RemrkG20.130}

\V

        The preceding result implies that a sufficient condition for the unordered sum $\sum_{X} f$ to be defined is that, for {\em each} bijection ${\varphi}:{\NN} \,{\rightarrow}\, X$,
    the sequence ${\sigma}_{f;{\varphi}}$ has a limit, and the limit is the same each~${\varphi}$.
    It is natural to ask whether one gets the same conclusions provided one knows only that ${\sigma}_{f;{\varphi}}$ has a limit for {\em at least one} bijection~${\varphi}$.
    We shall see in the next section that the answer is `No'.

\V
\V

        Let us close this section with a result which says, in effect, that relatively small errors in the values of a function $f:X \,{\rightarrow}\, {\RR}$ causes relatively small errors in $\sum_{X} f$.

        The background for this result comes from the problem that in real-life computation,
    one must often make do with {\em approximations} of the actual numbers one hoped to deal with.
    For instance, if one keys in the famous number ${\pi}$ on an inexpensive scientific calculator, one may get
        \begin{displaymath}
        {\pi} \,=\, 3.1415927
        \end{displaymath}
    Of course, everyone knows that a much more accurate answer would be
        \begin{displaymath}
        {\pi} \,=\, 3.14159265358979323846264338327950288419716939937510;
        \end{displaymath}
    but even this is not quite correct. The problem is that in the decimal representation of numbers on a computing device,
    only a finite numbers of decimal digits can be accomodated, so in general the computer must `round off' values to a certain number of significant digits.
    That is, one must approximate the `true' value of a number $y$ by an `approximate value' $\hat{y}$ with which one can actually do the computation.
    For approximations of this type the issue concerns mainly nonzero numbers, and the appropriate type of error to consider is the so-called {\em relative error}.

\V

            \subsection{\small{\bf Definition} (Relative Error)}
            \label{DefG20.135}

\V

        Let $y$ be a nonzero real number, and consider an approximation of the form $y \,{\approx}\, \hat{y}$,
    where $\hat{y}$ is a nonzero number with the same sign as $y$ (ie., both are positive or both are negative).
    Then the {\bf relative error} in this approximation is $E \,=\, {\displaystyle \frac{\hat{y}-y}{y}}$.
    A number ${\varepsilon}\,>\,0$ is an {\bf upper bound on the relative error} if $|E|\,\,{\leq}\,\,{\varepsilon}$; that is, if $-{\varepsilon}\,\,{\leq}\,\,E\,\,{\leq}\,\,{\varepsilon}$.

\V
\V

        {\bf Remark} Let ${\varepsilon}\,>\,0$ be an upper bound on the relative error of the approximation $y \,{\approx}\, \hat{y}$ described above.
    Then it is easy to see that the requirement that $y$ and $\hat{y}$ be of the same sign implies that ${\varepsilon}\,<\,1$.
    More precisely, if $y\,>\,0$ then $(1-{\varepsilon})y\,\,{\leq}\,\,\hat{y}\,\,{\leq}\,\,(1+{\varepsilon})y$.
    Likewise, if $y\,<\,0$ then $(1-{\varepsilon})y\,\,{\geq}\,\,\hat{y}\,\,{\geq}\,\,(1+{\varepsilon})y$.
    If, for instance, the approximation process is `rounding off after $k$ decimal digits',
    then one can take ${\varepsilon} \,=\, 1/10^{k}$.

        Note that another way of expressing the fact that the $\hat{y}$ approximates $y$ with relative error less than ${\varepsilon}$ is this:
    there exists a number $z$ in the interval $[1-{\varepsilon},1+{\varepsilon}]$ such that $\hat{y} \,=\, zy$.
    This formulation is used in the next result.

\V
\V

            \subsection{\small{\bf Theorem} (The Stability Theorem for Unordered Sums)}\IndB{unordered infinite sums of real numbers}{stability theorem for unordered sums}
            \label{ThmG20.140}

\V

        Suppose that $f:X \,{\rightarrow}\, {\RR}$ is a real-valued function defined on a nonempty set $X$.
    For simplicity, assume that for all $x$ in $X$ one has $f(x) \,\,{\neq}\,\, 0$.
    Let ${\varepsilon}$ be such that $0\,<\,{\varepsilon}\,<\,1$, and suppose that $g:X \,{\rightarrow}\, {\RR}$ is a function such that $g(x){\in}(1-{\varepsilon},1+{\varepsilon})$ for all $x$ in~$X$.
    Define $\hat{f}:X \,{\rightarrow}\, {\RR}$ by the rule $\hat{f}(x) \,=\, g(x)f(x)$ for all $x$ in~$X$. Then the following hold:

\V

        (a) If the sum $\sum_{X} f$ is undefined, then so is the sum $\sum_{X} \hat{f}$.

\V

        (b) If the sum $\sum_{X} f$ is defined, then so is $\sum_{X} \hat{f}$. More precisely:

    \h (i)\, if $\sum_{X} f$ diverges to one of the infinities, then $\sum_{X} \hat{f}$ diverges to the same infinity;

    \h (ii) if $\sum_{X} f$ converges, so does $\sum_{X} \hat{f}$. Futhermore, there exists a constant $B$, independent of ${\varepsilon}$, such that
        \begin{displaymath}
        \left|\left(\sum_{X} f\right) - \left(\sum_{X} \hat{f}\right)\right|\,\,{\leq}\,\,{\varepsilon}B;
        \end{displaymath}
    indeed, one can take $B \,=\, \sum_{X} |f|$.

\V

        The simple proof is left as an exercise.

        \V

        \underline{Remark} The reason for the name `Stability Theorem' attached to the preceding theorem should be clear:
    the theorem says that relatively small changes in the nonzero terms of an unordered sum cannot affect the divergence properties of the sum;
    and if the sum is convergent, the effects on the value of the sum of these small changes are also under control.
    Of course, by `relatively small' here is meant `with small relative error'.



                \section{{\bf Application: The Measure of Open Sets and of Compact Sets in ${\RR}$}}
                \label{SectG25}

\V

        The theory of `infinite unordered sums' allows one to give a simple treatment of the important concept of the `size' of a set,
    at least in the case of open and closed subsets of ${\RR}$.
    This concept plays an important role in the theory of the Riemann integral; see Chapter~\Ref{ChaptH}.
    (There is also a much more general theory, associated with the name of the French mathematician Henri Lebesgue,
    which assigns the notion of `measure' to a much wider class of sets than these; but we do not need that theory in {\TheseNotes}.)

\V
\V

        The basic problem under consideration is to formulate a concept of `total length' which applies to subsets of ${\RR}$.
    As has happened before in {\TheseNotes}, we begin by providing some guidelines which any `reasonable' formulation of this concept ought to satisfy:

\V


        \underline{Guideline 1} If $X$ is a subset of ${\RR}$ for which `total length' makes sense,
    then this length must be either a nonnegative number or the quantity~$+{\infty}$.

\V

        \underline{Guideline 2} If $X$ is a set with a single point, then its `total length' should be~$0$.

\V


        \underline{Guideline 3} If $X$ is an interval in ${\RR}$ -- open, closed or half-open -- then the total length of $X$ should be its usual length;
    namely $+{\infty}$ if the interval is unbounded, $|b-a|$ if it has endpoints $a$ and $b$ which are finite.

\V

        \underline{Guideline 4} If $X$ can be expressed as the union of disjoint sets $A$ and $B$ for which the notion of `total length' has been formulated,
   then the total length of $X$ should be the sum of the total lengths of $A$ and $B$.
    (If either $A$ or $B$ has infinite total length, thhis means that $X$ should have infinite total length as well.)

\V
\V

        These guidelines, combined with the concept of `unordered sum', suggest the following definition.
    In this definition, we use the more common terminology of `measure' instead of `total length',
    and for simplicity we restrict our attention to subsets of ${\RR}$ which are either open in ${\RR}$ or compact (i.e., closed and bounded) in ${\RR}$.

\V

        \subsection{\small{{\bf Definition}}}
        \label{DefG25.20}

\V

        (1)\IndB{measure}{of an open set in ${\RR}$} Suppose that $U$ is a nonempty \underline{open} set in ${\RR}$,
    and let ${\cal G}$ be the unique countable family of (nonempty) open intervals whose union equals $U$.
    (The existence and uniqueness of such a family is guaranteed by Theorem~\Ref{ThmB30.180}.

        \h (a) If the family ${\cal G}$ has at least one element which is an {\em un}bounded open interval,
    then define the {\bf measure ${\mu}(U)$ of the open set $U$} to be~$+{\infty}$.

        \h (b) Suppose that each interval $I$ in the family ${\cal G}$ is bounded. Let ${\lambda}:{\cal G} \,{\rightarrow}\, {\RR}^{+}$
    be given by ${\lambda}(I) \,=\, |b-a|$, where $a$ and $b$ are the endpoints of $I$.
    Define the {\bf measure ${\mu}(U)$ of the open set $U$} to be the unordered sum ${\mu}(U) \,=\, \sum_{{\cal G}} {\lambda}$.

        For completeness, define the measure of the empty set by ${\mu}({\emptyset}) \,=\, 0$.

\V

        (2)\IndB{measure}{of a compact set in ${\RR}$} Suppose that $Y$ is a nonempty compact set in ${\RR}$ (i.e., a set which is closed and bounded in ${\RR}$),
    and let $a \,=\, {\inf}\,Y$, $b \,=\, {\sup}\,Y$; note that $a$ and $b$ are real numbers (because $Y$ is nonempty and bounded).
    Let $J \,=\, \{x{\in}{\RR}: a\,\,{\leq}\,\,x\,\,{\leq}\,\,b\}$, and let $U \,=\, J\,{\setminus}\,Y$, so that $U$ is a bounded open set -- possibly empty -- in~${\RR}$.
    Then the {\bf measure of the compact set $Y$} is the quantity ${\mu}(Y) \,=\, |b-a| - {\mu}(U)$, where ${\mu}(U)$ is defined as above.

\V
\V


        \subsection{\small{{\bf Examples}}}
        \label{ExampG25.30}

\V

        (1) Let $Y \,=\, \{x_{1},x_{2},\,{\ldots}\,x_{m}\}$ be a finite set with exactly $m\,\,{\geq}\,\,2$ elements, written so that $x_{1}\,<\,x_{2}\,<\,\,{\ldots}\,\,<\,x_{m}$.
    Then ${\inf}\,Y \,=\, x_{1}$ and ${\sup}\,Y \,=\, x_{m}$. One sees that $U \,=\, [x_{1},x_{m}{\setminus}Y$ is the disjoint union of the open intervals $(x_{1},x_{2})$, $(x_{2},x_{3})$,\,{\ldots}\,$(x_{m-1},x_{m})$., and thus ${\mu}(U)$ equals the finite (collapsing) sum $\sum_{k=2}^{m} (x_{k} - x_{k-1}) \,=\, x_{m} - x_{1}$.
    Thus ${\mu}(Y) \,=\, (x_{m} - x_{1}) - (x_{m} - x_{1}) \,=\, 0$.
    That is, every finite subset of ${\RR}$ has measure~$0$.

\V

        (2) In a similar manner, one can easily show that if $Y$ is the closed bounded set $\{1, 1/2, 1/3, {\lambda}1/k,\,{\ldots}\,\}\,{\cup}\,\{0\}$,
    then ${\mu}(Y) \,=\, 0$.

\V

        (3) The set of rational numbers that lie in the interval $[0,1]$ is certainly a countable set.
    However, it is neither open nor closed, so the definition above does not apply to it.

\V

        (4) Let $C$ be the standard Cantor Ternary Set (see Definition~\Ref{DefA20.100});
    thus $C$ is a compact subset with ${\inf}\,C \,=\, 0$ and ${\sup}\,C \,=\, 1$.
    Let $U \,=\, [0,1]{\setminus}C$, so that $U$ is an open set; indeed, $U$ is the union of the open intervals which are removed from $[0,1]$
    during the `Middle Thirds' construction of~$C$ (see Remark~\Ref{RemrkA20.125}).

    Since the open intervals removed in the `Middle Thirds' process are clearly mutually disjoint,
    it is easy to determine ${\mu}(U)$.
    Indeed, in the first step of the process, one open interval, $(1/3,2/3)$ is removed; call it $U_{11}$.
    In the second step, two open intervals are removed from $[0,1]{\setminus}U_{11}$;
    call then $U_{21}$ and $U_{22}$.
    More generally, in the $k$-th step, $2^{k-1}$ open intervals are removed; call them $U_{k1}$, $U_{k2}$, \,{\ldots}\,$U_{k2^{k-1}}$.
    Finally, let ${\cal G}$ be the family whose elements are the open sets $U_{km}$,
    with $k$ in ${\NN}$ and $1\,\,{\leq}\,\,m\,\,{\leq}\,\,2^{k-1}$.
    Then $U$ is the (disjoint) union of the sets in the family~${\cal G}$, and ${\mu}(U) \,=\, \sum_{{\cal G} {\lambda}}$, where as before ${\lambda}(U_{km})$ is the ordinary length of the interval $U_{km}$.
    To compute this infinite unordered sum, define subsets $S_{1}$, $S_{2}$,\,{\ldots}\,$S_{k}$, \,{\ldots}\, of ${\cal G}$ by
        \begin{displaymath}
        S_{k} \,=\, \left\{U_{k1}, U_{k2},\,{\ldots}\,U_{k2^{k-1}}\right)\} \mbox{ for each $k$ in ${\NN}$},
        \end{displaymath}
    and set ${\cal F} \,=\, \left\{S_{1}, S_{2},\,{\ldots}\,S_{k},\,{\ldots}\,\right)\}$.
    It is clear that ${\cal F}$ is a partition of the set ${\cal G}$. Furthermore, each element $S_{k}$ of the partition ${\cal F}$ is a itself a collection of $2^{k-1}$ mutually disjoint open intervals, each of length $1/3^{k}$.
    Thus the Generalized Associative Law for Infinite Unordered Sums implies that
        \begin{displaymath}
        \sum_{{\cal G}} {\lambda}  \,=\, \sum_{S{\in}{\cal F}} \sum_{S}{\lambda} \,=\, \sum_{k{\in}{\NN}} \sum_{S_{k}} {\lambda} \,=\, 
    \sum_{k{\in}{\NN}} 2^{k-1}{\cdot}\left(\frac{1}{3^{k}}\right) \,=\, \frac{1}{3}\sum_{k{\in}{\NN}} \left(\frac{2}{3}\right)^{k-1}
        \end{displaymath}
    The final unordered sum can be computed using Example~\Ref{ExampG20.35} with $r$ in that example set to $r \,=\, 2/3$.
    One then gets
        \begin{displaymath}
        {\mu}(U) \,=\, \frac{1}{3}\left(\frac{1}{1-(2/3)}\right) \,=\, \left(\frac{1}{3}\right)\left(\frac{1}{1/3}\right) \,=\, 1.
        \end{displaymath}
    It follows that ${\mu}(C) \,=\, 1-{\mu}(U) \,=\, 0$; that is, the Cantor Ternary Set has measure~$0$.

\V

        \underline{Remark} It may appear that the underlying reason the Cantor Ternary Set turns out to have measure~$0$ is that it is a closed set which has no nonempty subsets which are open in ${\RR}$.
    However, there exist closed bounded subsets of ${\RR}$, which also have no open subsets, which have positive measure.
    An example of one such so-called `\IndA{fat Cantor set}'is constructed in the exercises.

\V
\V

        \subsection{\small{{\bf Theorem}}}
        \label{ThmG25.35}

\V

\hspace*{\parindent}(a)Suppose that $U$ is an open set in ${\RR}$ such that $U \,{\subseteq}\, [a,b]$ for some (finite) numbers $a$, $b$ with $a\,<\,b$.
    Then ${\mu}(U)\,\,{\leq}\,\,b-a$.

\V

        (b) Suppose that $U$ and $V$ are open sets in ${\RR}$ such that $U \,{\subseteq}\, V$.
    Then ${\mu}(U)\,\,{\leq}\,\,{\mu}(V)$.

\V

        (c) Suppose that $U_{1}$, $U_{2}$,\,{\ldots}\,$U_{k}$,\,{\ldots}\, is an infinite sequence of open bounded intervals,
    and $V \,{\subseteq}\, U_{1}\,{\cup}\,U_{2}\,{\cup}\,\,{\ldots}\,\,
    {\cup}\,U_{k}\,{\cup}\,\,{\ldots}\,$.
    Let $f:{\NN} \,{\rightarrow}\, {\RR}$ be given by $f(j) \,=\, {\mu}(U_{j})$ for each~$j$.
    Then ${\mu}(V)\,\,{\leq}\,\,\sum_{{\NN}} f$.

\V

        (d) Suppose that $X$ and $Y$ are compact subsets of ${\RR}$ such that $X \,{\subseteq}\, Y$.
    Then ${\mu}(X)\,\,{\leq}\,\,{\mu}(Y)$.


\V

        The simple proof is left as an exercise. \Q

\V
\V

        The extension of the concept of `measure', to apply to a much wider class of subsets of ${\RR}$,
    is surprisingly difficult to carry out, and is beyond the scope of {\TheseNotes}.
    However, there is one partial extension which is easy to carry out and which we shall need in Chapter~\Ref{ChaptH}:9

\V

        \subsection{\small{{\bf Definition}}}
        \label{DefG25.40}

        A subset $X$ of ${\RR}$ is said to be of {\bf measure $0$}, or a {\bf null set in ${\RR}$},
    provided that for every ${\varepsilon}\,>\,0$ there exists an open subset $U$ of ${\RR}$ such that $X \,{\subseteq}\, U$ and ${\mu}(U)\,<\,{\varepsilon}$.

\V
\V

        \subsection{\small{{\bf Examples}}}
        \label{ExampG25.50}

\V

        (1) If $X$ is a countable subset of ${\RR}$, then $X$ is a null set. Indeed, when $X$ is finite the result follows from an earlier example.
    Thus, assume that $X$ is countably infinite, and express $X$ as $\{x_{1}, x_{2},\,{\ldots}\,x_{k}, \,{\ldots}\,\}$,
    with $x_{i} \,\,{\neq}\,\, x_{j}$ if $i \,\,{\neq}\,\, j$.
    For each $j \,=\, 1,2,\,{\ldots}\,$, let ${\varepsilon}_{j} \,=\, {\varepsilon}/2^{j+2}$,
    and let $U_{j}$ be the open interval $(x_{j} - {\varepsilon}_{j}, x_{j} + {\varepsilon}_{j})$.
    Note that $X$ is contained in the union $V$ of the intervals $U_{j}$, $j \,=\, 1,2,\,{\ldots}\,$.
    Also ${\mu}(U_{j}) \,=\, {\varepsilon}/2^{j+1}$, so by Part~(c) of Theorem~\Ref{ThmG25.35} ${\mu}(V)$ and Part~(4) of Example~\Ref{ExampG20.35}
    one has ${\mu}(V)\,\,{\leq}\,\,{\varepsilon}$.
    The claimed result follows.

\V

        (2) If a set $X$ is a null set, then so is every subset of $X$.

\V

        (3) If $Y$ is a compact set such that ${\mu}(Y) \,=\, 0$, then $Y$ is a null set.

                \section{{\bf Ordered Infinite Sums; that is, Infinite Series}}
                \label{SectG30}

\V

        The standard approach to `infinite sums', as found in most freshman-calculus texts, is to combine the concept of `successive addition',
    as described in Definition~\Ref{DefB10.30}, with the idea of `convergent sequence';
    the `unordered sum' approach is probably not even mentioned.
    In practice, however, it is customary to use the word `series' instead of `sum' when following the standard approach.

        NOTE: The use of the word `series' in this context is absolutely standard in mathematics.
    Nevertheless, there are a few down-sides to that usage. For example, in ordinary English the word `series' is often used to mean a `succession' or `ordered list' of objects.
    Thus, in nonmathematical contexts `series' means `sequence'! This, combined with the fact that both `sequence' and `series' begin with the letters `se', causes no end of confusion for calculus students.

        A second problem with the word `series' is that the plural of `series' is `series' (and {\em not} `serieses', as some freshmen believe).
    Thus, one has to pay close attention to know that an author may be referring to more than one series in a given context.

\V

        \subsection{\small{{\bf Definition}}}
        \label{DefG30.20}

        Let ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,x_{k},\,{\ldots}\,)$ be an infinite sequence of real numbers.

\V

        (1) The expression $x_{1}+x_{2}+\,{\ldots}\,$, in which the `dots' indicate that the additions are to go on indefinitely,
    is called the {\bf infinite series associated with ${\xi}$}; the number $x_{k}$ is then called the {\bf $k$-th term} of this infinite series, and ${\xi}$ is called the {\bf sequence of terms associated with the infinite series $\sum_{k=1}^{{\infty}} x_{k}$}.
    One often writes such an expression using the well-known `Sigma' notation; for instance, as $\sum_{j=1}^{{\infty}} x_{j}$.
    On occasion we may write $\sum_{{\xi}}$ as a shorthand for the expression $\sum_{k=1}^{{\infty}} x_{k}$;
    however, one must then be careful to not confuse the expression $\sum_{{\xi}}$ with the unordered sum $\sum_{{\NN}} {\xi}$.

\V

        (2) For each $k$ in ${\NN}$ the {\bf $k$-th partial sum associated with the infinite series $\sum_{j=1}^{{\infty}} x_{j}$} is the number $s_{k}$ given by the finite sum
        \begin{displaymath}
        s_{k} \,=\, x_{1} + x_{2} + \,{\ldots}\, + x_{k}
        \end{displaymath}
    The sequence ${\sigma}_{{\xi}} \,=\, (s_{1},s_{2},\,{\ldots}\,)$ is then called the {\bf sequence of partial sums} associated with the given infinite series $\sum_{{\xi}}$.

    Equivalently, the sequence ${\sigma}_{{\xi}}$ can be described recursively by the rule
        \begin{displaymath}
        s_{1} \,=\, x_{1}, \h s_{k+1} \,=\, s_{k} + x_{k+1} \mbox{ for each $k$ in ${\NN}$}.
        \end{displaymath}
    Note: If the context makes it clear which sequence ${\xi}$ of terms we are using, we may write ${\sigma}$ instead of the more proper ${\sigma}_{{\xi}}$.

\V

        (3) If the sequence ${\sigma}$ of partial sums is convergent, with limit $L$ in ${\RR}$,
    then one says that {\bf series $\sum_{k=1}^{{\infty}} x_{k}$ is a convergent series},
    and that series {\bf converges to $L$}, or that {\bf the value of the series is $L$}.
    One then assigns the value $L$ to the series and writes $\sum_{k=1}^{{\infty}} x_{k} \,=\, L$.

\V

        (4) If the sequence ${\sigma}$ of partial sums is divergent, then one says that {\bf the series $\sum_{k=1}^{{\infty}} x_{k}$ is divergent}.

        \underline{Special Case} Suppose that $\lim_{k \,{\rightarrow}\, {\infty}} s_{k} \,=\, L$,
    where $L$ equals either $+{\infty}$ or $-{\infty}$.
    Then one says that {\bf the series $\sum_{{\xi}}$ diverges to $L$}, and one writes $\sum_{k=1}^{{\infty}} x_{k} \,=\, L$.

\V

        (5) If there exists a quantity $L$, either a real number or one of the infinities,
    such that $\sum_{k=1} x_{k} \,=\, L$ in the sense described in (3) and~(4) above, then one says that the series $\sum_{k=1}^{{\infty}} x_{k}$ {\bf exists} or {\bf is defined}.

\V
\V

        \subsection{\small{{\bf Examples}}}
        \label{ExampG30.30}

\V

\hspace*{\parindent}(1) An infinite series $\sum_{k=1}^{{\infty}} x_{k}$ for which at most finitely many terms are nonzero is certainly convergent.
    Indeed, let $m$ be a natural number such that $x_{k} \,=\, 0$ for all indices $k$ such that $k\,>\,m$.
    Then it is clear that the corresponding sequence of partial sums ${\sigma} \,=\, (s_{1},s_{2},\,{\ldots}\,)$ satisfies $s_{k} \,=\, s_{m}$ for all $k\,\,{\geq}\,\,m$, and thus the sequence ${\sigma}$ converges to the finite sum $s_{m} \,=\, x_{1}+x_{2}+\,{\ldots}\,+x_{m}$.
    It is sometimes convenient to refer to such an infinite series as a {\bf trivial infinite series};
    likewise, one then can refer to a series in which infinitely many of the terms are nonzero as a {\bf nontrivial infinite series}.
    

\V

        (2) An infinite series of the form $A+Ar+Ar^{2}+Ar^{3}+\,{\ldots}\,+Ar^{k}+\,{\ldots}\,$ is called a {\bf geometric series}.
    The quantities $A$ and $r$ are called, respectively, the {\bf initial term} and the {\bf common ratio} of the series.
    (The reason for the name `initial term' should be obvious. As for the `common ratio' terminology,
    suppose that the series is nontrivial -- see (1) above --  so that $A$ and $r$ are nonzero.
    Then each term of the series is nonzero, and the ratio $(Ar^{k+1})/(Ar^{k})$ of any two consecutive terms of the series is the same, namely $r$.)

       It follows from Part~(d) of Proposition~\Ref{ThmB25.80} and Part~(b) of Corollary~\Ref{CorC20.15} that a nontrivial geometric series $\sum_{k=1}^{{\infty}} Ar^{k-1}$ converges to $A/(1-r)$ if $|r|\,<\,1$.
    In contrast, it is an easy exercise to show that such a series is divergent if $|r|\,\,{\geq}\,\,1$.
    (As for a {\em trivial} geometric series, that is, one for which either $A$ or $r$ equals $0$,
    it is clear that the series always converges to the initial term $A$.)

\V

        (3) The {\bf harmonic series} is the infinite sum ${\displaystyle 1+\frac{1}{2}+\frac{1}{3} + \frac{1}{4}+ \,{\ldots}\,}$;
    it is so called because its terms form the harmonic sequence (see Example~\Ref{ExampA40.30}~(4)).
    It is easy to see that this series diverges to $+{\infty}$. Indeed, there are several well-known ways to show this;
    here is an approach which is based on the properties of the natural-logarithm function:

       Let $f(x) \,=\, {\ln}\,(x)$ for all $x\,>\,0$; then $f'(x) \,=\, 1/x$ for all $x\,>\,0$.
    In particular, if $0\,<\,a\,<\,b$ then the maximum value of $f'$ on the interval $[a,b]$ is $1/a$.
    It then follows from the Mean-Value Theorem, that $f(b) - f(a)\,\,{\leq}\,\,(b-a)/a$.
    Now let $j$ be a natural number and set $a \,=\, j$, $b \,=\, j+1$, so that $b-a \,=\, 1$. Then preceding inequality takes the form
        \begin{displaymath}
        {\ln}\,(j+1) - {\ln}\,(j)\,\,{\leq}\,\,\frac{1}{j} \mbox{ for all $j$ in ${\NN}$}.
        \end{displaymath}
    Add up the terms appearing on each side of the preceding inequalities for $j \,=\, 1,2,\,{\ldots}\,k$,
    and note that the sum of the left sides will involve lots of cancellation (and that ${\ln}\,1 \,=\, 0$), to get
        \begin{displaymath}
        {\ln}\,(k+1)\,\,{\leq}\,\,1+\frac{1}{2} + \,{\ldots}\, + \frac{1}{k}.
        \end{displaymath}
    That is, the $k$-th partial sum of the harmonic series is bounded below by ${\ln}\,(k+1)$.
    Since $\lim_{k \,{\rightarrow}\, {\infty}} {\ln}\,(k+1) \,=\, +{\infty}$, it follows that the harmonic series diverges to $+{\infty}$, as claimed.

\V

        (4) In Section~\Ref{SectE60} we obtained an expression for the standard exponential function as the limit of certain polynomial functions:
        \begin{displaymath}
        \exp\,x \,=\, \lim_{k \,{\rightarrow}\, {\infty}} p_{k-1}(x) \mbox{ where } p_{k-1}(x) \,=\, 1+x+\frac{x^{2}}{2}+\frac{x^{3}}{3!}+\frac{x^{k-1}}{(k-1)!}
        \end{displaymath}
    This result has a cleaner expression in terms of an infinite series. Indeed, for any given $x$ in ${\RR}$, consider the infinite sequence ${\rho} \,=\, (r_{1},r_{2},\,{\ldots}\,)$ given by
        \begin{displaymath}
        r_{k} \,=\, x^{k-1}/(k-1)! \mbox{ for each index $k$ in ${\NN}$};
        \end{displaymath}
    note that $r_{1} \,=\, 1$, and $p_{k-1}(x) \,=\, r_{1} + r_{2} + \,{\ldots}\, + r_{k}$ for each $k$ in ${\NN}$.
    Then the results of Section~\Ref{SectE60} imply that the infinite series $\sum_{k=1}^{{\infty}} r_{k}$ converges to $\exp\,x$.
    Thus the preceding `limit' expression for ${\exp}\,x$ can be written as an infinite series:
        \begin{displaymath}
        \exp\,x \,=\, 1+x+\frac{x^{2}}{2} + \frac{x^{3}}{3!} + \,{\ldots}\,+ \frac{x^{k}}{k!} + \,{\ldots}\, \,=\, \sum_{k=0}^{{\infty}} \frac{x^{k}}{k!}.
        \end{displaymath}

\V

        (5) In a like manner, the results of Section~\Ref{SectE60} allow us to express the standard sine and cosine functions as the values of the following infinite series:
        \begin{displaymath}
        {\sin}\,x \,=\, x - \frac{x^{3}}{3!} + \frac{x^{5}}{5!} - \frac{x^{7}}{7!} + \,{\ldots}\,+ (-1)^{k-1}\frac{x^{2k-1}}{(2k-1)!} + \,{\ldots}\,
        \end{displaymath}
    and
        \begin{displaymath}
        {\cos}\,x \,=\, 1 - \frac{x^{2}}{2!} + \frac{x^{4}}{4!} - \frac{x^{6}}{6!} + \,{\ldots}\,+ (-1)^{k-1}\frac{x^{2k}}{(2k)!} + \,{\ldots}\,.
        \end{displaymath}
    Both formulas are valid for all $x$ in ${\RR}$.

\V


        (6) Consider the 'Natural Logarithm' function ${\ln}:{\RR}^{+} \,{\rightarrow}\, {\RR}$.
    Then one has, for all $x\,>\,0$,
        \begin{displaymath}
        {\ln}'(x) \,=\, \frac{1}{x}, \, {\ln}''(x) \,=\, -\frac{1}{x^{2}}, \, {\ln}'''(x) \,=\, \frac{2}{x^{3}},
        \end{displaymath}
    and so on. The general formula is
        \begin{displaymath}
        {\ln}^{(k)}(x) \,=\, (-1)^{k-1}\frac{(k-1)!}{x^{k}} \mbox{ for each $k \,=\, 1, 2, \,{\ldots}\,$}.
        \end{displaymath}
    Now let $h_{k}:{\RR} \,{\rightarrow}\, {\RR}$ denote the $k$-th Taylor polynomial of the function ${\ln}$ about the center point $c \,=\, 1$, so that
        \begin{displaymath}
        h_{0}(x) \,=\, 0, \h h_{k}(x) \,=\, \frac{x-1}{1} - \frac{(x-1)^{2}}{2} + \frac{(x-1)^{3}}{3} - \frac{(x-1)^{4}}{4} + \,{\ldots}\, + (-1)^{k-1}\frac{x^{k}}{k}
        \end{displaymath}
    It follows from Taylor's Formula with Remainder (see Corollary~\Ref{CorE60.80}) that if $x\,>\,1$ then
        \begin{displaymath}
        {\ln}\,x - h_{k}(x) \,=\, (-1)^{k}\frac{(x-1)^{k+1}}{kc^{k+1}} \mbox{ for some $c$ such that $1\,<\,c\,<\,x$}.
        \end{displaymath}
    Since for such $c$ one has $c^{k}\,>\,1$, it follows easily that
        \begin{displaymath}
        |{\ln}\,x-h_{k}(x)|\,\,{\leq}\,\,\frac{|x-1|^{k+1}}{k+1}\,\,{\leq}\,\,\frac{1}{k+1} \mbox{ for all $x$ such that $1\,\,{\leq}\,\,x\,\,{\leq}\,\,2$}
        \end{displaymath}
    In particular, one has
        \begin{displaymath}
        {\ln}\,x \,=\, \lim_{k \,{\rightarrow}\, {\infty}} h_{k}(x) \mbox{ for all $x$ such that $1\,\,{\leq}\,\,x\,\,{\leq}\,\,2$}.
        \end{displaymath}
    Since $h_{k}(x)$ is the $k$-th partial sum for the infinite series
        \begin{displaymath}
        (x-1) - \frac{(x-1)^{2}}{2} + \frac{(x-1)^{3}}{3} - \frac{(x-1)^{4}}{4} + \,{\ldots}\, + (-1)^{k-1}\frac{(x-1)^{k}}{k} + \,{\ldots}\,,
        \end{displaymath}
    we can now write: if $1\,\,{\leq}\,\,x\,\,{\leq}\,\,2$, then
        \begin{equation}
        \label{EqnG.60}
        {\ln}\,x \,=\, (x-1) - \frac{(x-1)^{2}}{2} + \frac{(x-1)^{3}}{3} - \frac{(x-1)^{4}}{4} + \,{\ldots}\, + (-1)^{k-1}\frac{(x-1)^{k}}{k} + \,{\ldots}\,
        \end{equation}
    NOTE: With a little more work one can show that the preceding equation is also correct if $0\,<\,x\,<\,1$, but that it fails to hold if  $x\,>\,2$.

        The formula in Equation~\Ref{EqnG.60} is a slight variation of a formula obtained independently by several mathematicians during the latter part of the $17$-th century;
    the first publication was apparently by the famous cartographer Mercator, so it is often called the {\bf Mercator series}.
    In the special case $x \,=\, 2$ the formula reduces to the following striking result:
        \begin{equation}
        \label{EqnG.70}
        {\ln}\, 2 \,=\, 1 - \frac{1}{2} + \frac{1}{3} - \frac{1}{4} + \,{\ldots}\,+ (-1)^{k-1}\frac{1}{k} + \,{\ldots}\,
        \end{equation}
    The infinite series which appears on the right side of this equation is called the {\bf alternating harmonic series}.
    This is because it can be obtained from the harmonic series (see Example~(3) above)
        \begin{displaymath}
        1+\frac{1}{2}+\frac{1}{3}+\,{\ldots}\,+\frac{1}{k}+\,{\ldots}\,
        \end{displaymath}
    by alternating the signs of the terms.
    We shall see that both the harmonic series and the alternating harmonic series have interesting properties which makes them particularly useful for illustrating some of the features of the theory of infinite series (i.e., ordered infinite sums).

\v
\V

        The main idea behind the standard definition of `convergent infinite series' given above is to introduce the concept of `sequence of partial sums',
    and thereby reduce the discussion to the theory of convergent sequences that has already been treated in Chapter~\Ref{ChaptC}.
    It is worth noting, however, that {\em every} sequence of numbers can be throught of as a `sequence of partial sums'.

\V
\V

        \subsection{\small{{\bf Proposition}}}
        \label{PropG30.40}

        Let ${\sigma} \,=\, (s_{1}, s_{2},\,{\ldots}\,)$ be an infinite sequence of real numbers.
    Then there exists a unique infinite series $\sum_{j=1}^{{\infty}} x_{j}$ for which ${\sigma}$ is its sequence of partial sums.
    More precisely, $x_{1} \,=\, s_{1}$, and if $k\,\,{\geq}\,\,1$ then $x_{k+1} \,=\, s_{k+1}-s_{k}$.

\V

        {\bf Proof} \underline{Uniqueness} Suppose that ${\sigma}$ is the sequence of partial sums for some infinite series $\sum_{j=1}^{{\infty}} x_{j}$.
    Then certainly $s_{1} \,=\, x_{1}$, $s_{2} \,=\, x_{1}+x_{2}$, and so on.  The first of these equations can be read as $x_{1} \,=\, s_{1}$, so the first term of the desired series is determined by the given sequence ${\sigma}$.
    The second equation can then be rewritten in the equivalent form $x_{2} \,=\, s_{2}-x_{1} \,=\, s_{2}-s_{1}$;
    thus the second term of the desired series is also determined by ${\sigma}$.
    More generally, it is easy to show by Mathematical Induction that if $\sum_{j=1}^{{\infty}} x_{j}$ is an infinite series
    whose sequence of partial sums is ${\sigma}$, then $x_{k+1} \,=\, s_{k+1}-s_{k}$ for each $k\,\,{\geq}\,\,1$.
    In other words, if such a series exists, it is unique, and the expression given in the statement of the proposition is correct.

        \underline{Existence} Based on the results just obtained, set $x_{1} \,=\, s_{1}$, $x_{2} \,=\, s_{2}-s_{1}$, $x_{3} \,=\, s_{3}-s_{2}$, and so on;
    the general rule is given resursively by $x_{k+1} \,=\, s_{k+1}-s_{k}$ for each $k\,\,{\geq}\,\,1$.
    It is easy to show by Mathematical Induction that $s_{k} \,=\, x_{1}+x_{2}+\,{\ldots}\,+x_{k}$ for each $k$.

\V
\V

        \subsection{\small{{\bf Remark}}}
        \label{RemarkG30.50}

\V

        In light of the preceding result, it should be the case that every concept associated with infinite sequences ought to have a corresponding concept associated with infinite series.
    Indeed, we have already carried out such a correspondence in Definition~\Ref{DefG30.20}:
    the concepts of `limit of a sequence' and `convergent sequence, when applied to the sequence of partial sums, correspond to the concepts of `value of an infinite series' and `convergent infinite series'.
    The next result provides more examples of such correpondences between `sequence' concepts and `series' concepts.
    In each case, a standard concept for the sequence ${\sigma}$ is translated to the equivalent property for the series $\sum_{k=1}{\infty} x_{k}$;
    the name of this concept used in the theory of infinite series is then written in boldface.
    If the sequence concept under discussion admits slight variations, the slight variations of the corresponding series concepts are usually left to the reader to write down.

\V

        \subsection{\small{{\bf Theorem}}}
        \label{ThmG30.60}

        \underline{General Situation} In the next several examples, ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,)$ is a sequence of terms,
    $\sum_{k=1}^{{\infty}} x_{k}$ is the corresponding infinite series $\sum_{{\xi}} \,=\, x_{1} + x_{2} +\,{\ldots}\,+x_{k}+\,{\ldots}\, $,
    and ${\sigma} \,=\, (s_{1},s_{2},\,{\ldots}\,)$ is the corresponding sequence of partial sums;
    that is, $s_{k} \,=\, x_{1}+x_{2} + \,{\ldots}\,+ x_{k}$ for each index~$k$.

\V

        (1) \underline{Sequence Concept: `The Cauchy Property'}

        A necessary and sufficient condition for the sequence ${\sigma}$ to have the standard Cauchy Property is that the series $\sum_{k=1} x_{k}$ satisfy the following {\bf Cauchy Property for Infinite Series}:

        \h For every ${\varepsilon}\,>\,0$ there exists $B$ such that if $n$ in ${\NN}$ satisfies $n\,\,{\geq}\,\,B$, then $|x_{n+1} + x_{n+2} + \,{\ldots}\, + x_{n+k}|\,<\,{\varepsilon}$.

        \underline{Note} Cauchy's original formulation of what we now call the `Cauchy Property' was the series version, not the one for sequences.

\V

        (2) \underline{Sequence Concept: `Subsequence'}

        Suppose that ${\tau} \,=\, (t_{1},t_{2},\,{\ldots}\,)$ is a subsequence of the sequence~${\sigma}$ of partial sums.
    That is, there exist indices $k_{1}$, $k_{2}$,\,{\ldots}\,, with $1\,\,{\leq}\,\,k_{1}\,<\,k_{2}\,<\,\,{\ldots}\,$ such that $t_{j} \,=\, s_{k_{j}}$ for each $j$ in ${\NN}$.
    Then an infinite series $\sum_{j=1}^{{\infty}} z_{j}$ is the series whose sequence of partial sums is ${\tau}$ if, and only if, one has
        \begin{displaymath}
        z_{1} \,=\, x_{1} + x_{2} + \,{\ldots}\,+ x_{k_{1}}; \h
        z_{j+1} \,=\, x_{k_{j} + 1} + \,{\ldots}\,+x_{k_{j+1}} \mbox{ for each $j{\in}{\NN}$}.
        \end{displaymath}
        \underline{Note} If one replaces each $z_{j}$ in the infinite series $z_{1} + z_{2} + \,{\ldots}\,$ by the value given above, one gets
        \begin{displaymath}
        \sum_{j=1}^{{\infty}} z_{j} \,=\, (x_{1} + x_{2} + \,{\ldots}\,+ x_{k_{1}}) + (x_{k_{1}+1} + x_{k_{1} +2} + \,{\ldots}\,+ x_{k_{2}}) +
        (x_{k_{2} + 1} + \,{\ldots}\,+x_{k_{3}}) + \,{\ldots}\,
        \end{displaymath}
    In the expression on the right, the numbers $x_{k}$ appear in the same order as in the original series $x_{1} + x_{2} + \,{\ldots}\, + x_{k} + \,{\ldots}\,$.
    Indeed, the only difference is the presence of the parentheses in the new series.
    Thus, one says that the series $\sum_{j=1}^{{\infty}} z_{j}$ is {\bf obtained from $\sum_{k=1}^{{\infty}} x_{k}$ by inserting parentheses at the locations $1, k_{1}, k_{2},\,{\ldots}\,$}.
    Likewise, one says that $\sum_{k=1}^{{\infty}} x_{k}$ is {\bf obtained from the series $\sum_{j=1}^{{\infty}} z_{j}$ by removing parentheses}.

\V

        (3) \underline{Sequence Concept: `Monotonicity'}

        A necessary and sufficient condition for the sequence ${\sigma}$ to be monotonic up is that the terms $x_{k}$ are all nonnegative.
    One then says that the series $\sum_{k=1}^{{\infty}} x_{k}$ is a {\bf series of nonnegative terms}.
    (Technically, we need only that $x_{k}\,\,{\geq}\,\,0$ for $k\,\,{\geq}\,\,2$, but it is fairly conventional -- and easy -- to reduce to the case in which all the terms of the series, even the first, are nonnegative.)
    A similar remark relates `monontonic-down sequences' and {\bf series of nonpositive terms},
    and for the the variants `eventually monotonic up' and `eventually monotonic down'.

\V


        The next result uses the correspondences described above to translate known facts about sequences into the corresponding facts about infinite series.

\V

        \subsection{\small{{\bf Theorem}}}
        \label{ThmG30.70}

        Let $\sum_{k=1}^{{\infty}} x_{k}$ be an infinite series of real numbers, and let ${\sigma} \,=\, (s_{1},s_{2},\,{\ldots}\,)$ be the corresponding sequence of partial sums.

\V

        (a) The series $\sum_{k=1}^{{\infty}} x_{k}$ is convergent if, and only if, it has the Cauchy Property.

\V

        (b) If the series $\sum_{k=1}^{{\infty}} x_{k}$ is convergent, then so is any series obtained from $\sum_{k=1}^{{\infty}}$ by inserting parentheses,
    and the values of the series are equal.

\V

        (c) If the terms $x_{k}$ of the series $\sum_{k=1}^{{\infty}} x_{k}$ are all nonnegative,
    then either the series is convergent, or it diverges to $+{\infty}$.
    The former case occurs provided the sequence ${\sigma}$ is bounded above, the latter if ${\sigma}$ is {\em not} bounded above.

\V

        (d) A necessary condition for the series $\sum_{k=1}^{{\infty}} x_{k}$ to be convergent is is that $\lim_{k \,{\rightarrow}\, {\infty}} x_{k} \,=\, 0$;
    however, this condition is not sufficient.

\V

        The proofs of (a), (b) and (c) above follow from Theorem~\Ref{ThmG30.60} above, combined with Theorems~\Ref{ThmC70.20}, \Ref{ThmC20.10A}~(b) and~\Ref{ThmC20.10B}~(b).
    Part~(d) follows from Theorem~\Ref{ThmC70.10}, combined with Example~\Ref{ExampC70.15}.

\V
\V
    
        \subsection{\small{{\bf Examples}}}
        \label{ExampG30.80}
\V

\hspace*{\parindent}(1) Consider the infinite series $1-1+1-1+\,{\ldots}\,$; that is, $\sum_{k=1}^{{\infty}} (-1)^{k+1}$.
    One sees that the corresponding sequence ${\sigma}$ of partial sums is given by
        \begin{displaymath}
        s_{k} \,=\, \left\{
        \begin{array}{rl}
        1 & \mbox{if $k$ is odd} \\
        0 & \mbox{if $k$ is even}
        \end{array}
                                \right.
        \end{displaymath}
    In particular, the sequence ${\sigma}$ does not have a limit, hence the infinite series is not defined.
    In particular, this shows that an infinite series with bounded partial sums need not be convergent.
    (In light of Part~(c) of the preceding theorem, this conclusion would {\em not} be valid if the terms of the series were all nonnegative.)

\V

        (2) Consider the same series as in~(1). Insert parentheses into this series at the locations $1$, $3$, $5$,\,{\ldots}\,$2m-1$,\,{\ldots}\, to obtain
        \begin{displaymath}
        (1-1) + (1-1) + (1-1) +\,{\ldots}\, \,=\, 0+0+\,{\ldots}\,
        \end{displaymath}
    Clearly this new series does converge, and has value $0$.
    This shows that the converse of Part~(b) of the preceding theorem is not true.
    Otherwise stated, inserting parentheses in a convergent series does not change the convergence properties;
    but removing parentheses from a convergent series may yield a nonconvergent series.

\V

        (3) Consider again the same series as in~(1), but now insert parentheses at the locations $2$, $4$, $6$,\,{\ldots}\,$2m$,\,{\ldots}\, to obtain
        \begin{displaymath}
        1+(-1 + 1) + (-1+1) + (-1+1) +\,{\ldots}\, \,=\, 1+ 0+0+\,{\ldots}\,
        \end{displaymath}
    Clearly this new series does converge, and has value $1$.
    This also shows that the converse of Part~(b) of the preceding theorem is not true.
    However, things are even worse than we thought: by comparing with the result of the preceding example,
    we see that it is possible to insert parentheses in two different ways, and obtain different values for the resulting pair of series.

\V

        {\bf Remark} We know from the Generalized Associative Law for Addition that in a finite sum inserting or deleting parentheses has no effect on the final result.
    Examples~(2) and~(3) above tell us that we cannot expect all the usual laws of addition that work for finite ordered sums to necessarily work for convergent infinite ordered sums.
    We shall soon see that the situation is even worse: not even the Generalized Commutative Law extends to convergent infinite ordered sums.
    Fortunately, {\em some} facts that hold for finite sums do extend to convergent infinite ordered sums:

\V
\V

        \subsection{\small{{\bf Theorem}}}
        \label{ThmG30.90}

\V

\hspace*{\parindent}(a) Suppose that $\sum_{j=1}^{{\infty}} x_{j}$ is an infinite series which converges to a number $L$.
    Then for every real number $c$ the series $\sum_{j=1}^{{\infty}} (cx_{j})$ converges to $cL$.
    Symbolically:
        \begin{displaymath}
        \sum_{j=1}^{{\infty}} cx_{j} \,=\, c\left(\sum_{j=1}^{{\infty}} x_{j}\right).
        \end{displaymath}

\V

        (b) Suppose that $\sum_{j=1}^{{\infty}} x_{j}$ and $\sum_{j=1}^{{\infty}} y_{j}$ are infinite series which converge to the numbers $L$ and $M$, respectively.
    Then the series $\sum_{j=1}^{{\infty}} (x_{j}+y_{j})$ converges to $L+M$.

\V

        (c) Suppose that $x_{1} + x_{2} + \,{\ldots}\,+ x_{k} + \,{\ldots}\,$ is a convergent series with value $L$.
    Then both of the following infinite series are convergent and have value $L$:
        \begin{displaymath}
        x_{1} + 0 + x_{2} + 0 + x_{3} +\,{\ldots}\,+ 0 + x_{k} + \,{\ldots}\,
    \mbox{ and }
       0 + x_{1} + 0 + x_{2} + 0 + x_{3} +\,{\ldots}\,+ 0 + x_{k} + \,{\ldots}\,
        \end{displaymath}
    That is, inserting a `zero' term between each term of the original series has no effect on either the convergence or the value of the series.

\V

        (d) Suppose that $\sum_{j=1}^{{\infty}} x_{j}$ and $\sum_{j=1}^{{\infty}} y_{j}$ are infinite series which converge to the numbers $L$ and $M$, respectively.
    Let $\sum_{m=1}^{{\infty}} z_{m}$ be the series $x_{1} + y_{1} + x_{2} + y_{2} + \,{\ldots}\,$,
    which is obtained from the original pair of series by `interlacing' their terms.
    Then $\sum_{m=1}^{{\infty}} z_{k}$ converges to $L+M$.
    
\V

        {\bf Proof} Parts (a), (c) and (d) are left as exercises.


        \underline{Proof of (b)} Let ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,)$ and ${\eta} \,=\, (y_{1},y_{2},\,{\ldots}\,)$ denote the sequences of terms from which these series are formed,
    and let ${\sigma}_{{\xi}}$ and ${\sigma}_{{\eta}}$ denote the corresponding sequences of partial sums of the series;
    thus,
        \begin{displaymath}
        {\sigma}_{{\xi}} \,=\, (x_{1},x_{1}+x_{2},\,{\ldots}\,x_{1}+x_{2}+\,{\ldots}\,+x_{k},\,{\ldots}\,) \mbox{ and }
        {\sigma}_{{\eta}} \,=\, (y_{1},y_{1}+y_{2},\,{\ldots}\,y_{1}+y_{2}+\,{\ldots}\,+y_{k},\,{\ldots}\,) 
        \end{displaymath}
    Now let ${\zeta} \,=\, (z_{1},z_{2},\,{\ldots}\,)$ denote the sequence given by adding corresponding terms of ${\xi}$ and ${\eta}$,
    so that $z_{j} \,=\, x_{j}+y_{j}$ for all $j$.
    Then the $k$-th term of ${\sigma}_{{\zeta}}$ is
        \begin{displaymath}
        z_{1}+z_{2}+\,{\ldots}\,+z_{k} \,=\, (x_{1}+y_{1}) + (x_{2}+y_{2}) + \,{\ldots}\,+ (x_{k}+y_{k}).
        \end{displaymath}
    It follows by applying the Generalized Associative Law for Addition, Theorem~\Ref{ThmB10.60}, to the right side of this last equation,
    that one can write
        \begin{displaymath}
        z_{1}+z_{2}+\,{\ldots}\,+z_{k} \,=\, (x_{1} + x_{2} +\,{\ldots}\,+ x_{k}) + (y_{1} + y_{2} + \,{\ldots}\, + y_{k}).
        \end{displaymath}
    In other words, ${\sigma}_{{\zeta}} \,=\, {\sigma}_{{\xi}} + {\sigma}_{{\eta}}$.
    The  desired result now follows by applying the results of Theorem~\Ref{ThmC60.30}.

\V

        \subsection{\small{{\bf Remarks}}}
        \label{RemrkG30.100}

\V

\hspace*{\parindent}(1) At first glance Part~(d) may appear to be just a restatement of Part~(b).
    However, there is a subtle difference: The series under consideration in Part~(b) involves lots of inserted parentheses,
    while the series in Part~(d) does not. In light of the fact that one cannot freely remove parentheses in ordered infinite sums,
    Part~(d) does not follow directly from Part~(b).

\V

        (2) One can easily extend the statement of Part~(c) to allow the insertion of arbitrarily many zeros between consecutive terms of the original series.
    The details are left as an exercise.

\V
\V

        \subsection{\small{{\bf Important Example}}}
        \label{ExampG30.110}

\V

    If one multiplies both sides of Equation~\Ref{EqnG.70} by $1/2$, one gets
        \begin{displaymath}
        \frac{1}{2}{\ln}\,(2) \,=\, {\displaystyle 
         \begin{array}{rrrrrrrrrr}
        \frac{1}{2} & - & \frac{1}{4} & + & \frac{1}{6} & - & \frac{1}{8} & + & \,{\ldots}\, & ({\ast})
        \end{array}}
        \end{displaymath}
    (To see that this equation does follows from Equation~\Ref{EqnG.70}, use Part~(a) of Theorem~\Ref{ThmG30.90}.)

        Next, insert a zero term before the first term in the series on the right side of Equation~$({\ast})$;
    likewise, insert a zero term between each of the terms of that series. Then one gets
        \begin{displaymath}
        \frac{1}{2}{\ln}\,(2) \,=\,{\displaystyle 
         \begin{array}{rrrrrrrrrrrrrrrrrr}
        0 & + & \frac{1}{2} & + & 0 & - & \frac{1}{4} & + & 0 & + & \frac{1}{6} & + & 0 & - & \frac{1}{8} & + & \,{\ldots}\, & ({\ast}{\ast})
        \end{array}}
        \end{displaymath}
(The justification for Equation~$({\ast}{\ast})$ is Part~(d) of Theorem~\Ref{ThmG30.90}.)

        Now add each term of the preceding series with the corresponding term of the Alternating Harmonic Series (see Equation~\Ref{EqnG.70}),
    and use Part~(b) of Theorem~\Ref{ThmG30.90}, to get $\frac{1}{2}{\ln}\,(2) + {\ln}\,(2) \,=\,$
        \begin{displaymath}
{\displaystyle 
                 \begin{array}{rrrrrrrrrrrrrrrrrr}
        \left(0+1\right) & + & \left(\frac{1}{2} - \frac{1}{2}\right) & + & \left(0+\frac{1}{3}\right) & - & \left(\frac{1}{4} + \frac{1}{4}\right) & + & \left(0+\frac{1}{5}\right) & + & \left(\frac{1}{6}-\frac{1}{6}\right) & + & \left(0+\frac{1}{7}\right) & - & \left(\frac{1}{8} + \frac{1}{8}\right)  & + & \,{\ldots}\, & 
        \end{array}}
        \end{displaymath}
    After simplifying this becomes
        \begin{displaymath}
        \frac{3}{2}{\ln}\,(2) \,=\,{\displaystyle 
         \begin{array}{rrrrrrrrrrrrrrrrrr}
        1 & + & 0 & + & \frac{1}{3} & - & \frac{1}{2}& + & \frac{1}{5} & + & 0 & + & \frac{1}{7} & - & \frac{1}{4} & + & \,{\ldots}\, & 
        \end{array}}
        \end{displaymath}
    Now use Part~(d) of Theorem~\Ref{ThmG30.90} once again to justify deleting all the zero terms above to finally get
        \begin{displaymath}
        \frac{3}{2}{\ln}\,(2) \,=\,{\displaystyle 
         \begin{array}{rrrrrrrrrrrrrr}
        1 & + & \frac{1}{3} & - & \frac{1}{2}& + & \frac{1}{5} & + & \frac{1}{7} & - & \frac{1}{4} & + & \,{\ldots}\, & 
        \end{array}}
        \end{displaymath}
    The final series has exactly the same terms as the Alternating Harmonic Series, including the appropriate signs,
    but written in a different order.
    And it turns out that adding up the same terms but in this new order changes the value of the infinite series:
    we obtain the value $(3/2){\ln}\,(2)$ for this rearranged series, compared to the value ${\ln}\,(2)$ for the original Alternating Harmonic Series.

        \underline{Conclusion} The Generalized Commutative Law for Addition, which certainly holds for ordered finite sums,
    does not extend to ordered infinite sums.

        We investigate this phenomenon further in the next section.

\V
\V



        The next result seems somewhat specialized, but it turns out to be surprisingly useful.

\V

        \subsection{\small{{\bf Theorem}} (Alternating-Series Test)}
        \label{ThmG30.120}

\V

        \underline{Hypotheses} Suppose that ${\alpha} \,=\, (a_{1},a_{2},\,{\ldots}\,)$ is a sequence of real numbers such that

        \h (i)\,\, $a_{k}\,>\,0$ for each $k$ in ${\NN}$;

        \h (ii)\, ${\alpha}$ is monotonic down

        \h (iii) $\lim_{k \,{\rightarrow}\, {\infty}} a_{k} \,=\, 0$.

\V

        \underline{Conclusions}

\V

        (a) The infinite series $a_{1}-a_{2}+a_{3}-a_{4} + \,{\ldots}\,+ (-1)^{k-1}a_{k} + \,{\ldots}\, $ is convegent.
    More precisely, let $s_{k}$ denotes the $k$-th partial sum of this series and $L$ is the value of the series. Then one has
        \begin{equation}
        \label{IneqG.80A}
        0\,\,{\leq}\,\,L\,\,{\leq}\,\,a_{1}, \mbox{ and } 0\,\,{\leq}\,\,(-1)^{k}(L-s_{k})\,\,{\leq}\,\,a_{k+1} \mbox{ for all $k$ in ${\NN}$}
        \end{equation}

\V

        (b) Similarly, the infinite series $-a_{1} + a_{2} - a_{3} + \,{\ldots}\, + (-1)^{k}a_{k} + \,{\ldots}\,$ is convergent.
    Moreover, if $s_{k}$ denotes the $k$-th partial sum of this series and $L$ is the value of the series, then
        \begin{equation}
        \label{IneqG.80B}
       -a_{1}\,\,{\leq}\,\,L\,\,{\leq}\,\,0, \mbox{ and } 0\,\,{\leq}\,\,(-1)^{k-1}(L-s_{k})\,\,{\leq}\,\,a_{k+1} \mbox{ for all $k$ in ${\NN}$}
        \end{equation}

\V

        (c) If the sequence ${\alpha}$ is actually {\em strictly} decreasing, then all the inequalities in~\Ref{IneqG.80A} and~~\Ref{IneqG.80B} can be replaced by {\em strict} inequalities.

\V

        {\bf Proof}

        (a) Note that $s_{2m+1} \,=\, s_{2m}+a_{2m+1}\,\,{\geq}\,\,s_{2m}$ and $s_{2m} \,=\, s_{2m-1}-a_{2m}\,\,{\leq}\,\,s_{2m-1}$ for each $m$ in ${\NN}$, by Condition~(i).
    Likewise, $s_{2m+2} \,=\, s_{2m} + a_{2m+1} - a_{2m+2}\,\,{\geq}\,\,s_{2m}$ and $s_{2m+1} \,=\, s_{2m-1} - a_{2m} + a_{2m+1}\,\,{\leq}\,\,s_{2m-1}$ for each $m$ in ${\NN}$, by Condition~(ii).
    Combining these results, one sees that the segments $\mbox{Seg}\,[s_{k+1},s_{k}]$, $k \,=\, 1,2,\,{\ldots}\,$, forms a nested sequence of segments.
    Finally, Condition~(iii) implies that $\lim_{k \,{\rightarrow}\, {\infty}} |s_{k+1}-s_{k}| \,=\, 0$.
    One can now apply the Nested-Segments Theorem (Part~(b) Theorem~\Ref{ThmC20.40}) to conclude that the intersection of these segments is a singleton set $\{L\}$ and that the sequence ${\sigma}$ converges to $L$.
    It is also easy to see from the above that $0\,\,{\leq}\,\,a_{1}-a_{2} \,=\, s_{2}\,\,{\leq}\,\,s_{1} \,=\, a_{1}$.
    Since $L{\in}\mbox{Seg}\,[s_{2},s_{1}]$, it follows that $0\,\,{\leq}\,\,L\,\,{\leq}\,\,a_{1}$, as required.
    Also, since $s_{2m}\,\,{\leq}\,\,s_{2m+1}$ and $L{\in}\mbox{Seg}\,[s_{2m},s_{2m+1}]$ it follows that $0\,\,{\leq}\,\,s_{k}\,\,{\leq}\,\,L\,\,{\leq}\,\,s_{k+1}$
    if $k$ is even, so $0\,\,{\leq}\,\,L-s_{k}\,\,{\leq}\,\,s_{k+1}-s_{k} \,=\, a_{k+1}$ if $k$ is even.
    Likewise, if $k$ is odd then $s_{k+1}\,\,{\leq}\,\,L\,\,{\leq}\,\,s_{k+2}\,\,{\leq}\,\,s_{k}$, where the final inequality uses the fact that the segments are nested.
    It follows that if $k$ is odd then $-a_{k+1}\,\,{\leq}\,\,s_{k+1}-s_{k}\,\,{\leq}\,\,L-s_{k}\,\,{\leq}\,\,0$. Since $(-1)^{k} \,=\, -1$ when $k$ is odd, this last can be written $0\,\,{\leq}\,\,(-1)^{k}(L-s_{k})\,\,{\leq}\,\,a_{k+1}$ when $k$ is odd. Inequalities~\Ref{IneqG.80A} now follow.

\V

        The simple proofs of Parts~(b) and (c) are left as exercises.



\V
\V

        Among the most important examples of infinite series are the so-called `Power Series'.


        \subsection{\small{{\bf Definition}} (Power Series)}
        \label{DefG30.130}

\V

        (1) An expression of the form $a_{0} + a_{1}u + a_{2}u^{2} + \,{\ldots}\, a_{j}u^{j} + \,{\ldots}\,$, or, briefly, $\sum_{j=0}^{{\infty}} a_{j}u^{j}$,
    is called a {\bf power series in the quantity $u$ with corresponding coefficients $a_{0}$, $a_{1}$,\,{\ldots}\,}.

\V

        (2) If one differentiates each term of a power series $a_{0} + a_{1}u + a_{2}u^{2} + \,{\ldots}\, + a_{j}u^{j} +\,{\ldots}\,$ `with respect to $u$',
    in the sense of elementary calculus, one obtains a second power series $b_{0} + b_{1}u + b_{2}u^{2} + \,{\ldots}\, + b_{k}u^{k} + \,{\ldots}\,$,
    where
        \begin{displaymath}
        b_{0} \,=\, a_{1}, \, b_{1} \,=\, 2a_{2}, \, \,{\ldots}\,b_{k} \,=\, (k+1)a_{k+1}, \,{\ldots}\,
        \end{displaymath}
    One says that the power series $\sum_{k=0}^{{\infty}} b_{k}u^{k}$ is obtained from the original series,
    $\sum_{j=0}^{{\infty}} a_{j}u^{j}$, using {\bf term-by-term differentiation}.

\V

        (3) Similarly, let $\sum_{m=0}^{{\infty}} c_{m}u^{m}$ be a power series whose coefficients are of the form
        \begin{displaymath}
        c_{0} \,=\, C \mbox{ for some constant $C$};\, 
        c_{m} \,=\, \frac{a_{m-1}}{m} \mbox{ for each $m \,=\, 1,2,3,\,{\ldots}\,$}.
        \end{displaymath}
    That is,
        \begin{displaymath}
        \sum_{m=0}^{{\infty}} c_{m}u^{m} \,=\, C + a_{0}u
 + \frac{a_{1}}{2}u^{2} + \,{\ldots}\, + \frac{a_{m-1}}{m}u^{m} + \,{\ldots}\,
        \end{displaymath}
    One says that the power series $\sum_{m=0}^{{\infty}} c_{m}u^{m}$ is obtained from the original series, $\sum_{j=0}^{{\infty}} a_{j}u^{j}$, using {\bf term-by-term antidifferentiation}.
    (Or, if one uses older terminology, one refers to this process as {\bf term-by-term integration}.)
    

        \subsection{\small{{\bf Remarks}}}
        \label{RemrkG30.140}

\V

\hspace*{\parindent}(1) The use of the word `power' in the name `power series' is clear:
    the dominant feature of such a series is the presence of the successive powers of the quantity $u$.

\V

        (2) We are already familiar with examples of power series; namely, the Taylor series
    ${\displaystyle \sum_{j=0}^{{\infty}} \frac{f^{(j)}(c)}{j!}(x-c)^{j}}$ about a center point $c$ of a $C^{{\infty}}$ function $f$.
    Note that this is a power series in the quantity $u \,=\, (x-c)$.
    It is easy to show that the Taylor series about $c$ of $f'$ is obtained from the corresponding series for $f$ by using term-by-term differentiation.
    Likewise, the Taylor series about $c$ for an antiderivative of $f$ is obtained using term-by-term antidifferentiation on the series for~$f$.

\V

        (3) One should normally think of the role played by the quantity $u$ in the power series $\sum_{j=0}^{{\infty}} a_{j}u^{j}$ as follows:
    for each choice of $u$ in ${\RR}$ one gets a particular infnite series.
    For instance, consider the power series
        \begin{displaymath}
        1+ u + u^{2} + u^{3} + \,{\ldots}\, + u^{k-1} +
        \end{displaymath}
    If we set $u \,=\, 1/2$ we get the convergent geometric series
        \begin{displaymath}
        1 + \frac{1}{2} + \frac{1}{2^{2}} + \,{\ldots}\, + \frac{1}{2^{k-1}} + \,{\ldots}\,
        \end{displaymath}
    whose value is~$2$.
    If, in contrast, we set $u \,=\, 2$, we get the divergent geometric series
        \begin{displaymath}
        1 + 2 + 2^{2} + \,{\ldots}\, + 2^{k-1} + \,{\ldots}\,
        \end{displaymath}
    The preceding examples suggest a couple if initial questions one might ask about a power series $\sum_{j=0}^{{\infty}} a_{j}u^{j}$:

        \h (i)\, For which values of $u$ does the series converge, and for which does it diverge?

        \h (ii) What is the nature of a function that is defined by the values of a power series?

\noindent The principle convergence/divergence properties of power series are established in Section~\Ref{SectG50} below.

\V

        (4) In principle, {\em every} infinite series can be viewed as arising from a power series.
    For example, consider a series $x_{1} + x_{2} + \,{\ldots}\, + x_{k} + \,{\ldots}\,$.
    If we set $a_{j} \,=\, x_{j+1}$ for each $j \,=\, 0,1,2,\,{\ldots}\,$, then the given series can be viewed as the result of setting $u \,=\, 1$ in the power series $\sum_{j=0}^{{\infty}} a_{j}u^{j}$.
    Sometimes this viewpoint turns out to be useful; sometimes, not.

\V

        (5) The particular labeling of the coefficients used here, with the initial index being~$0$ instead of the usual~$1$,
    is conventional: one wishes to allow the zero-th power of $u$ to appear in such sums,
    and it then becomes convenient to have the label assigned to a coefficient match the exponent of the corresponding power of~$u$.
    However, this index convention is frequently violated. In any event, note that with this convention the $k$-th term of the series $\sum_{j=0}^{{\infty}} a_{j}u^{j}$ is $a_{j-1}u^{j-1}$, not $a_{j}u^{j}$.

\V

        (6) In some contexts one is forced to consider similar infinite series which involve negative powers of the quantity~$u$,
    or perhaps even fractional powers of~$u$. Normally it is easy to modify the preceding definition, and any associated facts, to take care of these new situations.

\V
\V






                \section{{\bf Absolute Convergence, Conditional Convergence}}
                \label{SectG40}

\V

        If ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,)$ is an infinite sequence of numbers,
    then ${\xi}$ is a real-valued function whose domain is the set $X \,=\, {\NN}$.
    Thus the results of the preceding sections tell us that there are (at least) two ways of defining the concept of the `sum of the values of the function ${\xi}$':

        \h (i)\, the unordered sum $\sum_{{\NN}} {\xi}$, as defined in Section~\Ref{SectG20};

        \h (ii) the infinite series $\sum_{k=1}^{{\infty}} x_{k}$, as defined in Section~\Ref{SectG30}.

\noindent Theorem~\Ref{ThmG20.120} shows that the concept of (ordered) infinite series is at least as general as that of unordered sum (over ${\NN}$),
    and that these two concepts agree when both apply.
    However, Theorem~\Ref{ThmG20.70} shows that that the theory of unordered sums retains one of the main properties of finite sums,
    namely the Generalized Associative Law holds for unordered infinite sums.
    In contrast, Example~\Ref{ExampG30.80} shows that the theory of (ordered) infnite series does {\em not} have a general Associative Law;
    likewise, Example~\Ref{ExampG30.110} shows that the Generalized Commutative Law, valid for finite sums, does not hold for (ordered) infinite series.

        In this section we explore in a little more depth the connections between the two types of infinite sums.

\V
\V

        \subsection{\small{{\bf Theorem}}}
        \label{ThmG40.20}

        Suppose that ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,)$ is an infinite sequence of real numbers;
    that is, ${\xi}$ is a real-valued function defined on ${\NN}$.


\V

        (a) The unordered sum $\sum_{{\NN}} {\xi}^{+}$ diverges to $+{\infty}$ if, and only if,
    the corresponding (ordered) infinite series $\sum_{k=1}^{{\infty}} x_{k}^{+}$ diverges to $+{\infty}$.

        Likewise, the unordered sum $\sum_{{\NN}} {\xi}^{-}$ diverges to $+{\infty}$ if, and only if the infinite series $\sum_{k=1}^{{\infty}} x_{k}^{-}$ diverges to $+{\infty}$.

\V

        (b) The following statements are equivalent:

        \h (i)\,\, The unordered sum $\sum_{{\NN}} {\xi}$ is convergent.

        \h (ii)\, Each of the (ordered) infinite series $\sum_{k=1}^{+} x_{k}^{+}$ and $\sum_{k=1}^{{\infty}} x_{k}^{-}$ is convergent.

        \h (iii) The (ordered) infinite series $\sum_{k=1}^{{\infty}} |x_{k}|$ is convergent.                                      

\V

        (c) The unordered $\sum_{{\NN}} {\xi}$ has value $+{\infty}$ if, and only if,
    the infinite series $\sum_{k=1}^{{\infty}} x_{k}^{+}$ diverges to $+{\infty}$,
    while the infinite series $\sum_{k=1}^{{\infty}} x_{k}^{-}$ is convergent.

        Likewise, the unordered $\sum_{{\NN}} {\xi}$ has value $-{\infty}$ if, and only if,
    the infinite series $\sum_{k=1}^{{\infty}} x_{k}^{-}$ diverges to $+{\infty}$,
    while the infinite series $\sum_{k=1}^{{\infty}} x_{k}^{+}$ is convergent.

\V

        (d) The unordered sum $\sum_{X} {\xi}$ is not defined if, and only if, each of the (ordered) series $\sum_{k=1}^{{\infty}} x_{k}^{+}$
    and $\sum_{k=1}^{{\infty}} x_{k}^{-}$ diverges to $+{\infty}$.

\V

        {\bf Proof}

\V

        (a) The `only if' part follows from Theorem~\Ref{ThmG20.120}. As for the converse, suppose that $\sum_{k=1}^{{\infty}} x_{k}^{+} \,=\, +{\infty}$.
    Let $M\,>\,0$ be given. Then there exists $N$ such that if $n$ in ${\NN}$ satisfies $n\,\,{\geq}\,\,N$ then $\sum_{k=1}^{n} x_{k}^{+}\,\,{\geq}\,\,M$.
    In particular, $\sum_{W} {\xi}^{+}\,\,{\geq}\,\,M$ for the finite subset $W \,=\, {\NN}_{n}$ of ${\NN}$.
    It follows easily that $\sum_{{\NN}} {\xi}^{+} \,=\, +{\infty}$.

        To prove the analogous result for ${\xi}^{-}$, apply what was just proved to the sequence ${\tau} \,=\, -{\xi}$.

\V

        (b) The equivalence of Statements~(i) and~(ii) follows directly from Part~(a) and the definition of `convergence' for unordered sums.
    To see that Statements~(ii) and~(iii) are equivalent, recall that for each index $k$ one has
        \begin{displaymath}
        x_{k}^{+}\,\,{\leq}\,\,x_{k}^{+} + x_{k}^{-} \,=\, |x_{k}| \mbox{ and }
        x_{k}^{-}\,\,{\leq}\,\,x_{k}^{+} + x_{k}^{-}\,\,{\leq}\,\,|x_{k}|
        \end{displaymath}
    Thus if $\sum_{k=1}^{{\infty}} |x_{k}|$ is convergent then it is clear that $\sum_{k=1}^{{\infty}} x_{k}^{+}\,\,{\leq}\,\,\sum_{k=1}^{{\infty}} |x_{k}|\,<\,+{\infty}$;
    likewise, $\sum_{k=1}^{{\infty}} x_{k}^{-}\,\,{\leq}\,\,\sum_{k=1}^{{\infty}}\,<\,+{\infty}$.
    In particular, both of the series $\sum_{k=1}^{{\infty}} x_{k}^{+}$ and $\sum_{k=1}^{{\infty}} x_{k}^{-}$ converge.
    Conversely, if each of the series $\sum_{k=1}^{{\infty}} x_{k}^{+}$ and $\sum_{k=1}^{{\infty}} x_{k}^{-}$ is convergent, then by Theorem~\Ref{ThmG30.90} one knows that $\sum_{k=1}^{{\infty}} (x_{k}^{+} + x_{k}^{-})$ converges;
    that is, $\sum_{k=1}^{{\infty}} |x_{k}|$ is convergent.

\V

        The simple proofs of Parts~(c) and~(d) are left as an exercise.

\V

        \subsection{\small{{\bf Corollary}}}
        \label{CorG40.30}

\V

        Let ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,):{\NN} \,{\rightarrow}\, {\RR}$ is a sequence of numbers,
    and suppose that the infinite series $\sum_{k=1}^{{\infty}} |x_{k}|$ is convergent.
    Then:

\V

        (a) The series $\sum_{k=1}^{{\infty}} x_{k}$ is convergent, and
        \begin{displaymath}
        \sum_{k=1}^{{\infty}} x_{k} \,=\, \left(\sum_{k=1}^{{\infty}} x_{k}^{+}\right) - \left(\sum_{k=1}^{{\infty}} x_{k}^{-}\right)
        \end{displaymath}
    (By Part~(b) of the preceding theorem, both series on the right side of the equation are convergent.)

\V

    (b) If ${\varphi}:{\NN} \,{\rightarrow}\, {\NN}$ is a bijection of ${\NN}$ onto ${\NN}$,
    and if ${\zeta} \,=\, (z_{1},z_{2},\,{\ldots}\,) \,=\, {\xi}{\circ}{\varphi}$,
    so $z_{j} \,=\, x_{{\varphi}(j)}$ for each $j$ in ${\NN}$,j=1 then ${\displaystyle \sum_{j=1}^{{\infty}} z_{j} \,=\, \sum_{k=1}^{{\infty}} x_{k}}$.

\V

        {\bf Proof}

\V

        (a) By Part~(b) of the preceding theorem one knows that the unordered sum $\sum_{{\NN}} {\xi}$ is convergent.
    Then Theorem~\Ref{ThmG20.120}, with the bijection ${\varphi}$ in that result given by the fomula ${\gamma}(j) \,=\, j$ for all $j$ in ${\NN}$,
    implies that the (ordered) infinite series $\sum_{k=1}^{{\infty}} x_{k}$ is convergent and has the same value as $\sum_{{\NN}} {\xi}$.
    This latter value equals $\sum_{{\NN}} {\xi}^{+} - \sum_{{\NN}} {\xi}^{-}$,
which in light of Part~(a) of the preceding theorem, combined with Theorem~\Ref{ThmG20.120} again, implies the desired formula.

\V

        (b) This follows immediately by using Theorem~\Ref{ThmG20.120}.

\V

        {\bf Remark} The properties of the Alternating Harmonic Series discussed in the preceding section show that the converse of this corollary is not true.     
    That is, if an infinite series $\sum_{k=1}^{{\infty}} x_{k}$ is convergent, the corresponding series $\sum_{k=1}^{{\infty}} |x_{k}|$ of absolute values need not be convergent.
    Likewise, rearrangements of the terms of an infinite series can affect the value of the series.

\V
\V

        The preceding results lead to the following definition.

\V
\V

        \subsection{\small{{\bf Definition}}}
        \label{DefG40.40}

\V

        Let $\sum_{k=1}^{{\infty}} x_{k}$ be a \underline{convergent} infinite series.
    This series is said to be {\bf absolutely convergent} provided the series $\sum_{k=1}^{{\infty}} |x_{k}|$ also is convergent.
    In contrast, the convergent series $\sum_{k=1}^{{\infty}} x_{k}$ is said to be {\bf conditionally convergent} provided the series $\sum_{k=1}^{{\infty}} |x_{k}|$ is {\em not} convergent.

\V
\V

        It is convenient to repeat some of the earlier results using the `absolute' and `conditional' terminology.

\V

        \subsection{\small{{\bf Theorem}}}
        \label{ThmG40.50}

\V

        (a) A convergent infinite series $\sum_{k=1}^{{\infty}} x_{k}$ is \underline{absolutely} convergent if, and only if,
    each of the series $\sum_{k=1}^{{\infty}} x^{+}$ and $\sum_{k=1}^{{\infty}} x^{-}_{k}$ is convergent.
    In that case one has
        \begin{displaymath}
        \sum_{k=1}^{{\infty}} x_{k} \,=\, \left(\sum_{k=1}^{{\infty}} x^{+}_{k}\right) - \left(\sum_{k=1}^{k} x_{k}^{-}\right) \mbox{ and }
        \sum_{k=1}^{{\infty}} |x_{k}| \,=\, \left(\sum_{k=1}^{{\infty}} x^{+}_{k}\right) + \left(\sum_{k=1}^{k} x_{k}^{-}\right)
        \end{displaymath}

\V

        (b) Suppose that $\sum_{k=1}^{{\infty}} x_{k}$ is absolutely convergent.
    If $(c_{1},c_{2},\,{\ldots}\,)$ is a bounded sequence of real numbers, then the series $\sum_{k=1}^{{\infty}} c_{k}x_{k}$ is also absolutely convergent.

        Likewise, if $\sum_{k=1}^{{\infty}} x_{k}$ and $\sum_{k=1}^{{\infty}} y_{k}$ are absolutely convergent,
    then so is are the series $\sum_{k=1}^{{\infty}} (x_{k} + y_{k})$ and $\sum_{k=1}^{{\infty}} (x_{k} {\cdot} y_{k})$.

\V

        (c) (`Rearrangement Property') Suppose that $\sum_{k=1}^{{\infty}} x_{k}$ is an absolutely convergent series,
    and that $\sum_{j=1}^{{\infty}} z_{j}$ is a rearrangement of the series $\sum_{k=1}^{{\infty}} x_{k}$;
    that is, there is a bijection ${\varphi}:{\NN} \,{\rightarrow}\, {\NN}$ of ${\NN}$ onto ${\NN}$ such that $z_{j} \,=\, x_{{\varphi}(j)}$ for each $j$ in ${\NN}$.
    Then $\sum_{j=1}^{{\infty}} z_{j}$ is also absolutely convergent, and one has $\sum_{j=1}^{{\infty}} z_{j} \,=\, \sum_{k=1}^{{\infty}} x_{k}$.

\V

        (d) (`Stability Property') Suppose that $\sum_{k=1}^{{\infty}} x_{k}$ is absolutely convergent.
    Then there is a constant $B$ such that if ${\varepsilon}$ satisfies $0\,<\,{\varepsilon}\,<\,1$,
    and if ${\tau} \,=\, (t_{1},t_{2},\,{\ldots}\,)$ is a sequence of numbers such that $1-{\varepsilon}\,<\,t_{k}\,<\,1+{\varepsilon}$ for each $k$,
    then
        \begin{displaymath}
        \left|\sum_{k=1}^{{\infty}} x_{k} - \sum_{k=1}^{{\infty}} t_{k}x_{k}\right|\,\,{\leq}\,\,{\varepsilon}B.
        \end{displaymath}

\V

        The proof consists essentially in using Theorem~\Ref{ThmG40.20} to relate the statements above to known properties of unordered sums,
    and is left as an exercise.

\V
\V

       In contrast, some of the properties of {\em conditionally} convergent series are quite different.

\V
\V


        \subsection{\small{{\bf Theorem}}}
        \label{ThmG40.60}

\V


        (a) A convergent infinite series $\sum_{k=1}^{{\infty}} x_{k}$ is \underline{conditionally} convergent if, and only if,
    each of the series $\sum_{k=1}^{{\infty}} x^{+}$ and $\sum_{k=1}^{{\infty}} x^{-}_{k}$ is divergent to $+{\infty}$.
    In particular, one \underline{cannot} write $\sum_{k=1}^{{\infty}} x_{k} \,=\, \left(\sum_{k=1}^{{\infty}} x_{k}^{+}\right) - \left(\sum_{k=1}^{{\infty}} x_{k}^{-}\right)$, since the expression ${\infty}-{\infty}$ is not defined.


\V

        (b) Suppose that $\sum_{k=1}^{{\infty}} x_{k}$ is conditionally convergent.
    Then there exists a bounded sequence $(c_{1},c_{2},\,{\ldots}\,)$ of real numbers such that the series $\sum_{k=1}^{{\infty}} c_{k}x_{k}$ is not convergent.

        Likewise, there exist conditionally convergent series $\sum_{k=1}^{{\infty}} x_{k}$ and $\sum_{k=1}^{{\infty}} y_{k}$ such that
    the series $\sum_{k=1}^{{\infty}} (x_{k} + y_{k})$ and $\sum_{k=1}^{{\infty}} (x_{k} {\cdot} y_{k})$ are not conditionally convergent.

\V

        (c) (`Riemann's Rearrangement Theorem') Suppose that $\sum_{k=1}^{{\infty}} x_{k}$ is a conditionally convergent series.
    Then:

        \h (i)\, For each quantity $L$, with $-{\infty}\,\,{\leq}\,\,L\,\,{\leq}\,\,+{\infty}$, there exists a rearrangement of the given series whose value is~$L$.
    That is, there is a bijection ${\varphi}:{\NN} \,{\rightarrow}\, {\NN}$ such that if one sets $z_{k} \,=\, x_{{\varphi}(k)}$ for each index~$k$, then $\sum_{k=1}^{{\infty}} z_{k} \,=\, L$.

        \h (ii) There exist rearrangements of the original series $\sum_{k=1}^{{\infty}}$ which do not have a value, finite or infinite.



\V

        (d) (`Instability Property') Suppose that $\sum_{k=1}^{{\infty}} x_{k}$ is conditionally convergent.
    Then there is \underline{no} constant $B$ such that if ${\varepsilon}$ satisfies $0\,<\,{\varepsilon}\,<\,1$,
    and ${\tau} \,=\, (t_{1},t_{2},\,{\ldots}\,)$ is a sequence of numbers such that $1-{\varepsilon}\,<\,t_{k}\,<\,1+{\varepsilon}$ for each $k$,
    then
        \begin{displaymath}
        \left|\sum_{k=1}^{{\infty}} x_{k} - \sum_{k=1}^{{\infty}} t_{k}x_{k}\right|\,\,{\leq}\,\,{\varepsilon}B.
        \end{displaymath}

\V

        The simple proofs are left as an exercise.

\V
\V

        \subsection{\small{{\bf Remarks}}}
        \label{RemrkG40.70}

\V

        Students in calculus courses are often mystified by the `absolute convergence' and `conditional convergence' terminology:

\V

        (1) Most texts define `absolute convergence' thusly:
\V

        \h `A series $\sum_{k=1}^{{\infty}} x_{k}$ is said to be {\bf absolutely convergent} provided the related series $\sum_{k=1}^{{\infty}} |a_{k}|$ is convergent.'

\V

\noindent The text then proves what, to the student, sounds like a vacuous result:

        \h `If a series converges absolutely, then it converges'.

\noindent This statement sounds vacuous to students because in ordinary language similar statements {\em would} be vacuous:

        \h `If a woman is walking quickly, then she is walking'. `If a man is very tall then he is tall.'

\V

        (2) In contrast, the definition of a series $\sum_{k=1}^{{\infty}} x_{k}$ being {\em conditionally} convergent assumes, from the start,
    that the series is convergent, and that the `conditional' describes the nature of that convergence.
    We do \underline{not} prove, for example, the theorem that `A conditionally convergent series is convergent'.

\V

        (3) However, the phrase `conditionally convergent' has its own problems for students.
    In fact, it makes it sound like there is doubt that the series is `truly' convergent.
    It gets worse when we explain to the students that the `conditional' refers to the fact that we might lose convergence if we rearranged the terms --
    as if the students had any intention of rearranging the terms at all!

\V

        Of course the underlying issue is that these words are being used in a technical sense, peculiar to mathematics;
    thus it is irrelevant that the `ordinary language' sense of these words is different.
    Nevertheless, it might help teachers of mathematics if the creaters of mathematical jargon thought more about their choice of words.

        For example, the `Stability' and `Instability' properties described in Theorems~\Ref{ThmG40.50} and~\Ref{ThmG40.60} might suggest using the phrases 
    `{\em stably} convergent' and `{\em unstably} convergent' instead of the `absolute' and `conditional' terminology.


\V
\V

        It might appear reasonable to divide the {\em divergent} infinite series into a pair of categories:
    `absolutely divergent' (i.e., no rearrangement is convergent) and `conditionally divergent' (i.e., the series diverges but some rearrangement converges.)
    However, that terminology is not of use here.
    Instead, we {\em do} find the following terminology to be of some help:

\V

        \subsection{\small{{\bf Definition}}}
        \label{DefG40.80}

\V

        A divergent infinite series $\sum_{k=1}^{{\infty}} x_{k}$ is said to {\bf diverge badly} if it is not the case that $\lim_{k \,{\rightarrow}\, {\infty}} x_{k} \,=\, 0$.
    Such a series is said to {\bf diverge very badly} if, in fact, the set $\{x_{1},x_{2},\,{\ldots}\,\}$ is not even bounded.

\V

        {\bf Remarks}

\V

        (1) Note that if a series diverges badly or very badly, then there is no way to `make it converge' by simply rearranging its terms or changing the signs of some of its terms.
    In particular, if $\sum_{k=1}^{{\infty}} |x_{k}|$ diverges badly, then so does the series $\sum_{k=1}^{{\infty}} x_{k}$.

\V

        (2) The technical meaning of the `diverges badly' and `diverges very badly' terminology introduced here is \underline{not} standard in analysis.

\V
\V

        We conclude this section with a pair of test for convergence that are particularly elegant.

\V

        \subsection{\small{{\bf Theorem}} (Dirichlet's Test)}
        \label{ThmG40.90}

\V

        Suppose that $\sum_{k=1}^{{\infty}} x_{k}$ is an infinite series whose partial sums,
    ${\sigma}_{1} \,=\, x_{1}$, ${\sigma}_{2} \,=\, x_{1} + x_{2}$,\,{\ldots}\,, form a {\em bounded} sequence of numbers;
    note that it is {\em not} assumed that this series is convergent.
    Let ${\alpha} \,=\, (a_{1},a_{2},\,{\ldots}\,)$ be a monotonic sequence  which converges to~$0$.
    Then the series $\sum_{k=1}^{{\infty}} a_{k}x_{k}$ is convergent.

\V

        {\bf Proof} Without loss of generality one may assume that the sequence ${\alpha}$ is monotonic {\em down}.
    (If ${\alpha}$ is monotonic up, replace ${\alpha}$ by ${\alpha}' \,=\, -{\alpha}$, and note that ${\alpha}'$ also converges to~$0$.)
    In particular, one must then have $a_{k}\,\,{\geq}\,\,0$ for each index~$k$.
    Consider now the infinite sum
        \begin{displaymath}
        a_{1} - a_{2} + a_{2} - a_{3} + a_{3} - a_{4} + \,{\ldots}\, + a_{k} - a_{k+1} + a_{k+1} \,{\ldots}\, \h ({\ast})
        \end{displaymath}
    It is clear that the sequence of partial sums for the series~
$({\ast})$ is
        \begin{displaymath}
        a_{1}, \, a_{1}-a_{2}, \, a_{1}, \, a_{1} - a_{3}, \, a_{1}, \, a_{1} - a_{4}, a_{1},\,{\ldots}\,a_{1}, \, a_{1} - a_{k},\,{\ldots}\,
        \end{displaymath}
    Since, by hypthesis, one has $\lim_{m \,{\rightarrow}\, {\infty}} a_{m} \,=\, 0$,
    it follows from Theorem~\Ref{ThmC20.90}, the `Odd/Even Convergence Theorem', that the series~$({\ast})$ is convergent.
    Indeed, the monotonicity hypothesis on ${\alpha}$ implies that this series is {\em absolutely} convergent.
    Now apply Part~(b) of Theorem~\Ref{ThmG40.50} to conclude that the series $\sum_{k=1}^{{\infty}} {\sigma}_{k}(a_{k} - a_{k+1})$ is absolutely convergent.
    Consider the partial sums ${\sigma}'_{1}$, ${\sigma}'_{2}$,\,{\ldots}\, of this last series:
        \begin{displaymath}
        {\sigma}'_{1} \,=\, {\sigma}_{1}(a_{1} - a_{2}), \, {\sigma}'_{2} \,=\, {\sigma}_{1}(a_{1} - a_{2}) + {\sigma}_{2}(a_{2} - a_{3}), \,
    {\sigma}'_{3} \,=\, {\sigma}_{1}(a_{1} - a_{2}) + {\sigma}_{2}(a_{2} - a_{3}) + {\sigma}_{3}(a_{3} -a_{4}),
        \end{displaymath}
    and so on. The general formula is
        \begin{displaymath}
        {\sigma}'_{k} \,=\, {\sigma}_{1}(a_{1} - a_{2}) + {\sigma}_{2}(a_{2} - a_{3}) + {\sigma}_{3}(a_{3} -a_{4}) + \,{\ldots}\, + {\sigma}_{k}(a_{k}-a_{k+1}) \h ({\ast}{\ast})
        \end{displaymath}
    Now regroup these terms (which is allowed because this is a {\em finite} sum) to get
        \begin{displaymath}
        {\sigma}'_{k} \,=\, {\sigma}_{1}a_{1} + ({\sigma}_{2} - {\sigma}_{1})a_{2} + ({\sigma}_{3}-{\sigma}_{2})a_{3} + \,{\ldots}\, + ({\sigma}_{k} - {\sigma}_{k-1})a_{k} - {\sigma}_{k}a_{k+1} \h ({\ast}{\ast}{\ast})
        \end{displaymath}
    However, one has $x_{1} \,=\, {\sigma}_{1}$, and $x_{k} \,=\, {\sigma}_{k} - {\sigma}_{k-1}$ if $k\,\,{\geq}\,\,2$.
    Thus Equation~$({\ast}{\ast}{\ast})$ takes the form
        \begin{displaymath}
        {\sigma}'_{k} \,=\, a_{1}x_{1} + a_{2}x_{2} + \,{\ldots}\, + a_{k}x_{k} - {\sigma}_{k}a_{k+1} \h ({\ast}{\ast}{\ast}{\ast})
        \end{displaymath}
    Since, by hypothesis, the partial sums ${\sigma}_{1}$, ${\sigma}_{2}$,\,{\ldots}\,form a bounded set,
    and the numbers $a_{k}$ approach~$0$ as $k$ goes to~${\infty}$, it follows that the sequence $({\sigma}'_{1},{\sigma}'_{2},\,{\ldots}\,)$ is convergent;
    that is, the series $\sum_{k=1}^{{\infty}} a_{k}x_{k}$ is convergent, as claimed.

\V

        \subsection{\small{{\bf Remark}}}
        \label{RemrkG40.95}

\V

        The process of going from Equation~$({\ast}{\ast})$ to Equation~$({\ast}{\ast}{\ast}{\ast})$ proves the following formula:
        \begin{equation}
        \label{EqnG.85}
a_{1}x_{1} + a_{2}x_{2} + \,{\ldots}\, + a_{k}x_{k} \,=\, 
{\sigma}_{k}a_{k+1} + {\sigma}_{1}(a_{1} - a_{2}) + {\sigma}_{2}(a_{2} - a_{3}) + {\sigma}_{3}(a_{3} -a_{4}) + \,{\ldots}\, + {\sigma}_{k}(a_{k}-a_{k+1})
        \end{equation}
    This equation is called {\bf Abel's Partial Summation Formula}.

\V
\V

        \subsection{\small{{\bf Corollary}} (Abel's Test)}
        \label{ThmG40.100}

\V

        Suppose that $\sum_{k=1}^{{\infty}} x_{k}$ is a {\em convergent} infinite series.
    Let ${\beta} \,=\, (b_{1},b_{2},\,{\ldots}\,)$ be a convergent monotonic sequence;
    note that we do not insist that ${\beta}$ converge to~{\em zero}.
    Then the series $\sum_{k=1}^{{\infty}} b_{k}x_{k}$ is convergent.

\V

        \underline{Proof} Let $B \,=\, \lim_{k \,{\rightarrow}\, {\infty}} b_{k}$.
    Define ${\alpha} \,=\, (a_{1},a_{2},\,{\ldots}\,)$ by the rule $a_{k} \,=\, B-b_{k}$.
    Then it is clear that ${\alpha}$ is monotonic and converges to~$0$. It is also clear that the sequence of partial sums of the series $\sum_{k=1}^{{\infty}} x_{k}$, being convergent, is bounded.
    Thus Dirichlet's Test can be applied to conclude that the series $\sum_{k=1}^{{\infty}} a_{k}x_{k}$ is convergent. However, one also has $b_{k} \,=\, B - a_{k}$, and the series $\sum_{k=1} Bx_{k}$ and $\sum_{k=1}^{{\infty}} a_{k}x_{k}$ both are convergent.
    Thus it follows from the usual algebraic rules for infinite series that the series $\sum_{k=1}^{{\infty}} b_{k}x_{k}$ is convergent. Indeed, one has
        \begin{displaymath}
        \sum_{k=1}^{{\infty}} Bx_{k} - \sum_{k=1}^{{\infty}} a_{k}x_{k} \,=\, 
    \sum_{k=1}^{{\infty}} (B-a_{k})x_{k} \,=\, \sum_{k=1}^{{\infty}} b_{k}x_{k}.
        \end{displaymath} 
    

                \section{{\bf Convergence Tests for Series of Nonnegative Terms}}
                \label{SectG50}

\V

        In light of the results in the preceding section, it is important to study the properties of those infinite series whose terms are all nonnegative.

        The first such property is given by the familiar `comparison tests' from elementary calculus.

\V

        \subsection{\small{{\bf Theorem}} (The Basic Comparison Tests for Convergence/Divergence)}
        \label{ThmG50.10}

\V

        Suppose that ${\alpha} \,=\, (a_{1},a_{2},\,{\ldots}\,)$ and ${\beta} \,=\, (b_{1},b_{2},\,{\ldots}\,)$ are sequences of real numbers such that for some real number $c\,>\,0$ one has
        \begin{displaymath}
        0\,\,{\leq}\,\,a_{k}\,\,{\leq}\,\,cb_{k} \mbox{ for all $k$ in ${\NN}$} \h ({\ast})
        \end{displaymath}

\V

        (a) If $\sum_{k=1}^{{\infty}} b_{k}$ is convergent, then so is $\sum_{k=1}^{{\infty}} a_{k}$.
    More precisely, one has
        \begin{displaymath}
        0\,\,{\leq}\,\,\sum_{k=1}^{{\infty}} a_{k}\,\,{\leq}\,\,c\left(\sum_{k=1}^{{\infty}} b_{k}\right), \mbox{ with equality if, and only if, $a_{k} \,=\, cb_{k}$ for all $k$}.
        \end{displaymath}

\V

        (b) If $\sum_{k=1}^{{\infty}} a_{k}$ is divergent, then so is $\sum_{k=1}^{{\infty}} b_{k}$.
    More precisely, one has
        \begin{displaymath}
        \sum_{k=1}^{{\infty}} a_{k} \,=\, \sum_{k=1}^{{\infty}} b_{k} \,=\, +{\infty}.
        \end{displaymath}

\V

        The simple proofs are left as an exercise.

\V
\V

    The next results study the convergence properties for the special class of power series -- see Definition~\Ref{DefG30.130}  above --  whose terms are nonnegative.

\V

        \subsection{\small{{\bf Theorem}}}
        \label{ThmG50.20}

\V

        Consider a power series ${\sigma} \,=\, \sum_{j=0}^{{\infty}} a_{j}r^{j}$,
    in which the quantities $a_{j}$ and $r$ are all assumed to be nonnegative.
    Let $S_{{\sigma}}$ be the set of all real numbers $r\,\,{\geq}\,\,0$ such that the sequence $(a_{0}, a_{1}r, a_{2}r^{2}, \,{\ldots}\,a_{j}r^{j},\,{\ldots}\,)$ is bounded;
    note that $S_{{\sigma}} \,\,{\neq}\,\, {\emptyset}$ since clearly $0{\in}S$.
    Let $R \,=\, {\sup}\,S_{{\sigma}}$ (which exists, since $S_{{\sigma}} \,\,{\neq}\,\, {\emptyset}$).
    Then the series $\sum_{j=0}^{{\infty}} a_{j}r^{j}$ is convergent if $r\,<\,R$ and is divergent if $r\,>\,R$.
    More precisely:

\V

        (a) Suppose that $R \,=\, 0$.
    Then the series ${\sigma}$ converges if $r \,=\, 0$, but diverges very badly (see Definition~\Ref{DefG40.80}) if $r\,>\,0$.


\V

        (b) Suppose that $R\,>\,0$. Then for each $r$ such that $0\,\,{\leq}\,\,r\,<\,R$, there exists a number ${\rho}$,
    with $0\,\,{\leq}\,\,{\rho}\,<\,1$, and a real number $B\,>\,0$, such that
        \begin{displaymath}
        a_{j}t^{j}\,\,{\leq}\,\,B{\rho}^{j} \mbox{ for each $j \,=\, 0,1,2,\,{\ldots}\,$ and each $t$ in $[0,r]$}.
        \end{displaymath}

        In constrast, if $r\,>\,R$ then the series ${\sigma}$ diverges very badly.
    (Note that if $R \,=\, +{\infty}$ then this part of the statement is vacuously true, since the condition $r\,>\,+{\infty}$ can never occur.)

\V

        {\bf Proof}

\V

        (a) The hypothesis that $R \,=\, 0$ implies that $S_{{\sigma}} \,=\, \{0\}$. If $r \,=\, 0$ then the series reduces to the trivial series $a_{0} + 0 + 0 + \,{\ldots}\,$, which of course converges to~$a_{0}$.
    In contrast, if $r\,>\,0$ then $r\not\in S$, hence (by definition of $S_{{\sigma}}$) the sequence $(a_{0}, a_{1}r,  \,{\cdots}\,  a_{k}r^{k},\,{\ldots}\,)$ is unbounded.
    It follows easily that the series $\sum_{j=0}^{{\infty}} a_{j}r^{j}$ diverges very badly.

\V

        (b) Suppose $0\,\,{\leq}\,\,r\,<\,R$.
    Then, by the fact that $R$ is the supremum of the set $S_{{\sigma}}$, there exists $r_{1}$ in $S$ such that $r\,<\,r_{1}\,\,{\leq}\,\,R$.
    Thus, there exists a number $B\,>\,0$ such that $0\,\,{\leq}\,\,a_{j}r_{1}^{j}\,\,{\leq}\,\,B$ for all $j$.
    Now suppose that $0\,\,{\leq}\,\,t\,\,{\leq}\,\,r$. One gets
        \begin{displaymath}
        0\,\,{\leq}\,\,a_{j}t^{j}\,\,{\leq}\,\,a_{j}r^{j} \,=\, a_{j}r_{1}^{j}\left(\frac{r}{r_{1}}\right)^{j}\,\,{\leq}\,\,B{\rho}^{j},
        \end{displaymath}
    for all~$j$, where ${\rho} \,=\, r/r_{1}$.
    Since $0\,\,{\leq}\,\,r\,<\,r_{1}$, it follows that $0\,\,{\leq}\,\,{\rho}\,<\,1$, as required.

        Suppose instead that $r\,>\,R$. Then, since $R$ is an upper bound of the set $S_{{\sigma}}$, it follows that $r$ is not in $S_{{\sigma}}$.
    In particular, the sequence $(a_{0}, a_{1}r,\,{\ldots}\,a_{j}r^{j},\,{\ldots}\,)$ is not bounded above. It follows that the series $\sum_{j=0}^{{\infty}} a_{j}r^{j}$ is very badly divergent.

\V
\V

        \subsection{\small{{\bf Definition}} (Radius of Convergence of a Power Series)}
        \label{ThmG50.30}

\V

        Let $\sum_{j=0}^{{\infty}} a_{j}r^{j}$ and $R$ be as in the preceding theorem.
    Then one calls $R$ the {\bf radius of convergence} of this power series.

        More generally, if $\sum_{j=0}^{{\infty}} b_{j}u^{j}$ is a {\em general} power series,
    i.e., no restrictions that the coefficients or $u$ be nonnegative, then the radius of convergence of the associated series $\sum_{j=0}^{{\infty}} |b_{j}||u|^{j}$
    is also called the radius of convergence of the (general) series $\sum_{j=0}^{{\infty}} b_{j}u^{j}$.

\V


        \subsection{\small{{\bf Remarks}}}
        \label{RemrkG50.40}

\V

\hspace*{\parindent}(1) The `radius' terminology comes from the archetype of the concept of `power series';
    namely, the Taylor series $\sum_{j=0}^{{\infty}} a_{j}(x-c)^{j}$, about a point $c$, of a $C^{{\infty}}$ function~$f$;
    here one has $a_{j} \,=\, f^{(j)}(c)/j!$.
    In that context one thinks of $c$ as the `center point' of the series, so that $r \,=\, |x-c|$ is the distance from the center.

\V


       (2) The preceding theorem can be interpreted as saying that a power series is convergent (in a rather strong sense) {\em inside} the radius of convergence, i.e., when $r\,<\,R$,
    while it is divergent (in a rather strong sense) {\em outside} the radius of convergence, i.e., when $r\,>\,R$.
    In particular, the theorem says nothing when $r \,=\, R$; that is, {\em at} the radius of convergence;
    and as some examples below illustrate, nothing general {\em can} be said when $r \,=\, R$.

        Note that this last statement holds even for a `general' power series; that is,
    a series $\sum_{j=0}^{{\infty}} b_{j}u^{j}$ for which there is no restriction that $b_{j}$ or $u$ be nonnegative.
    Indeed, suppose that $R$ is the radius of convergence of such a series, so that $R$ is the radius of the series $\sum_{j=0}^{{\infty}} |b_{j}||u|^{j}$.
    This latter series is convergent when $|u|\,<\,R$, and thus by Corollary~\Ref{CorG40.30} the original series $\sum_{j=0}^{{\infty}} b_{k}u^{k}$  also converges.
    Likewise, if $|u|\,>\,R$, then the series $\sum_{j=0}^{{\infty}} |b_{j}||u|^{j}$ diverges {\em very badly},
    and thus the same is true for the series $\sum_{j=0}^{{\infty}} b_{j}u^{j}$.

\V

        (3) The preceding example tells us that we could have defined the quantity $R$ described in Theorem~\Ref{ThmG50.20} directly in terms of the convergence/divergence of the given power series:
    $R$ is the unique quantity such that the series is convergent when $r\,<\,R$ and divergent when $r\,>\,R$.
    Many texts do define $R$ that way, with no reference to the boundedness of the sequence of terms of the power series.
    The advantage of focusing on the boundedness of the sequence of terms of the power series is that often it is easier to check for that boundedness than it is to check directly for the convergence of the series.


\V
\V


        \subsection{\small{{\bf Examples}}}
        \label{ExampG50.50}

\V

\hspace*{\parindent}(1) The power series ${\sigma} \,=\, {\displaystyle \sum_{j=0}^{{\infty}} \frac{r^{j}}{j!}}$ has radius of convergence $R \,=\, +{\infty}$.
    Indeed, as is shown in Example~\Ref{ExampC20.20}~(3), the sequence $(1, r, r^{2}/2!, r^{3}/3!, \,{\ldots}\,)$ converges to~$0$, and thus is bounded, for each $r\,\,{\geq}\,\,0$.
    Thus the set $S_{{\sigma}}$ referred to in Theorem~\Ref{ThmG50.20} equals the interval $[0,+{\infty})$, so ${\sup}\,S_{{\sigma}} \,=\, +{\infty}$.

\V

        (2) Consider the series ${\sigma} \,=\, \sum_{j=0}^{{\infty}} (j!)r^{j}$. Then it is easy to see that if $r\,>\,0$ then the sequence $(1, r, (2!)r^{2}, (3!)r^{3},\,{\ldots}\,)$ diverges to~$+{\infty}$.
    It follows that $S_{{\sigma}} \,=\, \{0\}$, hence $R \,=\, 0$.

\V

        (3) The geometric series ${\sigma} \,=\, 1+r+r^{2} + r^{3}+ \,{\ldots}\,$ clearly has radius of convergence $R \,=\, 1$.
    Indeed, the sequence $(1,r,r^{2},\,{\ldots}\,)$ is bounded above by~$1$ if $0\,\,{\leq}\,\,r\,\,{\leq}\,\,1$, and is unbounded if $r\,>\,1$.
    Thus, $S_{{\sigma}} \,=\, [0,1]$, so $R \,=\, {\sup}\,S_{{\sigma}} \,=\, 1$.

        More generally, if $R_{0}\,>\,0$, then one can show that the geometric series ${\displaystyle {\sigma} \,=\, \sum_{j=0}^{{\infty}} \left(\frac{r}{R_{0}}\right)^{j}}$ has $R \,=\, R_{0}$.

        Notice that with such a geometric series the series fails to converge when $r \,=\, R$.
    Indeed, when $r \,=\, R$ the series reduces to $1+1+1+\,{\ldots}\,$. In particular, it diverge badly, but not {\em very} badly, when $r \,=\, R$.

\V

        (4) Consider the series ${\displaystyle {\sigma} \,=\, \sum_{k=1}^{{\infty}} \frac{r^{k}}{k}}$;
    this series is obtained from the origial geometric series using term-by-term antidifferentiation (see Part~(3) of Definition~\Ref{DefG30.130}).
    It is obvious that if $0\,\,{\leq}\,\,r\,\,{\leq}\,\,1$ then $r{\in}S_{{\sigma}}$.

        The situation for $r\,>\,1$ is not so obvious. However, the following analysis provides the answer:

        Let $x_{k} \,=\, {\ln}\,(r^{k}/k)$. Then, by the usual properties of logarithms, one has
        \begin{displaymath}
        x_{k} \,=\, k{\ln}\,(r) - {\ln}\,(k) \,=\,
    k{\cdot}\left({\ln}\,(r) - \frac{{\ln}\,k}{k}\right)
        \end{displaymath}
    One can easily show that $\lim_{k \,{\rightarrow}\, {\infty}} ({\ln}\,k)/k \,=\, 0$; for example, use L'H\^{o}pital's Rule.
    Since ${\ln}\,r\,>\,0$ when $r\,>\,1$, it then follows easily, using the Archimedean Principle, that $\lim_{k \,{\rightarrow}\, {\infty}} x_{k} \,=\, +{\infty}$. Thus one gets
        \begin{displaymath}
        \lim_{k \,{\rightarrow}\, {\infty}} \frac{r^{k}}{k} \,=\, \lim_{k \,{\rightarrow}\, {\infty}} e^{x_{k}} \,=\, +{\infty} \mbox{ when $r\,>\,1$}.
        \end{displaymath}
    Combining this with what was shown before, one sees that $S_{{\sigma}} \,=\, [0,1]$, and so the radius of convergence of the given series is $R \,=\, 1$.
    Note that in this case the series reduces to the Harmonic Series $1+1/2+1/3+\,{\ldots}\,$ when $r \,=\, R \,=\, 1$. In particular, the series diverges at the radius of convergence itself;
    however, since the terms $1/k$ of the Harmonic Series converge to $0$, it turns out in this example that the divergence of the power series when $r \,=\, R$ is {\em not} `bad'.

\V

        (5) More generally, it is easy to modify the preceding argument to show that if $m$ is any fixed natural number such that $m\,\,{\geq}\,\,2$, then the power series ${\displaystyle \sum_{k=1}^{{\infty}} \frac{r^{k}}{k^{m}}}$ has radius of convergence $R \,=\, 1$.
    However, in this case the power series is convergent when $r \,=\, R$.

\V

        (6) Now let $m$ be a fixed positive integer, and consider the power series ${\sigma} \,=\, \sum_{k=1}^{{\infty}} k^{m}r^{k-1}$;
    note that if $m \,=\, 1$ then this series is obtained from the Geometric Series using term-by-term differentiation
    (see Part~(2) of Definition~\Ref{DefG30.130}).
    It is clear that if $r\,\,{\geq}\,\,1$ then the sequence of terms is unbounded,
    but it is less clear what happens when $0\,<\,r\,<\,1$. However, by an argument similar to that used in Example~(4) above one can show that if $0\,<\,r\,<\,1$ then $\lim_{k \,{\rightarrow}\, {\infty}} k^{m}r^{k-1} \,=\, 0$.
    In particular, $r{\in}S_{{\sigma}}$ when $0\,\,{\leq}\,\,r\,<\,1$.
    Thus once again one has $R \,=\, 1$.
    Note, however, that in this case the power series diverges very badly when $r \,=\, R$.

\V

        {\bf Remark} The preceding examples illustrate the fact that the convergence behavior of a power series of nonnegative terms at the radius of convergence can vary wildly from case to case:
    the series may converge when $r \,=\, R$, or it may diverge but not badly, or it may diverge badly but not very badly, or it may diverge very badly.
    Compare this to the behavior just inside the radius, where the series always converges,
    and the behavior just outside the radius, where the divegence is always very bad.
    The moral of the story is: be very careful when dealing with a power series at its radius of convergence.

\V
\V

        Examples~(4) and~(6) above can be used to prove the following important result.

\V

        \subsection{\small{{\bf Theorem}} (Radius of Convergence and Term-by-Term Differentiation and Integration)}
        \label{ThmG50.60}

\V

        Suppose that $\sum_{j=0}^{{\infty}} a_{j}r^{j}$ is a power series for which the coefficients $a_{j}$ and the quantity $r$ are all nonnegative.
    Let $R$ denote the radius of convergence of this power series. Then $R$ is also the radius of convergence of any power series obtained from 
    $\sum_{j=0}^{{\infty}} a_{j}r^{j}$ using term-by-term differentiation or term-by-term integration a finite number of times.

\V

        {\bf Proof} Suppose that $\sum_{k=0}^{{\infty}} b_{k}r^{k}$ is obtained from the original series using term-by-term differentiation a single time;
    thus $b_{k} \,=\, (k+1)a_{k+1}$ for each $k \,=\, 0,1,2,\,{\ldots}\,$.
    Let $R'$ be the radius of convergence of this derived power series.
    By the definition of the radius of convergence $R$ of the original series, one knows that the sequence
    $(a_{0}, a_{1}r, a_{2}r^{2},\,{\ldots}\,)$ is unbounded if $r\,>\,R$. Since $b_{k}r^{k} \,=\, \left((k+1)a_{k+1}r^{k+1}\right)$,
    it follows that the sequence $(b_{0}, b_{1}r, b_{2}r^{2},\,{\ldots}\,)$ is also unbounded when $r\,>\,R$.
    In particular, $R'\,\,{\leq}\,\,R$.

    Now suppose that $0\,<\,r\,<\,R$, and let $r_{1}$ be a number such that $0\,<\,r\,<\,r_{1}\,<\,R$.
    By the definition of the radius of convergence $R$, one knows that the sequence $(a_{0}, a_{1}r_{1}, a_{2},r_{1}^{2},\,{\ldots}\,)$ is bounded.
    Let $M$ be a real number such that $a_{j}r_{1}^{j}\,\,{\leq}\,\,M$ for all $j \,=\, 0,1,2,\,{\ldots}\,$.
    Note that
        \begin{displaymath}
        b_{k}r^{k} \,=\, (k+1)a_{k+1}r^{k} \,=\, a_{k+1}r_{1}^{k+1} \left((k+1)\frac{r}{r_{1}}\right)^{k+1}\frac{1}{r} 
    \,\,{\leq}\,\,\frac{M}{r}\left((k+1){\rho}^{k+1}\right),
        \end{displaymath}
    where ${\rho} \,=\, r/r_{1}\,<\,1$. It follows from the results obtained in Example~\Ref{ExampG50.50}~(6) above that $\lim_{k \,{\rightarrow}\, {\infty}} (k+1){\rho}^{k+1} \,=\, 0$.
    Thus one has $\lim_{k \,{\rightarrow}\, {\infty}} b_{k}r^{k} \,=\, 0$; in particular, the terms of this sequence are bounded.
    It follows that $r\,\,{\leq}\,\,R'$. Since this is true for every $r$ in the interval $(0,R)$, it follows that $R\,\,{\leq}\,\,R'$.
    Combining these results yields $R' \,=\, R$, as claimed. The fact that the radius of convergence remains unchanged under repeated differentiations of the original poser series now follows by using Mathematical Induction.

        Next, let $\sum_{m=0}^{{\infty}} c_{m}r^{m}$ be obtained from the original power series using term-by-term antidifferentiation.
    Then the original power series $\sum_{j=0}^{{\infty}} a_{k}r^{k}$ can be obtained by differentiating the series $\sum_{m=0}^{{\infty}} c_{m}r^{m}$ term-by-term.
    Thus, by what was just proved for term-by-term differentiation, the series $\sum_{m=0}^{{\infty}} c_{m}r^{m}$ and $\sum_{j=0}^{{\infty}} a_{j}r^{j}$ have the same radius of convergence.
    And once again the result for repeated term-by-term antidifferentiation of the original series follows by Mathematical Induction.

\V
\V

% Exercises: The radius of convergence is (is not) invariant under permutation of the coefficents


\V
\V

        The next result provides a `formula' for the radius of convergence $R$ in terms of the coefficients~$a_{j}$.

\V

        \subsection{\small{{\bf Theorem}} (Cauchy's Power-Series Theorem)}
        \label{ThmG50.70}

\V

        Suppose that $\sum_{j=0}^{{\infty}} a_{j}r^{j}$ is a power series in which the quantity $r$ and the coefficients $a_{j}$, $j \,=\, 0,1,2,\,{\ldots}\,$ are all nonnegative.
    Let $R$ denote the corresponding radius of convergence of this series, and let $L \,=\, {\displaystyle {\limsup}_{k \,{\rightarrow}\, {\infty}} \sqrt[k]{a_{k}}}$.
    Then $R \,=\, 1/L$.

        Note: As is customary, we use the convention here that $R \,=\, +{\infty}$ when $L \,=\, 0$ and $R \,=\, 0$ when $L \,=\, +{\infty}$.

\V

        {\bf Proof} Suppose that $c\,>\,L$. Then, by the basic properties of `$\limsup$',
    there exists $N$ in ${\NN}$ so that if $k$ is an index such that $k\,\,{\geq}\,\,N$, then $\sqrt[k]{a_{k}}\,<\,c$.
    Thus for $k\,\,{\geq}\,\,N$ one has $a_{k}\,<\,c^{k}$, hence ${\displaystyle a_{k}\left(\frac{1}{c}\right)^{k}}\,<\,1$.
    It follows that the sequence ${\displaystyle \left(a_{0}, a_{1}\left(\frac{1}{c}\right), a_{2}\left(\frac{1}{c}\right)^{2},\,{\ldots}\,\right)}$ is bounded, hence ${\displaystyle \frac{1}{c}\,\,{\leq}\,\,R}$.
    Since this is true for {\em every} $c\,>\,L$, it follows that ${\displaystyle \frac{1}{L}\,\,{\leq}\,\,R}$.

        Next, suppose that $0\,<\,c_{1}\,<\,c_{2}\,<\,L$. Then there exist infinitely many indices $k$ such that $\sqrt[k]{a_{k}}\,>\,c_{2}$.
    That is, there exist infinitely many $k$ such that $a_{k}\,>\,c_{2}^{k}$.     Divide both sides by $c_{1}^{k}$ to get
        \begin{displaymath}
     a_{k}\left(\frac{1}{c_{1}}\right)^{k}
\,>\,\left(\frac{c_{2}}{c_{1}}\right)^{k} \h ({\ast})
        \end{displaymath}
    Since $c_{2}/c_{1}\,>\,1$, and $({\ast})$ holds for infinitely many values of $k$, it follows that the sequence ${\displaystyle \left(a_{0}, a_{1}\left(\frac{1}{c_{1}}\right), \,{\ldots}\,\right)}$ is unbounded.
    Thus $1/c_{1}\,\,{\geq}\,\,R$. Since this is true for every $c_{1}$ such that $0\,<\,c_{1}\,<\,L$, it follows that $1/L\,\,{\geq}\,\,R$.

        Combine the preceding results to get ${\displaystyle R \,=\, \frac{1}{L}}$, as required.

\V
\V

        {\bf Remark} Cauchy introduced the concept of the limit supremum of a sequence in his treatment of the preceding result;
    he called it the `upper limit' (but in French, of course).

\V
\V

        Cauchy's Power Series Theorem can be used to prove a pair of well-known tests for convergence.

\V
\V


        \subsection{\small{{\bf Theorem}} (The Root Test)}
        \label{ThmG50.80}

\V

        Suppose that $\sum_{j=0}^{{\infty}} a_{j}$ is a series of nonnegative terms.
    Let ${\displaystyle L \,=\, {\limsup}_{k \,{\rightarrow}\, {\infty}}\, \sqrt[k]{a_{k}}}$.

\V

        (a) If $L\,<\,1$ then the series $\sum_{j=0}^{{\infty}} a_{j}$ is convergent.

\V

        (b) If $L\,>\,1$ then the series $\sum_{j=0}^{{\infty}} a_{j}$ is badly divergent.

\V

        {\bf Proof}

\V

        Note that, by Cauchy's Power Series Theorem,  the radius of convergence of the power series $\sum_{j=0}^{{\infty}} a_{j}r^{j}$ is $R \,=\, 1/L$.

\V


    (a) If $L\,<\,1$ then $R \,=\, 1/L\,>\,1$. Then it follows from Theorem~\Ref{ThmG50.20} that the power series is convergent at $r \,=\, 1$.

\V

        (b) Similarly, if $L\,>\,1$ then $R \,=\, 1/L\,<\,1$, so $1\,>\,R$. Then it follows from the same theorem that the series diverges badly when $r \,=\, 1$.

\V
\V

        \subsection{\small{{\bf Theorem}} (The Ratio Test)}
        \label{ThmG50.90}

\V

        Suppose that $\sum_{j=0}^{{\infty}} a_{j}$ is a series of {\em positive} terms.
    Let ${\displaystyle L^{+} \,=\, {\limsup}_{j \,{\rightarrow}\, {\infty}} \frac{a_{j+1}}{a_{j}}}$,
    and let ${\displaystyle L^{-} \,=\, {\liminf}_{j \,{\rightarrow}\, {\infty}} \frac{a_{j+1}}{a_{j}}}$.

\V

        (a) If $L^{+}\,<\,1$ then the series converges.

\V

        (b) If $L^{-}\,>\,1$ then the series diverges badly.

\V

        {\bf Proof}

\V

        (a) By the basic properties of `limsup', if $L^{+}\,<\,c\,<\,1$, there exists $N$ in ${\NN}$ so that if $j\,\,{\geq}\,\,N$ then $a_{j+1}/a_{j}\,<\,c$.
    It follows that for every $m$ in ${\NN}$ one has $a_{N+m}\,<\,c^{m}a_{N}$.
    Divide both sides by $c^{N+m}$ to get $a_{N+m}(1/c)^{N+m}\,<\,a_{N}(1/c)^{N}$ for all $m$. In particular, the sequence $\left(a_{0}, a_{1}(1/c), a_{2}(1/c)^{2}, \,{\ldots}\,\right)$ is bounded, and thus $1/c\,\,{\leq}\,\,R$, where $R$ is the radius of convergence of the power series $\sum_{j=0}^{{\infty}} a_{j}r^{j}$.
    Since $R\,\,{\geq}\,\,1/c\,>\,1$, it follows that this power series is convergent when $r \,=\, 1$; that is, the original series converges, as claimed.

\V

        (b) Left as an exercise.

\V
\V

        There is one more standard convergence test for series of nonnegative terms that should be mentioned.

\V

        \subsection{\small{{\bf Theorem}} (The Integral Test)}
        \label{ThmG50.100}

\V

        Suppose that $f:[m,{\infty}) \,{\rightarrow}\, {\RR}$ is a continuous function which is monotonic down on $[m,{\infty})$, where $m{\in}\{0,1,2,\,{\ldots}\,\}$.
    Let $F:[m,{\infty}) \,{\rightarrow}\, {\RR}$ be the antiderivative of $f$ on $[m,{\infty})$ such that $F(m) \,=\, 0$; that is, $F \,=\, D^{-1}_{m} f$.
    Let $L \,=\, \lim_{x \,{\rightarrow}\, +{\infty}} F(x)$.
    (It is easy to see that this limit exists.)

        (a) If $L$ is finite then the series $\sum_{k=m}^{{\infty}} f(k)$ converges, and one has
        \begin{displaymath}
        L + f(m)\,\,{\geq}\,\,\sum_{k=m}^{{\infty}} f(k)\,\,{\geq}\,\,L.
        \end{displaymath}

\V

        (b) If $L \,=\, +{\infty}$ then $\sum_{k=1}^{{\infty}} f(k) \,=\, +{\infty}$.

\V

        The simple proof is left as an exercise.

\V

        \subsection{\small{{\bf Examples}}}
        \label{ExampG50.110}
 
\V

\hspace*{\parindent}(1) (The `$p$-series Test') Let $p$ be a positive real number.
    Then the series ${\displaystyle \sum_{k=1}^{{\infty}} \frac{1}{k^{p}}}$ is called the {\bf p-series}.

        \underline{Case 1} Suppose that $p \,=\, 1$. Let $f(x) \,=\,1/x$ for all $x\,>\,0$, so that $D^{-1}_{1} f \,=\, {\ln}$.
    Note that $\lim_{x \,{\rightarrow}\, {\infty}} {\ln}\,x \,=\, +{\infty}$.
    Since $f$ is strictly decreasing on $[1,+{\infty})$, one can use the integral test to conclude that the series $\sum_{k=1}^{{\infty}} 1/k$ diverges to $+{\infty}$; that is, the $p$-series diverges if $p \,=\, 1$.
    Of course we alredy know this from before: this is the Harmonic Series.

\V

        \underline{Case 2} Suppose that $p\,>\,1$. Let $f:(0,+{\infty}) \,{\rightarrow}\, {\RR}$ be given by $f(x) \,=\, 1/x^{p}$; clearly this function is continuous and strictly decreasing on $(0,+{\infty})$.
    One computes that ${\displaystyle D^{-1}_{1}f(x) \,=\, \frac{-1}{(p-1)x^{p-1}}}$, from which one gets $L \,=\, 1/(p-1)$.
    In particular, the $p$-series is convergent if $p\,>\,1$.

\V

        \underline{Case 3} A similar argument shows that the $p$-series is divergent if $0\,<\,p\,<\,1$.

\V
\V

        (2) Consider the series ${\displaystyle \sum_{k=2}^{{\infty}} \frac{1}{(k{\ln}\,k})}$.
    Define $f:[2,+{\infty}) \,{\rightarrow}\, {\RR}$ by the rule $f(x) \,=\, 1/(x{\ln}\,x)$;
    note that $f$ is continuous and monotonic down on the interval $[2,+{\infty})$.
    It is easy to verify that $D^{-1}_{2}f(x) \,=\, {\ln}\,({\ln}\,x) - {\ln}\,({\ln}\,2)$.
    Clearly this last function diverges to $+{\infty}$. It follows from the Integral Test that the given series diverges.

\V
\V

        {\bf Remark} The use of `integral' in the phrase `integral text' reflects the older meaning of integral as `antiderivative'.




                \section{{\bf Series of Functions}}
                \label{SectG60}

        In Section~\Ref{SectF05} we discussed convergence properties of {\em sequences} of functions defined on a set.
    Since the convergence of an infinite series can be reduced to facts about convergence of sequences,
    it should come as no surprise that many of the results in Section~\Ref{SectF05} have analogs in the context of infnite series.

\V

        \subsection{\small{{\bf Definition}}}
        \label{DefG60.20}
 
\V

        Let ${\varphi} \,=\, (f_{1},f_{2},\,{\ldots}\,f_{k},\,{\ldots}\,)$ be a sequence of real-valued functions defined on a nonempty subset $X$ of ${\RR}$.
   Associated with the expression $\sum_{k=1}^{{\infty}} f_{k}$ is the corresponding sequence ${\sigma} \,=\, (s_{1},s_{2},\,{\ldots}\,)$ of {\bf partial sums},
    where for each index $k$ one has
        \begin{displaymath}
        s_{k}(x) \,=\, f_{1}(x) + \,{\ldots}\,+ f_{k}(x) \mbox{ for each $x$ in $X$}.
        \end{displaymath}

        (1) The infinite series $\sum_{k=1}^{{\infty}} f_{k}$ is said to {\bf converge at a point $x$ of $X$} provided the numerical sequence $(s_{1}(x),s_{2}(x),\,{\ldots}\,)$ is convergent.

\V

        (2) The series $\sum_{k=1}^{{\infty}} f_{k}$ is said to {\bf converge pointwise on $X$} to a function $g:X \,{\rightarrow}\, {\RR}$
    provided the corresponding sequence of partial sums of the series converges pointwise on $X$ to $g$, in the sense of Definition~\Ref{DefF05.20}~(1);
    eqivalently: provided $\sum_{k=1}^{{\infty}} f_{k}(x) \,=\, g(x)$ for every $x$ in $X$.

\V

        (3) The series $\sum_{k=1}^{{\infty}} f_{k}$ is said to {\bf converge uniformly on $X$} to a function $g:X \,{\rightarrow}\, {\infty}$
    provided the corresponding sequence of partial sums of the series converges uniformly on $X$ to $g$, in the sense of Definition~\Ref{DefF05.20}~(2).

\V

        (4) The series $\sum_{k=1}^{{\infty}} f_{k}$ is said to  be {\bf pointwise Cauchy on $X$}
    provided that the corresponding sequence of partial sums is pointwise Cauchy on $X$, in the sense of Definition~\Ref{DefF05.60}~(1).
    Likewise, the series $\sum_{k=1}^{{\infty}} f_{k}$ is said to  be {\bf uniformly Cauchy on $X$}
    provided that the corresponding sequence of partial sums is uniformly Cauchy on $X$, in the sense of Definition~\Ref{DefF05.60}~(2).

\V
\V


        \subsection{\small{{\bf Theorem}}}
        \label{ThmG60.30}

\V

        Let $X$ be a nonempty subset of ${\RR}$ and let $\sum_{k=1}^{{\infty}} f_{k}$ be an infinite series of functions with domain~$X$.

\V

        (a) A necessary and sufficient condition for this series to converge pointwise on $X$ to some function is that it be pointwise Cauchy on~$X$.

\V

        (b) A necessary and sufficient condition for this series to converge uniformly on $X$ to some function is that it be uniformly Cauchy on~$X$.

\V

        {\bf Proof} Apply Theorem~\Ref{ThmF05.70} to the sequence of partial sums of the given series.

\V


        \subsection{\small{{\bf Theorem}} (The `Uniform-Convergence-Preserves-Continuity' Theorem for Series)}
        \label{ThmG60.40}

\V

        Let $X$ be a nonempty subset of ${\RR}$.

\V

        (a) Let $\sum_{k=1}^{{\infty}} f_{k}$ be an infinite series of functions, each with domain~$X$.
    Assume that the series converges uniformly on $X$ to a function $g:X \,{\rightarrow}\, {\RR}$.
    If each summand $f_{k}$ is continuous at a point $c$ of $X$, then $g$ is also continuous at~$c$. Likewise, if each function $f_{k}$ is continuous on $X$, then $g$ is continuous on~$X$.

\V

        (b) Let $\sum_{k=1}^{{\infty}} f_{k}$ be an infinite series of functions which are defined and contiuous on~$X$.
    Assume that the series converges pointwise on $X$ to a function $g:X \,{\rightarrow}\, {\RR}$,
    and that for each interval $[a,b]$ for which $X\,{\cap}\,[a,b] \,\,{\neq}\,\, {\emptyset}$ the convergence is uniform.
    Then $g$ is continuous on $X$.

\V

        {\bf Proof} Apply Theorem~\Ref{ThmF05.40} to the sequence of partial sums associated with the series $\sum_{k=1}^{{\infty}} f_{k}$.

\V


        \subsection{\small{{\bf Theorem}} (The `Uniform-Convergence and Term-by-Term Antidifferentiation' Theorem for Series)}
        \label{ThmG60.50}

\V

        Let $\sum_{k=1}^{{\infty}} f_{k}$ be a sequence of real-valued functions defined on an open interval $I$ in ${\RR}$.
    Assume that this series converges pointwise on $I$ to a function $g:I \,{\rightarrow}\, {\RR}$, and that on each closed bounded subinterval $[a,b]$ of $I$ the convergence is uniform.
    If each of the functions $f_{k}$ has an antiderivative on $I$, then $g$ has an antiderivative on $I$.
    More precisely, fix a point $c$ in $I$, and set $F_{k} \,=\, D^{-1}_{c} f_{k}$.
    The the series $\sum_{k=1}^{{\infty}} F_{k}$ converges pointwise on $I$ to a function $G:I \,{\rightarrow}\, {\RR}$ such that $G'(x) \,=\, g(x)$ for all $x$ in $I$, and $G(c) \,=\, 0$.
    Moreover, the series $\sum_{k=1}^{{\infty}} F_{k}$ converges uniformly to $G$ on every closed bounded subinterval $[a,b]$ of~$I$.

\V

        {\bf Proof} Apply Theorem~\Ref{ThmF05.90} to the sequence of partial sums of the given series.

\V
\V

        The next result does not seem to follow by simply applying a result from Section~\Ref{SectF05} directly to the sequence of partial sums of a series of functions.

\V

        \subsection{\small{{\bf Theorem}} (The Weierstrass `$M$-Test')}
        \label{ThmG60.60}

\V

        Let $(f_{1},f_{2},\,{\ldots}\,)$ be a sequence of real-valued functions defined on a nonempty subset $X$ of~${\RR}$.
    Suppose that there exists a sequence of nonnegative numbers $M_{1},M_{2},\,{\ldots}\,$ such that for each index $k$ one has $|f_{k}(x)|\,\,{\leq}\,\,M_{k}$ for all $x$ in $X$.
    If the numerical series $\sum_{k=1}^{{\infty}} M_{k}$ is convergent, then the series $\sum_{k=1}^{{\infty}} f_{k}$ is uniformly convergent on~$X$.

\V

        {\bf Proof} Let ${\varepsilon}\,>\,0$ be given, and let $N$ be large enough that if $n\,\,{\geq}\,\,N$,
    then $0\,\,{\leq}\,\,M_{n+1} + M_{n+2} + \,{\ldots}\,+ M_{n+k}\,<\,{\varepsilon}$ for every $k$ in ${\NN}$.
    Then one has
        \begin{displaymath}
        |f_{n+1}(x) + f_{n+2}(x) + \,{\ldots}\, + f_{n+k}(x)|\,\,{\leq}\,\,
        |f_{n+1}(x)| + |f_{n+2}(x)| + \,{\ldots}\, + |f_{n+k}(x)|\,\,{\leq}\,\,
        \]
        \[
    M_{n+1} + M_{n+2} + \,{\ldots}\,+M_{n+k}\,<\,{\varepsilon} \mbox{ for all $x$ in $X$}.
        \end{displaymath}
    Thus, the series $\sum_{k=1}^{{\infty}} f_{k}$ is uniformly Cauchy on~$X$.
    The desired result now follows easily.

\V

        {\bf Remark} The name `$M$ Test' for this result is completely standard in analysis,
    so the reader should certainly be familiar with this name and know to which result it refers.
    Nevertheless, it is often a bad idea to tie the name of a result or concept too closely with irrelevant features such as the specific notation used by a particular author.
    For instance, suppose one wished to apply the Weierstrass test in a context in which the letters $M_{k}$ were already being used for something else.
    Then to refer to the `$M$~Test' but use letters other than $M$ would seem a bit strange, and might even cause confusion.
    Perhaps a better name for the preceding result would have been `The Weierstrass Uniform-Comparison Test'.

\V
\V

        {\bf Important Case -- Functions Defined by Power Series}

\V

        \subsection{\small{{\bf Definition}}}
        \label{DefG60.70}

\V

        \hspace*{\parindent}(1) Suppose that $\sum_{j=0}^{{\infty}} a_{j}(x-c)^{j}$ is a power series whose radius of convergence is $R\,>\,0$.
    This series defines a function $f:(c-R,c+R) \,{\rightarrow}\, {\RR}$ by the rule $f(x) \,=\, \sum_{j=0}^{{\infty}} a_{j}(x-c)^{j}$ for each $x$ in the interval $(c-R, c+R)$.
    (If $R \,=\, +{\infty}$, then we interpret $(c-R,c+R)$ to be the interval $(-{\infty},+{\infty})$; that is, the interval~${\RR}$.)
    We refer to $f$ as {\bf the function determined by the series $\sum_{j=0}^{{\infty}} a_{j}(x-c)^{j}$}.

\V

            (2) Suppose that $f:(c-R,c+R) \,{\rightarrow}\, {\RR}$ is a function defined on an interval of the form $(c-R,c+R)$,
    where $c{\in}{\RR}$ and $0\,<\,R\,\,{\leq}\,\,+{\infty}$; we allow the full domain of~$f$ to be a proper superset of this interval.
    Then one says that {\bf $f$ is represented on $(c-R,c+R)$ by the power series $\sum_{j=0}^{{\infty}} a_{j}(x-c)^{j}$}
    provided the radius of convergence of this power series is at least~$R$, and on the interval $(c-R,c+R)$ the function $f$ agrees with the function determined by this power series.

\V

        Functions of the type described in the preceding definition has some very pleasant properties.

\V

        \subsection{\small{{\bf Theorem}}}
        \label{ThmG60.80}

\V

        Suppose that a function $f:(c-R,c+R) \,{\rightarrow}\, {\RR}$ is represented on a nonempty open interval $(c-R,c+R)$ by a power series $\sum_{j=0}^{{\infty}} a_{k}(x-c)^{j}$.
    Then:

\V

        (a) For each $x$ in the interval $(c-R,c+R)$ the series $\sum_{j=0}^{{\infty}} a_{j}(x-c)^{j}$ converges absolutely to $f(x)$.
    Furthermore, for each $r$ such that $0\,<\,r\,<\,R$ the series converges uniformly on the subinterval $[c-r,c+r]$ to $f$.

\V

        (b) The function $f$ is differentiable on $(c-R,c+R)$, and $f':(c-R,c+R) \,{\rightarrow}\, {\RR}$ is represented on the interval by the power series $\sum_{j=0}^{{\infty}} ja_{j}(x-c)^{j-1}$.

\V

        (c) More generally, $f$ is of class $C^{{\infty}}$ on $(c-R,c+R)$. In fact,
    if $k{\in}{\NN}$ then $f^{(k)}$ is represented on the interval by the power series obtained by differentiating the original series $\sum_{j=0}^{{\infty}} a_{j}(x-c)^{j}$ term-by-term $k$~times.

\V

        (d) Similarly, the $k$-th order antiderivative $D^{-k}_{c} f$ is defined on $(c-R,c+R)$,
    and can be obtained by repeatedly antidifferentiating the power series for $f$ term-by-term.

\V

        (e) The power series $\sum_{j=0}^{{\infty}} a_{j}(x-c)^{j}$ which represents $f$ on the interval $(c-R,c+R)$ is unique.
    In fact, it is the Taylor series of $f$ about the center point~$c$.

\V

        {\bf Proof}

\V

        (a) This follows directly from Part~(b) of Theorem~\Ref{ThmG50.20}.

\V

        (b), (c) and (d) These follow directly from Theorem~\Ref{ThmG50.60}.

\V

        (e) If $f(x) \,=\, \sum_{j=0}^{{\infty}} a_{j}(x-c)^{j}$ on $(c-R,c+R)$, then set $x \,=\, c$ to get $a_{0} \,=\, f(c)$.
    More generally, for $k$ in ${\NN}$ compute $f^{(k)}$ by differentiating the series $k$~times term-by-term to get (after using Part~(c))
        \begin{displaymath}
        f^{(k)}(x) \,=\, k!a_{k} + (k+1)k\,{\ldots}\,2a_{k+1}(x-c) + (k+2)(k+1)k\,{\ldots}\,3a_{k+2}(x-c)^{2} + \,{\ldots}\,
        \end{displaymath}
    In particular when $x \,=\, c$ one gets $f^{(k)}(c) \,=\, k!a_{k}$, so that $a_{k} \,=\, f^{(k)}(c)/k!$.
    Thus the series $\sum_{j=0}^{{\infty}} a_{j}(x-c)^{j}$ has the same coefficients as the Taylor series of $f$ about the center point~$c$, and the desired result follows.

\V
\V

        The class of functions represented by power series, about a given center, behaves nicely under the usual algebraic operations.

\V

        \subsection{\small{{\bf Theorem}}}
        \label{ThmG60.90}

\V

        Let $c$ be a real number, let $R$ be a quantity such that $0\,<\,R\,\,{\leq}\,\,+{\infty}$,
    and let $I \,=\, (c-R,c+R)$; note that if $R \,=\, +{\infty}$ then $I \,=\, {\RR}$.
    Suppose that $f_{1}, f_{2},\,{\ldots}\,f_{m}:I \,{\rightarrow}\, {\RR}$ are functions which can be represented by power series on the interval~$I$;
    that is, for each $k \,=\, 1,2,\,{\ldots}\,m$ and each $j \,=\, 0,1,2,\,{\ldots}\,$, there are coefficients $a_{j}^{(k)}$ such that
        \begin{displaymath}
        f_{k}(x) \,=\, \sum_{j=0}^{{\infty}} a_{j}^{(k)}(x-c)^{j} \mbox{ for each $x$ in $I$} \h ({\ast})
        \end{displaymath}
    Let $R_{k}$ denote the radius of convergence of the power series $\sum_{j=0}^{{\infty}} a_{j}^{(k)}(x-c)^{j}$,
    so that one has $R\,<\,R_{k}$ for each $k \,=\, 1,2,\,{\ldots}\,m$.
    Then:

\V

        (a) Every function $F:I \,{\rightarrow}\, {\RR}$ of the form $F \,=\, c_{1}f_{1} + c_{2}f_{2} + \,{\ldots}\, + c_{m}f_{m}$,
    where $c_{1},c_{2},\,{\ldots}\,c_{m}$ are in ${\RR}$, can be represented by a power series on $I$.
    More precisely, one has
        \begin{displaymath}
        F(x) \,=\, \sum_{j=0}^{{\infty}} b_{j}(x-c)^{j}, \mbox{ for all $x$ in $I$, where $b_{j} \,=\, \sum_{k=1}^{m} c_{k}a_{j}^{(k)}$ for each $j \,=\, 0,1,2,\,{\ldots}\,$}.
        \end{displaymath}
    If $R'$ denotes the radius of convergence of the series $\sum_{j=0}^{{\infty}} b_{j}(x-c)^{j}$, then $R'\,\,{\geq}\,\,{\min}\,\{R_{1},\,{\ldots}\,R_{m}\}$.

\V

        (b) The product function $P:I \,{\rightarrow}\, {\RR}$, given by the rule
        \begin{displaymath}
        P(x) \,=\, f_{1}(x){\cdot}f_{2}(x){\cdot}\,{\ldots}\,{\cdot}f_{m}(x) \mbox{ for all $x$ in $I$},
        \end{displaymath}
    can be represented by a power series in $I$.
    More precisely, define coefficients $p_{k}$, for $k \,=\, 0,1,2,\,{\ldots}\,$, by the rule
        \begin{equation}
        \label{EqnG.90}
        p_{k} \,=\, \sum_{i_{1} + i_{2} + \,{\ldots}\, + i_{m} \,=\, k} a_{i_{1}}^{(1)}{\cdot}a_{i_{2}}^{(2)}{\cdot}\,{\ldots}\,{\cdot}a_{i_{m}}^{(m)}
        \end{equation}
    If $R''$ denotes the radius of convergence of the series $\sum_{k=0}^{{\infty}} p_{k}(x-c)^{k}$, then $R''\,\,{\geq}\,\,{\min}\,\{R_{1},\,{\ldots}\,R_{m}\}$.

\V

        {\bf Proof}

\V

        (a) The simple proof is left to the reader.

\V

        (b) Choose $x$ such that $|x-c|\,<\,R$. Then $x$ lies within the radius of convergence of each of the given power series.
    In particular, each quantity $M_{k} \,=\, \sum_{j=0}^{{\infty}} |a^{(k)}_{j}(x-c)^{j}|$ is finite.

        Next, let $X$ be the set of all $m$-tuples of the form $(j_{1},j_{2},\,{\ldots}\,j_{m})$,
    with each $j_{k}$ in ${\NN}\,{\cup}\,\{0\}$ for each $k \,=\, 1,2,\,{\ldots}\,m$.
    Choose a real number $x$ such that $|x-c|\,<\,R$, and define a corresponding function $G:X \,{\rightarrow}\, {\RR}$ by the following rule:
        \begin{displaymath}
        G((i_{1},i_{2},\,{\ldots}\,i_{m})) \,=\, (a^{(1)}_{i_{1}}x^{i_{1}}){\cdot}(a^{(2)}_{i_{2}}x^{i_{2}}){\cdot}\,{\ldots}\,{\cdot}(a^{(m)}_{i_{m}}x^{i_{m}}) \h ({\ast}{\ast})
        \end{displaymath}

        \underline{Claim 1} The unordered sum $\sum_{X} |G|$ is convergent.

        \underline{Proof of Claim 1} Let $W$ be a finite nonempty subset of $X$.
    Then there exists a positive integer $k$ such that if $(i_{1},i_{2},\,{\ldots}\,i_{m}){\in}W$,
    then $k\,\,{\geq}\,\,i_{j}$ for each $j \,=\, 1,2,\,{\ldots}\,m$. Let $W_{k}$ be the set of all elements
    $(i_{1},i_{2},\,{\ldots}\,i_{m})$ of $X$ such that $i_{j}\,\,{\leq}\,\,k$ for each index~$j$.
    Thus, $W_{k}$ is the $m$-fold Cartesian product of the set $\{0,1,2,\,{\ldots}\,k\}$ with itself.
    It is clear that $W_{k}$ is a finite set (it has exactly $(k+1)^{m}$ elements), and that $W$ is a subset of $W_{k}$.
    Since $|G|$ is a nonnegative function, it follows that $\sum_{W} |G|\,\,{\leq}\,\,\sum_{W_{k}} |G|$.
    However, it follows from the basic laws of finite sums that
        \begin{displaymath}
        \sum_{W_{k}} |G| \,=\, \sum_{0\,\,{\leq}\,\,i_{1},i_{2},\,{\ldots}\,i_{m}\,\,{\leq}\,\,k} |a^{(1)}_{i_{1}}(x-c)^{i_{1}}|{\cdot}|a^{(2)}_{i_{2}}(x-c)^{i_{2}}|{\cdot}\,{\ldots}\,{\cdot}|a^{(m)}_{i_{m}}(x-c)^{i_{m}}|
        \end{displaymath}
    The finite sum on the right can be formulated as `iterated ordered sums' to yield
        \begin{displaymath}
        \sum_{W} |G|\,\,{\leq}\,\,\sum_{W_{k}} |G| \,=\, \sum_{i_{1}=0}^{k} \sum_{i_{2}=0}^{k}  \,{\cdots}\, 
    \sum_{i_{m}=0}^{k} |a^{(1)}_{i_{1}}(x-c)^{i_{1}}|{\cdot}|a^{(2)}_{i_{2}}(x-c)^{i_{2}}|{\cdot}\,{\ldots}\,{\cdot}|a^{(m)}_{i_{m}}(x-c)^{i_{m}}| \,=\, 
    \]
    \[
    \left(\sum_{i_{1}=0}^{k} |a^{(1)}_{i_{1}}(x-c)^{i_{1}}|\right){\cdot}
    \left(\sum_{i_{2}=0}^{k} |a^{(2)}_{i_{2}}(x-c)^{i_{2}}|\right){\cdot} \,{\ldots}\, {\cdot}\left(\sum_{i_{m}=0}^{k} |a^{(m)}_{i_{m}}(x-c)^{i_{m}}|\right)
        \end{displaymath}
    It follows that there is a real constant $M$ such that $\sum_{W} |G|\,\,{\leq}\,\,M$ for all finite subsets $W$ of $X$.
    Indeed, let $M \,=\, M_{1}{\cdot}M_{2}{\cdot}\,{\ldots}\,{\cdot}M_{m}$, where the finite quantities $M_{j}$ are defined above.
    In particular, ${\sup}\,U_{X;|G|}\,\,{\leq}\,\,M\,<\,+{\infty}$, so the unordered sum $\sum_{X} |G|$ is convergent, as claimed.

        Note that it follows from Claim 1, combined with Theorem~\Ref{ThmG20.65}, that the unordered sum $\sum_{X} G$ is also convergent.

        \underline{Claim 2} The unordered sum $\sum_{X} G$ converges to $P(x)$.

        \underline{Proof of Claim 2} This is done is by induction on the number $m$ of factors in the product $P \,=\, f_{1}{\cdot}f_{2}{\cdot}\,{\ldots}\,{\cdot}f_{m}$.

        Indeed, the desired result is trivially true if $m \,=\, 1$, since in this case the result reduces to saying that if $f_{1}$ can be expressed as a power series on $I$, then the same is true for $F \,=\, f_{1}$.

        Now suppose that $m\,\,{\geq}\,\,2$ and that the desired result is true for products involving fewer than $m$ power series.
        Define a partition ${\cal F} \,=\, \{X_{0},X_{1},X_{2},\,{\ldots}\,\}$ of $X$ as follows:
    for each $k \,=\,0, 1,2,\,{\ldots}\,$, $X_{k}$ is the set of all $m$-tuples $(j_{1},j_{2},\,{\ldots}\,j_{m})$ such that $j_{m} \,=\, k$.
    Indeed, this is the partition of $X$ determined by the surjective function ${\varphi}(i_{0},i_{1},\,{\ldots}\,i_{m}) \,=\, i_{m}$.
    It now follows from Theorem~\Ref{ThmG20.70}, the Generalized Associative Law for Infinite Unordered Sums, that
        \begin{displaymath}
        \sum_{X} G \,=\, \sum_{{\cal F}} \hat{G},
        \end{displaymath}
    where
        \begin{displaymath}
        \hat{G}(X_{j}) \,=\, \sum_{X_{j}} G.
        \end{displaymath}
    However, each term in the latter sum has the factor $a^{(m)}_{j}(x-c)^{j}$.
    It is easy to see that one can then write
        \begin{displaymath}
        \sum_{X_{j}} G \,=\, \left(\sum_{0\,\,{\leq}\,\,i_{1},i_{2},\,{\ldots}\,i_{m-1}} a^{(1)}_{i_{1}}(x-c)^{i_{1}}a^{(2)}_{i_{2}}(x-c)^{i_{2}}\,{\ldots}\,a^{(m-1)}_{i_{m-1}}(x-c)^{i_{m-1}}\right)(a^{(m)}_{j}(x-c)^{j}).
        \end{displaymath}
    By the induction hypothesis, the sum which multiplies the factor $a^{(m)}_{j}(x-c)^{j}$ equals $f_{1}(x){\cdot}\,{\ldots}\,{\cdot}f_{m-1}(x)$, independent of~$j$.
    Thus, since the chosen number $x$ lies inside the radius of convergence of each of the functions $f_{1}$, $f_{2}$,\,{\ldots}\,$f_{m}$, one can write
        \begin{displaymath}
        \sum_{X} G \,=\, \left(f_{1}(x){\cdot}\,{\ldots}\,{\cdot}f_{m-1}(x)\right)\left(\sum_{j=0}^{{\infty}} a^{(m)}_{j}(x-c)^{j}\right) \,=\, f_{1}(x){\cdot}\,{\ldots}\,{\cdot}f_{m-1}(x){\cdot}f_{m}(x).
        \end{displaymath}

        The desired power series expansion for the product function $P$ now follows easily.
    Indeed, let $\tilde{{\varphi}}:X \,{\rightarrow}\, {\NN}\,{\cup}\,\{0\}$ be the surjective function given by the rule
        \begin{displaymath}
        \tilde{{\varphi}}(j_{1},j_{2},\,{\ldots}\,j_{m}) \,=\, j_{1} + j_{2} + \,{\ldots}\,+j_{m},
        \end{displaymath}
    and let $\tilde{{\cal F}}$ denote the corresponding partition of $X$.
    Thus, $\tilde{{\cal F}} \,=\, \{Y_{0},Y_{1},\,{\ldots}\,\}$, where for each $k \,=\, 0,2,\,{\ldots}\,$ one has $Y_{k} \,=\, \{(j_{1},j_{2},\,{\ldots}\,j_{m}){\in}X: j_{1} + j_{2} + \,{\ldots}\,+j_{m} \,=\, k\}$.
    It then follows from the Generalized Associative Law for Unordered Infinite Sums that
        \begin{displaymath}
        \sum_{X} G \,=\, \sum_{\tilde{{\cal F}}} \tilde{f} \,=\, \sum_{k=0}^{{\infty}} \tilde{f}(Y_{k}),
        \end{displaymath}
    where
        \begin{displaymath}
        \tilde{f}(Y_{k}) \,=\, \sum_{Y_{k}} G \,=\, \sum_{j_{1} + j_{2} + \,{\ldots}\,+ j_{m} \,=\, k} \left((a^{(1)}_{j_{1}}(x-c)^{j_{1}}){\cdot}(a^{(2)}_{j_{2}}(x-c)^{j_{2}}){\cdot}\,{\ldots}\,{\cdot}
    (a^{(m)}_{j_{m}}(x-c)^{j_{m}})\right) \,=\, 
    \]
    \[
    \left(\sum_{j_{1} + j_{2} + \,{\ldots}\,+ j_{m} \,=\, k} a^{(1)}_{j_{1}}{\cdot}a^{(2)}_{j_{2}}{\cdot}\,{\ldots}\,{\cdot}
    a^{(m)}_{j_{m}}\right)(x-c)^{k} \,=\, p_{k}(x-c)^{k}
        \end{displaymath}
    Combining this with the results of Claim~2 one finally gets
        \begin{displaymath}
        P(x) \,=\, \sum_{k=0}^{{\infty}} p_{k}(x-c)^{k},
        \end{displaymath}
    where $p_{k}$ is given by Equation~\Ref{EqnG.90}, as required.

\V

        \subsection{\small{{\bf Remarks}}}
        \label{RemrkG60.100}

\V

        \hspace*{\parindent}(1) Many texts state the preceding theorem explicitly only in the case $m \,=\, 2$.
    In that case one normally writes $f$ and $g$ instead $f^{(1)}$ and~$f^{(2)}$,
    and one writes $a_{j}$ and $b_{j}$ instead of the more complicated $a^{(1)}_{j}$ and $a^{(2)}_{j}$.
    In this simpler context Equation~\Ref{EqnG.90} can be written in the simpler form
        \begin{equation}
        \label{EqnG.100}
        p_{k} \,=\, a_{0}b_{k} + a_{1}b_{k-1} + \,{\ldots}\, + a_{k-1}b_{1} + a_{k}b_{0}.
        \end{equation}
    The expression $\sum_{k=0}^{{\infty}} p_{k}(x-c)^{k}$, in which $p_{k}$ is given by Equation~\Ref{EqnG.100},
    is called the {\bf Cauchy Product of the series $\sum_{j=0}^{{\infty}} a_{j}(x-c)^{j}$ with the series $\sum_{j=0}^{{\infty}} b_{j}(x-c)^{j}$}.
    It then makes sense to refer to the series obtained using Equation~\Ref{EqnG.90} above as the {\bf Extended Cauchy Product of the $m$ given power series}.

\V

        (2) It is clear that the radius of convergence of the power series $\sum_{k=0}^{{\infty}} p_{k}(x-c)^{k}$, obtained in the preceding theorem,
    is at least as large as the smallest of the radii $R_{1}$, $R_{2}$,\,{\ldots}\,$R_{m}$ of the original series.
    However, it is possible that the radius of convergence of this product series might be much larger than ${\min}\,\{R_{1},R_{2},\,{\ldots}\,R_{m}\}$.
    For instance, consider the product $P(x) \,=\, f(x){\cdot}g(x)$, where
        \begin{displaymath}
        f(x) \,=\, \frac{1+x^{4}}{1+x^{2}} \mbox{ and }
        g(x) \,=\, \frac{1+x^{2}}{1+x^{4}} \mbox{ for all $x$ in ${\RR}$}.
        \end{displaymath}
   Each of these functions is defined for all $x$ in ${\RR}$, and the Taylor series about the center~$0$ of each has radius of convergence $R \,=\, 1$.
    Their product, $P(x) \,=\, f(x)g(x)$, equals the constant~$1$ for all $x$ in ${\RR}$, and thus its Taylor series about~$0$ has radius of convergence~$+{\infty}$.

\V
\V

        Before discussing several more properties enjoyed by functions which can be represented by power series, it is useful to introduce some terminology.

\V

        \subsection{\small{{\bf Definition}} (Real-Analytic Functions)}
        \label{DefG60.110}

\V

        Let $f:I \,{\rightarrow}\, {\RR}$ be a real-valued function defined on an open set $U$, and let $c$ be a point in $U$.
    One says that $f$ is {\bf real analytic at $c$} provided that $f$ can be represented on some subinterval $(c-R,c+R)$ of $U$, with $R\,>\,0$,
    by a power series $\sum_{j=0}^{{\infty}} a_{k}(x-c)^{k}$.
    The function $f$ is said to be {\bf real analytic on $U$} provided it is real analytic at each point of $U$.


\V

        \subsection{\small{{\bf Example}}}
        \label{ExampG60.115}

        It is an instructive exercise to prove that if $f:{\RR}{\setminus}\{0\} \,{\rightarrow}\, {\RR}$ is the reciprocal function,
    given by the rule $f(x) \,=\, 1/x$, then $f$ is analytic on its domain.

\V
\V

        \subsection{\small{{\bf Remarks}}}
        \label{RemrkG60.120}

\V

\hspace*{\parindent}(1) It is understood in the preceding definition that the radius of convergence $R'$ of the power series
    $\sum_{j=0}^{{\infty}} a_{j}(x-c)^{j}$ is at least as large as the positive number $R$, but that we allow the possibility that $R'\,>\,R$.
    In particular, for each $x$ in the interval $(c-R,c+R)$ the power series converges to the corresponding value $f(x)$ of~$f$.

\V

        (2) Some texts give the following alternate, but equivalent, definition: The function $f$ is said to be real analytic at $c$
    provided that $f$ is $C^{{\infty}}$ at $c$ and the Taylor series ${\displaystyle \sum_{j=0}^{{\infty}} \frac{f^{(j)}(c)}{j!}(x-c)^{j}}$
    converges to $f(x)$ in some nonempty open interval $(c-R,c+R)$ about~$c$.

\V

        (3) Consider the polynomial $f(x) \,=\, -3 + 5(x-1) + 2(x-1)^{2}$.

        \underline{Question} Is this function real analytic on ${\RR}$? (Think about your response before reading further.)

        A natural response to this question is to say

       \h `Obviously it {\em is} real analytic. In fact, it is already expressed as a power series about $c \,=\, 1$, with coefficients $a_{0} \,=\, -3$, $a_{1} \,=\, 5$, $a_{2} \,=\, 2$, and $a_{k} \,=\, 0$ if $k\,\,{\geq}\,\,3$.
    Clearly this series has infinite radius of convergence.'


\noindent However, this response misses a subtle feature of the definition of `real analytic'.
    Namely, the definition requires that the function $f$ be expressible as a convergent power series about {\em each} point $c$ of the interval~$I$.
    The polynomial above is expressed as a convergent power series about $c \,=\, 1$, but what about other values of~$c$?

        In this example the solution is simple: use a technique which we employed, in Section~\Ref{SectE60},
    when discussing ways of calculating Taylor polynomials without computing derivatives.
    Namely, write $x-1$ in the form $(c-1) + (x-c)$ and expand the powers of $(x-1)$ in terms of powers of $x-c$, using the standard Binomial Theorem:
        \begin{displaymath}
        x-1 \,=\, (c-1) + (x-c), \, (x-1)^{2} \,=\, ((c-1) + (x-c))^{2} \,=\, (c-1)^{2} + 2(c-1)(x-c) + (x-c)^{2}.
        \end{displaymath}
    Substitute these expressions into the original formula for $f$ and do some obvious simplifications to get
        \begin{displaymath}
        f(x) \,=\, b_{0} + b_{1}(x-c) + b_{2}(x-c)^{2}, \mbox{ where $b_{0} \,=\, 2c^{2} + c -6$, $b_{1} \,=\, 4c+1$, and $b_{2} \,=\, 2$}.
        \end{displaymath}

\V
\V

        The final remark above illustrates a more general question:
    if a function can be represented by a convergent power series about a particular point $c$, is it real analytic?
    The next theorem answers this question by using the `re-expand about other points' method illustrated in that remark.

\V

        \subsection{\small{{\bf Theorem}}}
        \label{ThmG60.130}

\V

        Suppose that $f:(c-R,c+R) \,{\rightarrow}\, {\RR}$ is a real-valued function which can be represented as a convergent power series
    $\sum_{j=0}^{{\infty}} a_{j}(x-c)^{j}$ on a nonempty open interval $I \,=\, (c-R,c+R)$.
    Then $f$ is real analytic on~$I$. More precisely, if $p$ is any point in $I$ then there is a power series $\sum_{j=0}^{{\infty}} b_{j}(x-p)^{j}$
    which represents $f$ on the open interval $(b-R',b+R')$, where $R' \,=\, R-|c-p|$.
    (If $R \,=\, +{\infty}$ this equation should be interpreted to mean $R' \,=\, +{\infty}$.
    In either case, it is clear thaT $R'\,>\,0$)

\V

        {\bf Proof} Let $x$ be any point of the interval $I$. For each index $j\,\,{\geq}\,\,1$ express the quantity $(x-c)^{j}$ as a polynomial in $x-p$:
        \begin{displaymath}
        (x-c)^{j} \,=\, ((p-c) + (x-p))^{j} \,=\, \sum_{l=0}^{j}
 C(j,l)(p-c)^{l}(x-p)^{j-l}
        \end{displaymath}
 where $C(j,l)$ denotes the {\bf binary coefficient} ${\displaystyle \frac{j!}{l!(j-l)!}}$.
    Note that one has
        \begin{displaymath}
        |x-c|^{j}\,\,{\leq}\,\,\sum_{l=0}^{j}
 C(j,l)|p-c|^{l}|x-p|^{j-l}
        \end{displaymath}
    Now let $X$ be the set of all ordered pairs of the form $(j,l)$ with $j$, $l$ nonnegative integers such that $0\,\,{\leq}\,\,l\,\,{\leq}\,\,j$.
    Define $F:X \,{\rightarrow}\, {\RR}$ by the rule $F(j,l) \,=\, a_{j}C(j,l)(p-c)^{l}(x-p)^{j-l}$.

        \underline{Claim} If $|x-p|\,<\,R-|p-c|$, then the unordered sum $\sum_{X} |F|$ is convergent.

        \underline{Proof of Claim} For convenience, set $r \,=\, |x-p| + |p-c|$, so that $0\,\,{\leq}\,\,r\,<\,R$.
    Let $W$ be a nonempty finite subset of $X$, and let $k$ be the largest first entry of any pair $(j,l)$ in $W$.
    Then it is clear that $W$ is a subset of the finite set $W_{k} \,=\, \{(j,l){\in}X: 0\,\,{\leq}\,\,l\,\,{\leq}\,\,j\,\,{\leq}\,\,k\}$.
    Thus one has
        \begin{displaymath}
        \sum_{W} |F|\,\,{\leq}\,\,\sum_{W_{k}} |F| \,=\, \sum_{j=0}^{k} \sum_{l=0}^{j} |a_{j}|C(j,l)|p-c|^{l}|x-p|^{j-l} \,=\,  
    \sum_{j=0}^{k} |a_{j}|\left(|p-c| + |x-p|\right)^{j}\ \,=\, \sum_{j=0}^{k} |a_{j}|r^{j}\,\,{\leq}\,\,\sum_{j=0}^{{\infty}} |a_{j}|r^{j}.
        \end{displaymath}
    The last sum, which is independent of $k$, is finite because $r\,<\,R$ and the original series $\sum_{j=0}^{{\infty}} a_{k}(x-c)^{j}$
    is absolutely convergent inside the radius of convergence.
    In other words, $\sum_{W} |F|\,\,{\leq}\,\,\sum_{j=0}^{{\infty}} |a_{j}|r^{j}\,<\,+{\infty}$ for every finite subset $W$ of $X$.
    Thus $\sum_{X} |F|$ is convergent, as claimed, and it follows as usual that the unordered sum $\sum_{X} F$ is also convergent.

        Finally, let ${\varphi}:X \,{\rightarrow}\, \{0,1,2,\,{\ldots}\,\}$ be the surjective function given by ${\varphi}(j,l) \,=\, j-l$, and let ${\cal F} \,=\, \{Z_{0},Z_{1},\,{\ldots}\,\}$ be the corresponding partition of $X$.
    Thus, $Z_{k}$ consists of all the pairs $(j,l)$ for which $j-l \,=\, k$.
    Apply the Generalized Associative Law for Unordered Infinite Sums with this partition to get
        \begin{displaymath}
        \sum_{X} F \,=\, \sum_{k=0}^{{\infty}} \hat{F}(Z_{k}).
        \end{displaymath}
    However, ${\displaystyle \sum_{Z_{k}} F \,=\, b_{k}(x-p)^{k}}$ where $b_{k} \,=\, \sum_{l=0}^{k} a_{k+l}C(k+l,l)(p-c)^{l}$.
    Thus
        \begin{displaymath}
        \sum_{X} F \,=\, \sum_{k=0}^{{\infty}} b_{k}(x-p)^{k}.
        \end{displaymath}
    However, if one lets ${\psi}:X \,{\rightarrow}\, \{0,1,2\,{\ldots}\,\}$ be the surjection given by ${\psi}(j,l) \,=\, j$,
    one gets the partition $\tilde{{\cal F}} \,=\, \{\tilde{Z}_{0},\tilde{Z}_{1},\,{\ldots}\,\}$,
    where $\tilde{Z_{j}} \,=\, \{(j,l): 0\,\,{\leq}\,\,l\,\,{\leq}\,\,j\}$.
    One easily computes that
        \begin{displaymath}
        \sum_{\tilde{Z}_{j}} F \,=\, \sum_{l=0}^{j} a_{j}C(j,l)(p-c)^{l}(x-p)^{j-l} \,=\, a_{j}\sum_{l=0}^{j} C(j,l)(p-c)^{l}(x-p)^{j-l}
     \,=\, a_{j}((p-c) + (x-p))^{j} \,=\, a_{j}(x-c)^{j}.
        \end{displaymath}
    Thus one can use the Generalized Associative Law again, but this time with the partition $\tilde{{\cal F}}$, to show that
        \begin{displaymath}
        \sum_{X} F \,=\, \sum_{j=0}^{{\infty}} a_{j}(x-c)^{j} \,=\, f(x).
        \end{displaymath}
    Combine these result to get $f(x) \,=\, \sum_{k=0}^{{\infty}} b_{k}(x-p)^{k}$ if $|x-p|\,<\,R'$.

\V
\V

        \subsection{\small{{\bf Theorem}}}
        \label{ThmG60.140}

\V

        Suppose that $f:U \,{\rightarrow}\, {\RR}$ and $g:V \,{\rightarrow}\, {\RR}$ are real analytic functions on open sets $U$ and $V$, respectively.
    Suppose further that $f[U] \,{\subseteq}\, V$.
    Then the composition $h \,=\, g{\circ}f:U \,{\rightarrow}\, {\RR}$ is real analytic on~$U$.

\V

        {\bf Outline of Proof} Let $c$ be a point of $U$ and set $d \,=\, f(c)$. 
        The `analyticity' hypothesis on $f$ guarantees that there is a power series $\sum_{j=0}^{{\infty}} a_{j}(x-c)^{j}$, and a positive quantity $R$,
    such that $f(x) \,=\, \sum_{j=0}^{{\infty}} a_{j}(x-c)^{j}$ for all $x$ such that $|x-c|\,<\,R$.
    Likewise, there is a power series $\sum_{k=0}^{{\infty}} b_{k}(y-d)^{k}$, and a positive number $R'$,
    so that $g(y) \,=\, \sum_{k=0}^{{\infty}} b_{k}(y-d)^{k}$ for all $y$ such that $|y-d|\,<\,R'$.
    Without loss of generality we may assume that if $|x-c|\,<\,R$ then $f(x)-d\,<\,R'$: simply replace $R$ by a smaller positive quantity if needed.
    Note that for $|x-c|\,<\,R$ one has
        \begin{displaymath}
        h(x) \,=\, g(f(x)) \,=\, \sum_{k=0}^{{\infty}} b_{k}(f(x))^{k} \,=\, 
    \sum_{k=0}^{{\infty}} b_{k}\left(a_{0} + a_{1}(x-c) + a_{2}(x-c)^{2} +  \,{\cdots}\, \right)^{k}
        \end{displaymath}
    In accordance with Theorem~\Ref{ThmG60.90}, one can express the the quantity $\left(a_{0} + a_{1}(x-c) + a_{2}(x-c)^{2} +  \,{\cdots}\, \right)^{k}$
    as a power series $a^{(k)}_{0} + a^{(k)}_{1}(x-c) + \,{\ldots}\, + a^{(k)}_{j}(x-c)^{j} +\,{\ldots}\,$, where the coefficients $a^{(k)}_{j}$ are given by Equation~\Ref{EqnG.90}.
    Now let $X$ be the set of all pairs $(k,j)$ with $k$ and $j$ nonegative integers, and fix $x$ such that $|x-c|\,<\,R$.
    Define $F:X \,{\rightarrow}\, {\RR}$ by the rule $F(k,j) \,=\, b_{k}a^{(k)}_{j}(x-c)^{j}$.
    The desired result then follows by a `Generalized Associative Law' argument similar to the ones used above.
    The details are left as an exercise.

\V

        \subsection{\small{{\bf Corollary}}}
        \label{CorG60.150}

\V

        Suppose that $f:U \,{\rightarrow}\, {\RR}$ is real analytic on  an open subset $U$ of ${\RR}$, and assume that for all $x$ in $U$ one has $f(x) \,\,{\neq}\,\, 0$.
    Then the function $1/f$ is also real analytic on~$U$.

\V

        {\bf Proof} Apply the preceding theorem with $g(y) \,=\, 1/y$ for all $y \,\,{\neq}\,\, 0$; see Example~\Ref{ExampG60.115}.

\newpage

\input{Exercises_M140AB_G_2017} %% NOTE: Automatically starts on a new page

