
% M140AB_E.TeX  Notes for `Single-Variable Analysis'
%
% Revised: 01/20/2017 Encoding: Western ASCII ?
%


                  \chapter{Derivatives and Antiderivatives in ${\RR}$}
                  \label{ChaptE}


        \underline{Quotes for Chapter~\Ref{ChaptE}}: \IndB{chapter quotes}{for Chapter~\Ref{ChaptE} (Derivatives and Antiderivatives in ${\RR}$}

\V

\begin{quotation}
{\footnotesize
        (1) `{\em 6accd{\ae}13eff7i3l9n4o4qrr4s8t12vx}'

        (This is an anagram that appears in a document which Isaac Newton sent to Gottfried Leibniz, via Henry Oldenberg, the secretary of the Royal Society, in 1676.
    The numbers indicate how often the given letter appears in the anagram; for example, `$6a$' means that there are six occurrences of the letter $a$; likewise there is one occurrence of the ligature {\ae}.
    The anagram summarizes, in a concealed way, the new techniques Newton used in obtaining the results he described in the letter.
    Can you figure out the message Newton hid in this anagram? Hint: The message was written in Latin.)


\V  

        (2) `I turn away with fright and horror from this lamentable evil of functions which do not have derivatives.'

        (Translation of part of a letter from C. Hermite to L.~Stieltjes.) 
    % (p. 973 of Klein's `Mathematical Thought')

\V

        (3) `The proofs [of a certain theorem] in many text-books (and in the first three editions of this book) are inaccurate.'

        (From a footnote on Page~217 of the tenth edition of the celebrated text {\em A Course of Pure Mathematics} by G.~H.~Hardy.
    Can you guess which theorem was the cause of so many inaccurate proofs?)

\V

        (4) `Dans le calcul int\'{e}gral il ma paru n\'{e}cesaire de d\'{e}montrer g\'{e}n\'{e}ralement
    l'existence des int\'{e}grales ou fonctions primatives avant de faire connaitre leur diverses propri\'{e}t\'{e}s.'

        Rough translation: `In the integral calculus it seems necessary to me that one show in a general manner
    the existence of antiderivatives before making known their various properties.' (From the introduction to [CAUCHY~1823])

\V

}%EndFootNoteSize
\end{quotation}

\VV

            \small{\bf Introduction}

\V

       The standard elementary (single-variable) calculus courses in the United States focus on two closely related concepts:
    the process of differentiation, and its inverse, the process of antidifferentiation. The purpose of the current chapter is to treat these topics, but more rigorously than in calculus.
    (These courses also include applications of these processes to subjects such as geometry, physics and economics, but such applications are not considered here.)
    Note that it is traditional in these courses to combine the treatment of antidifferentiation with the important topic of the definite integral,
    but in {\ThisText} we postpone the latter topic until Chapter~\Ref{ChaptH}.

    As was mentioned in the `Preliminaries' for Chapter~\Ref{ChaptA} (see Section~\Ref{SectA05}), readers of {\ThisText} should have already taken a course in elementary calculus.
    To avoid simply repeating the standard treatments which are already familiar from such a course,
    in this chapter we often present alternate treatments which can provide different insights to calculus. (Frequently the standard approaches are also included, but as exercises.)
    Many readers of analysis texts at the level of {\ThisText} eventually teach calculus themselves, perhaps as graduate teaching assistants;
    it is useful for such readera to know that there are alternate approaches.

\VV

                \section{{\bf The Derivative -- Basic Definitions}}
                \label{SectE20}\IndB{ZZ Sections}{\Ref{SectE20} The Derivative -- Basic Definitions}
        
\VV

        The reader should be familiar, from elementary calculus, with the standard motivating examples which lead to the concept of `derivative':
    slopes of curves, velocities, rates of change, and so on. Thus, we can omit those examples here and go straight to the formal definitions they suggest.

\V

            \subsection{\small{\bf Definition} (Differentiability; the Derivative)}
            \label{DefE20.20}

        Let $f:I \,{\rightarrow}\, {\RR}$ be a real-valued function defined on an open interval $I$ in ${\RR}$.

\V

        (1) Suppose that $c$ is a point of $I$. One says that {\bf the function $f$ is {\bf differentiable}\IndB{differentiability}{differentiable} at $c$}
    provided that the expression ${\displaystyle \lim_{x \,{\rightarrow}\, c} \frac{f(x)-f(c)}{x-c}}$ exists and is finite.
    The process of computing such limits is called {\bf differentiation}\IndB{differentiability}{differentiation}.
    (The word `differentiation' reflects the presence of the {\em differences} $f(x)-f(c)$ and $x-c$ in the preceding fraction.)
    If $f$ is differentiable at each point $c$ of a subset $X$ of $I$, then one says that $f$ is {\bf differentiable on $X$}. \IndB{differentiability}{differentiable on a set}

\V

        (2) Let $X$ be the set of all $c$ in $I$ such that $f$ is differentiable at $c$, in the sense of Part~(1); note that $X$ need not be an interval.
    If $X \,\,{\neq}\,\, {\emptyset}$ one can associate with $f$ a second function $f':X \,{\rightarrow}\, {\RR}$ given by the rule
        \begin{equation}
        \label{LimE.10}
        f'(c) \,=\, \lim_{x \,{\rightarrow}\, c} \frac{f(x)-f(c)}{x-c} \mbox{ for all $c$ in $X$}.
        \end{equation}
    The function $f'$ is called {\bf the derivative of $\Bfm{f}$}\IndA{derivatives}
    (because it is {\em derived} from $f$; namely, by the process of differentiation).

\VV

            \subsection{\small{\bf Remarks}}
            \label{RemrkE20.20}

\hspace*{\parindent} (1)\,The quantity $f(x)$ is defined for all $x$ in the open interval~$I$,
    but the expression ${\displaystyle g(x) \,=\, \frac{f(x)-f(c)}{x-c}}$ fails to be defined at $x \,=\, c$, since division by zero is not allowed.
    The fact that the derivative involves expressions of the form $\lim_{x \,{\rightarrow}\, c} g(x)$, where $g$ is not defined at~$c$,
    is the main reason that in calculus one follows the approach to limits found in Section~\Ref{SectD50}.

\V

        (2)\,Suppose that $I$ is an open interval, $J$ is an open subinterval of $I$, and $c$ is a point of~$J$ (and thus a point of~$I$).
    Let $f:I \,{\rightarrow}\, {\RR}$ be a function defined on $I$, and let $h:J \,{\rightarrow}\, {\RR}$ be the restriction of $f$ to~$J$.
    Then $f$ is differentiable at $c$ if, and only if, $h$ is differentiable at~$c$; and in this case one has $f'(c) \,=\, h'(c)$; see Remark~\Ref{RemrkD50.20}~(c).

\VV

            The definition of `derivative' used here agrees with that found in all elementary-calculus texts and in most texts on real analysis;
    in particular, we restrict this concept to functions defined on {\em open} intervals in ${\RR}$.
    However, on occasion it is useful to allow the following simple extensions of the `derivative' concept.


            \subsection{\small{\bf Definition} (One-Sided Derivatives)}\IndBD{derivatives}{one-sided derivatives}            
            \label{DefE20.25}

\V

\hspace*{\parindent} Suppose that $f$ is a real-valued function defined on a half-open interval $(a,c]$.
    If $\lim_{x{\nearrow}c} {\displaystyle \frac{f(x)-f(c)}{x-c}}$ exists and is finite, then one says that that $f$ is {\bf differentiable from the left} at $c$.
    One denotes this limit by $f'_{-}(c)$ and calls it the {\bf left-hand derivative of $\Bfm{f}$ at $\Bfm{c}$}. Similarly, suppose that $f$ is defined on an interval $[c,b)$.
    If ${\displaystyle \lim_{x{\searrow}c} {\displaystyle \frac{f(x)-f(c)}{x-c}}}$ exists and is finite,
    one says that $f$ is {\bf differentiable from the right} at~$c$. In this case one denotes that limit by
    $f'_{+}(c)$ and calls it the {\bf right-hand derivative of $\Bfm{f}$ at $\Bfm{c}$}. Each such quantity is called a {\bf one-sided derivative}.

\VV

        The next result states the obvious relation between one-sided derivatives
    and the usual (i.e., `two-sided') derivative; its simple proof is left as an exercise.

\V

            \subsection{\small{\bf Theorem}}
            \label{ThmE20.25A}
\V

        Let $f:I \,{\rightarrow}\, {\RR}$ be a function defined on an open interval~$I$, and let $c$ be a point of~$I$.
    Let $g:(a,c] \,{\rightarrow}\, {\RR}$ and $h:[c,b) \,{\rightarrow}\, {\RR}$ be the restrictions of $f$ to the intervals $(a,c]$ and $[c,b)$, respectively.
    Then $f'(c)$ exists (in the sense of Definition~\Ref{DefE20.20} if, and only if, both of the one-sided derivatives $g_{-}'(c)$ and $h_{+}'(c)$ exist and are equal to each other.
    In this case one has $f'(c) \,=\, g_{-}(c) \,=\, h_{+}(c)$. \Q

\V

        This result can be useful in dealing with the derivative of a function which is given by different formulas on adjacent intervals. Examples are given later in this chapter.

\VV


            \subsection{\small{\bf Theorem (Differentiability Implies Continuity)}}\IndB{differentiability}{differentiability implies continuity}
            \label{ThmE20.30}

\V

        Let $f:I \,{\rightarrow}\, {\RR}$ be a real-valued function defined on an open interval $I$ in ${\RR}$.

\V
        (a) If $f$ is differentiable at a point $c$ of $I$, then $f$ is continuous at~$c$.

\V

        (b) The converse of (a) is not true. That is, it is {\em not} the case that the statement
    `$f$ is continuous at $c$' implies the statement `$f$ is differentiable at~$c$'.
    Otherwise stated: A necessary condition for $f$ to be differentiable at $c$ is that it be continuous at~$c$,
    but this `continuity' condition is not sufficient to guarantee that $f$ is differentiable at~$c$.

\V

        \underline{Proof} (a) Notice that if $x{\in}I$ and $x \,\,{\neq}\,\, c$, then
        \begin{displaymath}
        f(x)-f(c) \,=\, \left(\frac{f(x)-f(c)}{x-c}\right){\cdot}(x-c)
        \end{displaymath}
    Since, by hypothesis, $f$ is differentiable at $c$, the first factor of the product on the right side of this equation approaches a finite limit.
    Let us denote this limiting value by $L$. (Of course we could denote it by $f'(c)$, but the issue here is that the limit exists and is finite.)
    Note also that the second factor on the right, namely $x-c$, also approaches the limiting value $0$.
    It then follows from the Product Rule for Limits that limit of the right side, as $x$ approaches $c$, exists and equals $L{\cdot}0 \,=\, 0$.
    Thus, the same is true for the left side; that is, $\lim_{x \,{\rightarrow}\, c} (f(x)-f(c)) \,=\, 0$, so that $f$ is continuous at $c$, as claimed.

\V

        (b) Examples (2) and (3) below involve functions which are continuous at a point but fail to be differentiable at that point. \Q

\V


            \subsection{\small{\bf Examples}}
            \label{ExampE20.40}

\V

        \hspace*{\parindent}(1) Let $f:{\RR} \,{\rightarrow}\, {\RR}$ be the function given by the rule
        \begin{displaymath}
        f(x) \,=\,  \left\{
        \begin{array}{ll}
        x^{2} & \mbox{if $x$ is rational} \\
         0    & \mbox{if $x$ is irrational}
        \end{array}
        \right.
        \end{displaymath}
    Let $c \,=\, 0$. One computes that if $x \,\,{\neq}\,\, 0$ then
        \begin{displaymath}
        \frac{f(x)-f(c)}{x-c} \,=\, \frac{f(x)}{x} \,=\, 
        \left\{
        \begin{array}{ll}
         x & \mbox{if $x$ is rational} \\
         0 & \mbox{if $x$ is irrational}
        \end{array}
                    \right.
        \end{displaymath}

        It is clear that $f$ is not continuous at $c$ when $c \,\,{\neq}\,\, 0$,
    so by Theorem~\Ref{ThmE20.30} the function $f$ is {\em not} differentiable at any nonzero~$c$.

    In contrast, it is easy to see that $f$ {\em is} continuous at $c \,=\, 0$. As for differentiability there, it is clear that
    ${\displaystyle -|x|\,\,{\leq}\,\,\frac{f(x)}{x}\,\,{\leq}\,\,|x|}$ for all $x \,\,{\neq}\,\, 0$, 
    and thus, by Part~(d) of Theorem~\Ref{ThmC90.50} (the Squeeze Property for Limits on Intervals),
    it follows that ${\displaystyle \lim_{x \,{\rightarrow}\, 0} \frac{f(x)}{x} \,=\, 0}$.
    That is, $f$ is differentiable at $c \,=\, 0$, and $f'(0) \,=\, 0$. In particular, this function is differentiable at exactly one point.

\V

        (2) Let $g:{\RR} \,{\rightarrow}\, {\RR}$ be the function given by the rule
        \begin{displaymath}
        g(x) \,=\,
        \left\{
        \begin{array}{ll}
        x & \mbox{if $x$ is rational} \\
        0 & \mbox{if $x$ is irrational}
        \end{array}
        \right.
        \end{displaymath}
    By the argument used in Example~(1) above, it is easy to show that $g$ is continuous at exactly one number $c$, namely at $c \,=\, 0$;
    in particular, the only point at which it is even possible for $g$ to be differentiable is $c \,=\, 0$.
    Note, however, that if $x \,\,{\neq}\,\, 0$ then one has
        \begin{displaymath}
        \frac{g(x)-g(0)}{x-0} \,=\, \frac{g(x)}{x} \,=\, 
        \left\{
        \begin{array}{ll}
         1 & \mbox{if $x$ is rational} \\
         0    & \mbox{if $x$ is irrational}
        \end{array}
        \right.
        \end{displaymath}
    This expression does not approach a limit as $x$ approaches $0$, so $g$ is not differentiable at the one point where there was a chance of this happening.

\V

        (3) Let $h:{\RR} \,{\rightarrow}\, {\RR}$ be the Absolute-Value Function; that is, $h(x) \,=\, |x|$ for every $x$ in~${\RR}$.
    Set $c \,=\, 0$ and note that if $x \,\,{\neq}\,\, 0$, then
        \begin{displaymath}
        \frac{h(x) - h(c)}{x - c} \,=\, \frac{|x-0|}{x-0} \,=\, \frac{|x|}{x} 
    \,=\, 
        \left\{
        \begin{array}{rl}
        -1 & \mbox{if $x\,<\,0$} \\
        +1 & \mbox{if $x\,>\,0$}
        \end{array}
        \right.
        \end{displaymath}
    It is clear that the left-hand limit of the expression $|x|/x$ equals~$-1$ while the right-hand limit of the same expression equals~$+1$.
    That is, $h'_{-}(0)$ and $h'_{+}(0)$ exist, but are not equal. It follows from Theorem~\Ref{ThmE20.25A} that $h$ is not differentiable at~$0$.



% EXERCISE Continuous piecewise-linear functions which are not linear also provide examples continnuity not being sufficient.

\VV

        {\bf Remark} There exist functions $h:{\RR} \,{\rightarrow}\, {\RR}$ with the property that $h$ is continuous at {\em every} point of ${\RR}$
    yet differentiable at {\em no} point of ${\RR}$; it is perhaps one of these functions which induced the `horror'
    expressed by the great mathematician Hermite in Chapter Quote~(2) at the start of this chapter. %% GIVE REFERENCE? PROOF?

\VV

        The preceding examples emphasized the idea of a function being differentiable -- or not differentiable -- at a single point.
    Such examples play a small role in elementary calculus: the emphasis there is on functions which are differentiable at each point of an open interval.
    The following examples are of that type and should seem more familiar.

\V

            \subsection{\small{\bf Some Standard Examples}}
            \label{ExampE20.70}

\V

\hspace*{\parindent}(1) Let $f:{\RR} \,{\rightarrow}\, {\RR}$ be a linear function on~${\RR}$;
    that is, there are real numbers $A$ and $B$ such that $f(x) \,=\, A\,x + B$ for all $x$ in~${\RR}$.
    Then $f$ is differentiable on~${\RR}$, and $f'(x) \,=\, A$ for each $x$ in~${\RR}$. Indeed, for each $x$ and $c$ in ${\RR}$ with $x \,\,{\neq}\,\, c$ one has
        \begin{displaymath}
        \frac{f(x)-f(c)}{x-c} \,=\, \frac{(A\,x + B) - (A\,c + B)}{x-c} \,=\, \frac{A\,(x-c)}{x-c} \,=\, A, \mbox{ hence }
        \lim_{x \,{\rightarrow}\, c} \frac{f(x)-f(c)}{x-c} \,=\, A.
        \end{displaymath}
    Two special cases have their own terminology:

\VA

        \h (i)\,If $A \,=\, 0$, then $f$ is the constant function of value $B$ on~${\RR}$.

       \h (ii) If $A \,=\, 1$ and $B \,=\, 0$, then $f$ is the identity function on~${\RR}$.

\V

        {\bf Remark}\, In light of Remark~\Ref{RemrkE20.20}~(2) above, it follows that if $g:I \,{\rightarrow}\, {\RR}$
    is the restriction to an open interval $I$ of a linear function $f:{\RR} \,{\rightarrow}\, {\RR}$, where $f(x) \,=\, A\,x + B$ for all $x$ in~${\RR}$ as above,
    then $g$ is differentiable on $I$ and $g'(c) \,=\, A$ for all $x$ in~$I$. For example, it follows easily from this that the absolute-value function $h$, given by
        \begin{displaymath}
        h(x) \,=\, |x| \,=\,
        \left\{
        \begin{array}{rl}
        -1 & \mbox{if $x\,<\,0$}   \\
         0 & \mbox{if $x \,=\, 0$} \\
        +1 & \mbox{if $x\,>\,0$}
        \end{array}
        \right.,
        \end{displaymath}
 is differentiable at all $c \,\,{\neq}\,\, 0$. Indeed, on the open interval $(-{\infty},0)$ $h$ is the restriction to $(-{\infty},0)$ of the linear function with $A \,=\, -1$ and $B \,=\, 0$,
    so $h'(c) \,=\, -1$ if $c\,<\,0$. A similar argument shows that $h'(c) \,=\, +1$ if $c\,>\,0$.

        Of course it has already been shown that the function $h$ is not differentiable at $c \,=\, 0$; see Example~\Ref{ExampE20.40}~(3).

\V

        (2) Suppose the function $f:{\RR} \,{\rightarrow}\, {\RR}$ is given by $f(x) \,=\, A\,x^{2}$ for all $x$ in~${\RR}$, where $A$ is a fixed real number.
    Then for each $x$ and $c$ in~${\RR}$ with $x \,\,{\neq}\,\, c$, one has
        \begin{displaymath}
        \frac{f(x) - f(c)}{x - c} \,=\, \frac{(A\,x^{2} - A\,c^{2})}{x - c}
     \,=\, A\,\left(\frac{x^{2}-c^{2}}{x-c}\right) \,=\, 
        A\,\left(\frac{(x-c)\,(x+c)}{x-c}\right) \,=\, A\,(x+c).
        \end{displaymath}
    It follows from the usual limit laws that $\lim_{x \,{\rightarrow}\, c} A\,(x+c) \,=\, A\,(c+c) \,=\, 2\,A\,c$, and thus
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\, c} \frac{f(x) - f(c)}{x-c}  \,=\,2\,A\,c.
        \end{displaymath}
    That is, $f$ is differentiable at~$c$, and $f'(c) \,=\, 2\,A\,c$, for each $c$ in~${\RR}$.

\V

        (3) More generally, suppose that $k$ is any natural number. By using the well-known factorization
        \begin{displaymath}
        x^{k} - c^{k} \,=\, (x-c)\,(x^{k-1} + x^{k-2}\,c + \,{\ldots}\, + x\,c^{k-2} + c^{k-1}),
        \end{displaymath}
    it is easy to show that if $f(x) \,=\, x^{k}$ for all $x$, then $f'(c) \,=\, k\,c^{k-1}$ for every number~$c$.

\V

        (4) Let $f:{\RR}{\setminus}\{0\} \,{\rightarrow}\, {\RR}$ be the `reciprocal function';
    that is, $f(x) \,=\, 1/x \,=\, x^{-1}$ for all $x \,\,{\neq}\,\, 0$.
    One then computes that
        \begin{displaymath}
        f(x)-f(c) \,=\, \frac{1}{x} - \frac{1}{c} \,=\, \frac{c-x}{x\,c} \mbox{ for all $x, c \,\,{\neq}\,\, 0$}.
        \end{displaymath}
    Thus if one also has $x \,\,{\neq}\,\, c$, it follows that
        \begin{displaymath}
        \frac{f(x)-f(c)}{x-c} \,=\, \frac{c-x}{(x-c)\,x\,c} \,=\,
     \,=\, -\frac{(x-c)}{(x-c)\,x\,c} \,=\, -\frac{1}{x\,c}.
        \end{displaymath}
    It follows from the basic laws for limits that ${\displaystyle \lim_{x \,{\rightarrow}\, c} -\frac{1}{x\,c} \,=\, -\frac{1}{c^{2}}}$, so that
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\, c} \frac{f(x)-f(c)}{x-c} \,=\, 
    \lim_{x \,{\rightarrow}\, c} -\frac{1}{x\,c} \,=\, -\frac{1}{c^{2}} \mbox{ for $c \,\,{\neq}\,\, 0$}.
        \end{displaymath}
    That is, if $c \,\,{\neq}\,\, 0$ then $f$ is differentiable at $c$ and $f'(c) \,=\, -1/c^{2}$.

\V

        (5) Suppose that $f(x) \,=\, \sqrt{x} \,=\, x^{1/2}$ for all $x\,\,{\geq}\,\,0$.
    Using a familiar trick from algebra, the so-called `Multiply-and-Divide Trick', one then computes that
        \begin{displaymath}
        f(x)-f(c) \,=\, \sqrt{x}-\sqrt{c} \,=\,
   \left(\frac{(\sqrt{x}-\sqrt{c})(\sqrt{x}+\sqrt{c})}{\sqrt{x}+\sqrt{c}}\right)
     \,=\, \left(\frac{(\sqrt{x})^{2}-\sqrt{x}\sqrt{c}+\sqrt{x}\sqrt{c}-(\sqrt{c})^{2}}{\sqrt{x}+\sqrt{c}}\right)
     \,=\, \left(\frac{x-c}{\sqrt{x}+\sqrt{c}}\right)
        \end{displaymath}
    for all $x,c\,>\,0$. (Note that the denominator $\sqrt{x}+\sqrt{c}$ is positive for all such $x$ and $c$, so there is no `division-by-zero' issue.)
    If, in addition $x \,\,{\neq}\,\, c$, then one has
        \begin{displaymath}
        \frac{f(x)-f(c)}{x-c} \,=\, \frac{x-c}{(x-c)(\sqrt{x}+\sqrt{c})}
     \,=\, \frac{1}{\sqrt{x}+\sqrt{c}}.
        \end{displaymath}
    Since $\lim_{x \,{\rightarrow}\, c} \sqrt{x} \,=\, \sqrt{c}$ (see Example~\Ref{ExampD30.110}), it follows by the usual limit laws that
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\, c} \frac{1}{\sqrt{x} + \sqrt{c}} \,=\, 
    \frac{1}{2\sqrt{c}}.
        \end{displaymath}
    Thus, one finally obtains
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\, c} \frac{f(x)-f(c)}{x-c} \,=\, \lim_{x \,{\rightarrow}\, c} \frac{1}{\sqrt{x}+\sqrt{c}} \,=\, \frac{1}{2\sqrt{c}}.
        \end{displaymath}
    In other words, if $c\,>\,0$ then $f'(c)$ is defined, and
        \begin{displaymath}
        f'(c) \,=\, \frac{1}{2\,\sqrt{c}} \,=\, \frac{1}{2}\,c^{-1/2}.
        \end{displaymath}

        In the case $c \,=\, 0$ it makes sense to consider the right-hand derivative.
    Since one must have $x\,>\,0$ in that case, the corresponding difference quotion takes the form
        \begin{displaymath}
        \frac{f(x)-f(0)}{x-0} \,=\, \frac{1}{\sqrt{x}}.
        \end{displaymath}
    This expression approaches $+{\infty}$ as $x$ decreases to~$0$, so the corresponding one-sided derivative fails to exist at $c \,=\, 0$,
    since by definition derivatives, whether one-sided or two-sided, are finite.


\V

            \subsection{\small{\bf Remarks}}
            \label{RemrkE20.35}

\V

        There are several commonly used notations for the derivative that one should know.

\V

\hspace*{\parindent}(1) Many texts use the following formula to define the derivative:
        \begin{displaymath}
        f'(x) \,=\, \lim_{h \,{\rightarrow}\, 0} \frac{f(x+h)-f(x)}{h}.
        \end{displaymath}
    One advantage of this formulation is that it works for those who insist that the input of both $f$ and $f'$ ought to be the same `variable', often $x$.
    Note that $h$ plays the role of $x-c$ which appears in Definition~\Ref{DefE20.20} above.
    In particular, with this notation there is no need to introduce the extra letter $c$ as was done in the preceding examples.
    Thus, instead of defining $f$ by, say, the formula $f(x) \,=\, x^{k}$ and thus $f'(c) \,=\, k\,c^{k-1}$ as in Example~(3) above,
    one could write the more familiar $f'(x) \,=\, k\,x^{k-1}$.

        Similarly, many texts, especially the older ones, use the notation ${\Delta}x$ in place of $h$.
    For those texts the definition becomes
        \begin{displaymath}
        f'(x) \,=\, \lim_{{\Delta}x \,{\rightarrow}\, 0} \frac{f(x+{\Delta}x)-f(x)}{{\Delta}x}.
        \end{displaymath}
    The Greek letter ${\Delta}$ (i.e., upper-case delta) has the sound (at least in Classical Greek) of the English `d'.
    It suggests the idea of `difference'; indeed, ${\Delta}x \,=\, (x+{\Delta}x) - x$. That is, ${\Delta}x$ is the change in the variable $x$ in going from $x$ to $x+{\Delta}x$.

        Those authors who insist on the traditional `variables' notation might write, say, $y \,=\, f(x)$ as the `equation' of the function~$f$,
    and refer to the expression $f(x+{\Delta}x)-f(x)$ as `the change in $y$ corresponding to the given change in $x$'. They would denote this change in the quantity $y$ by ${\Delta}y$, and then write
        \begin{displaymath}
        f'(x) \,=\, \lim_{{\Delta}x \,{\rightarrow}\, 0} \frac{{\Delta}y}{{\Delta}x},
        \end{displaymath}
    and refer to the quantity $f'(x)$ as `the derivative of $y$ with respect to $x$' at the given value of the variable~$x$.
    Such authors often write $y'$ in place of $f'(x)$. The `prime' notation here mentions the dependent variable $y$ explicitly, but leaves it understood from the context that the corresponding independent variable is $x$.

\V

        (2) The `prime' notation, $f'$, for the derivative of the function~$f$, is due to Lagrange.\IndB{derivative notations}{Lagrange's `prime' notation}
    There are several older notations which one needs to know since they also are still in common use:

\VA

        \h (a) {\bf Leibniz' `differential' notation} The most popular notation is the so-called `differential' notation of Leibniz.
    \IndB{derivative notations}{Leibniz `differential' notation} Using the classical `variables' description of functions,
    consider the function $y \,=\, f(x)$. Instead of comparing $f(x)$ with $f(x+{\Delta}x)$ for a small change ${\Delta}x$ in the input variable~$x$
    to obtain the corresponding small change ${\Delta}y \,=\, f(x+{\Delta}x)-f(x)$ in the output variable~$y$,
    Leibniz introduced the symbol $dx$ for an {\em infinitely small} -- but nonzero -- change in $x$, and $dy$ for the corresponding {\em infinitely small} change in~$y$.
    Note that in Leibniz' time an `infinitely small' quantity is to be smaller in magnitude than any positive real number, yet still can be nonzero.
    Leibniz thinks of $dx$ and $dy$ as the horizontal and vertical sides, respectively of an infinitely small right triangle
    whose hypotenuse connects the `neighboring' points $(x,y)$ and $(x+dx,y+dy)$ of the graph $y \,=\, f(x)$. 
    Leibniz then divides the (nonzero) $dx$ into $dy$ to get the slope of this hypotenuse; that is, the slope of $y \,=\, f(x)$ at the point~$(x,y)$.
    In terms of Lagrange's `prime' notation, $f'(x) \,=\, dy/dx$; equivalently, $dy \,=\, f'(x)\,dx$.

        NOTE: `Infinitely small quantities', or `infinitesimals', were used long before Leibniz and long after.
    One of the goals of the rigorization of analysis, as started in the nineteenth century, was to replace the use of `infinitesimals'
    by the theory of limits; that is, by the approach followed, for example, in {\ThisText}. More recently,
    rigorous treatments of `infinitesimals' have been developed under the heading of `Non-standard Analysis'; see, for example, [ABRAHAM~1996].

\VA

        \h (b) {\bf Newton's `dot' notation}\IndB{derivative notations}{Newton's `dot' notation}
    As a physicist Newton had a dynamic view of calculus: quantities were viewed as functions of the time~$t$,
    and calculus measured the time rate of change of such quantities. For example, if $u \,=\, f(t)$,
    then Newton would write $\stackrel{.}{u}$ instead of $du/dt$ or $f'(t)$. The `dot' notation
    for differentiation with respect to the time is still in common use in physics and engineering.

\VA

        \h (c) {\bf Euler's `D' notation}\IndB{derivative notations}{Euler's `D notation} This notation is both obvious and simple.
    More precisely, if $y \,=\, f(x)$, then one writes $Dy$ or $(Df)(x)$ or, more simply, $Df\,(x)$.


\VV

                \section{{\bf Derivatives of Higher Order}}
                \label{SectE20A}\IndB{ZZ Sections}{\Ref{SectE20A} Derivatives of Higher Order}
        
\VV



        In most cases of interest to calculus, the derivative $f'$ of a function $f$ is itself a differentiable function,
    and thus its derivative $(f')'$ can also be studied. Repeating this idea leads to the following.

\V

            \subsection{\small{\bf Definition} (Derivatives of Higher Order)}\IndB{derivatives}{higher-order derivatives}
            \label{DefE20.80}

\V

        Let $f:I \,{\rightarrow}\, {\RR}$ be a real-valued function which is differentiable at each point of an open interval $I$.

\V

        (1) The {\bf \IndAA{first derivative} of $f$}, denoted by $f^{(1)}$, is the function $f'$.
    One also calls $f^{(1)}$ the {\bf derivative of order $\Bfm{1}$} of~$f$.
    

\V

        (2) If $f^{(1)}$ is also differentiable at each point of $I$, then its derivative, $(f^{(1)})'$, is called the {\bf second derivative}, or {\bf derivative of order $2$}, of $f$; it is denoted by $f^{(2)}$.

\V

        (3) More generally, suppose that $n{\in}{\NN}$ is such that the derivative of order $n$, $f^{(n)}$, has already been defined on~$I$.
    If $f^{(n)}$ is also differentiable at each point of $I$, then its derivative, $(f^{(n)})':I \,{\rightarrow}\, {\RR}$,
    is called the {\bf derivative of order $n+1$}, or the {\bf $(n+1)$-st derivative} of $f$. It is denoted~$f^{(n+1)}$.

\V

        (4) If, for some natural number $n$, $f^{(n)}$ is defined at each point of $I$,
    then one says that $f$ is {\bf $\Bfm{n}$-times differentiable on $\Bfm{I}$}.

\V

        (5) For completeness, one sets $f^{(0)} \,=\, f$; that is, $f^{(0)}$ is the function which comes from $f$ by not differentiating $f$ at all.
    (Of course this notation makes sense even without the assumption that $f$ is differentiable on~$I$, but there is little need for it then.)
    Also, to agree with standard notation in elementary calculus, one lets $f'$, $f''$, $f'''$,\,{\ldots}\, be alternate notations for $f^{(1)}$, $f^{(2)}$, $f^{(3)}$,\,{\ldots}\,,
    although one rarely uses the `prime' notation for, say, $n\,>\,4$.

\V

            \subsection{\small{\bf Example}}
            \label{ExampE20.90}
\V

\hspace*{\parindent}Let $f:{\RR} \,{\rightarrow}\, {\RR}$ be given by the rule $f(x) \,=\, x^{k}$ for all $x$ in ${\RR}$,
    where $k$ is a fixed natural number. Then by Example~\Ref{ExampE20.70} above, one  has $f'(x) \,=\, k\,x^{k-1}$ for all $x$.
    By repeated use of the same example, for all $x$ one has $f''(x) \,=\, k\,(k-1)\,x^{k-2}$, $f'''(x) \,=\, k\,(k-1)\,(k-2)\,x^{k-3}$, and so on.
    Eventually one obtains $f^{(k)}(x) \,=\, k\,(k-1)\,(k-2), \,{\ldots}\, {\cdot}2{\cdot}1 \,=\, k!$ for all $x$,
    so that $f^{(k)}$ is a constant function, and thus $f^{(n)}(x) \,=\, 0$ for all $x$ if $n{\in}{\NN}$ satisfies $n\,\,{\geq}\,\,k+1$.

\VV

            \subsection{\small{\bf Definition} (\IndAA{$C^{k}$ functions} \IndA{smooth ({\em i.e., $C^{{\infty}})$} functions})}
            \label{DefE20.95}

\V

        Let $f:I \,{\rightarrow}\, {\RR}$ be defined at each point of an open interval $I$, and let $n$ be a nonnegative integer.

\V

        (1) One says that $f$ is {\bf of class} $\Bfm{C^{n}}$ on $I$, or that $f$ is $\Bfm{C^{n}}$ on $I$,
    provided that all the functions $f^{(0)}, f^{(1)},\,{\ldots}\,f^{(n)}$ exist, and are continuous, at each point of $I$.

        If $f$ is of class $C^{k}$, but not of class $C^{k+1}$, on $I$, then one says that $f$ is {\bf strictly} $\Bfm{C^{k}}$ on $I$.
    \IndB{$C^{k}$ functions}{strictly $C^{k}$}

\V

        (2) If the function $f$ is of class $C^{k}$ on $I$ for each $k$ in ${\NN}$,
    then one says that $f$ is {\bf of class} $\Bfm{C^{{\infty}}}$, or that $f$ is {\bf smooth}, on $I$.

\VV

        {\bf Remark} Most of the standard functions which one uses in elementary calculus are smooth.
    In particular, one typically does not encounter {\em strictly} $C^{k}$ functions there with $k$ finite. The next example fills that gap.

\VV

            \subsection{\small{\bf Example}}
            \label{ExampE20.85}

\V

        Let $F:{\RR} \,{\rightarrow}\, {\RR}$ be the function given by the following rule:
        \begin{displaymath}
        F(x) \,=\, \left\{
        \begin{array}{rl}
        -x^{2} & \mbox{if $x\,\,{\leq}\,\,0$} \\
        x^{2} & \mbox{if $x\,>\,0$}
        \end{array}
                            \right.
        \end{displaymath}
    From Example~\Ref{ExampE20.70}~(2) above it is clear that $F$ is $C^{{\infty}}$ on the open intervals $(-{\infty},0)$ and~$(0,+{\infty})$.
    Indeed, one has $F'(x) \,=\, -2\,x$ if $x\,<\,0$, while $F'(x) \,=\, 2\,x$ if $x\,>\,0$; that is, $F'(x) \,=\, 2\,|x|$ for all $x \,\,{\neq}\,\, 0$.

        The situation at $c \,=\, 0$ is more complicated. Indeed, if $x \,\,{\neq}\,\, 0$ one computes that
        \begin{displaymath}
        \frac{F(x)-F(0)}{x-0} \,=\, \left\{
        \begin{array}{cl}
        {\displaystyle \frac{-x^{2}}{x} \,=\, -x} & \mbox{if $x\,<\,0$} \\
                                                      &                     \\
        {\displaystyle \frac{x^{2}}{x} \,=\, x}   & \mbox{if $x\,>\,0$}
        \end{array}
                                                \right.
        \end{displaymath}
    As $x$ approaches $0$ from either side the corrresponding fractions $-x$ and $x$ both approach~$0$.
    Thus, $F$ is differentiable at $c \,=\, 0$, and $F'(0) \,=\, 0$, so that $F'(0) \,=\, 2\,|0|$. Combining these results,
    one sees that $F$ is differentiable on~${\RR}$, and $F'$ is twice the absolute-value function.
    The latter function is continuous on ${\RR}$, so $F$ is $C^{1}$ on~${\RR}$. However, $F'$ is not differentiable at $c \,=\, 0$,
    so $F$ is not $C^{2}$ on~${\RR}$; indeed, it is not even twice-differentiable on~${\RR}$.

\VV

            \subsection{\small{\bf Remarks}}
            \label{RemrkE20.87}

\V

 (1)\,In light of the preceding example, it is natural to ask whether there are $C^{k}$ functions on an interval $I$ which are $(k+1)$-times differentiable on $I$ but not $C^{k+1}$ on~$I$.
    The answer is that such functions do exist. However, the construction of such examples is nontrivial, and is held off until later.

\V

        (2)\,The alternate notations for derivatives mentioned in Remark~\Ref{RemrkE20.35} above
    have extensions to derivatives of higher order.\IndB{derivative notations}{derivatives of higher order}
    For example, second derivatives are written as follows: if $y \,=\, f(x)$, then
        \begin{displaymath}
        f^{(2)}(x) \,=\, f''(x) \,=\, y'' \,=\, \frac{d^{2}y}{dx^{2}} \,=\, \frac{d^{2}f(x)}{dx^{2}} \,=\,  \stackrel{..}{y} \,=\, D^{2}y \,=\, D^{2}f(x).
        \end{displaymath}
    Similarly, $k$-th order derivatives are written
        \begin{displaymath}
        f^{(k)}(x) \,=\, \frac{d^{k}y}{dx^{k}} \,=\, \frac{d^{k}f(x)}{dx^{k}} \,=\,  D^{k}y \,=\, D^{k}f(x);
        \end{displaymath}
    the corresponding `prime' and `dot' notations are omitted because when $k\,\,{\geq}\,\,4$ they become difficult to read.

        The notation `$d^{k}y/dx^{k}$' sometimes confuses students. For example, if $k \,=\, 2$, then its genesis is the formula
        \begin{displaymath}
        f''(x) \,=\, \left(f'\right)'(x) \,=\, \frac{d\,}{dx}\left(\frac{dy}{dx}\right).
        \end{displaymath}
    The right side has two copies of $d$ and one of $y$ in the numerator, and two copies of $dx$ in the denominator,
    which explains the abbreviation $d^{2}y/dx^{2}$. Note, in particular, that $dx^{2}$ is itself a shorthand for $(dx)^{2}$, not $d(x^{2})$.
    Likewise, the notation $D^{2}f$ is shorthand for $D(Df)$, not $(Df)^{2}$, so there are two copies of $D$ but only one of~$f$.


\VV

\StartSkip{ % First E45.30

        In elementary calculus the first important problem studied is that of `differentiation':
    given a function~$f:I \,{\rightarrow}\, {\RR}$ on an open interval~$I$, determine its derivative~$f'$. 
    In the next section we recall the standard computational rules for computing derivatives.

        The second important problem studied in calculus is the reverse: given a function $f:I \,{\rightarrow}\, {\RR}$ on an open interval~$I$,
    determine a function $F:I \,{\rightarrow}\, {\RR}$ such that $f$ is the derivative $F'$ of~$F$.
    Solving this type of problem is, in a sense, much more important in applications than the original `differentiation problem'.
    Unfortunately, it is generally much harder to solve than the differentiation problem.
    In any event, it is appropriate to introduce at this place the standard terminology associated with this problem.

\V



%%%             \subsection{\small{\bf Definition} (Antiderivatives)}
            \label{DefE45.30}\IndB{derivatives}{antiderivatives}

        Suppose that $f:I \,{\rightarrow}\, {\RR}$ is function defined on an open interval $I$ in ${\RR}$.

\V

        (1) A function $F:I \,{\rightarrow}\, {\RR}$ is said to be {\bf an antiderivative} of $f$ on $I$ provided $F'(x) \,=\, f(x)$ for all $x$ in $I$.

\V

        (2) More generally, let $k$ be a natural number. A function $G:I \,{\rightarrow}\, {\RR}$ is said to be a
    $\Bfm{k}$-th {\bf order antiderivative} of $f$ on $I$\IndBD{antiderivatives}{$k$-th order antiderivative}
    provided $G^{(k)}(x) \,=\, f(x)$ for all $x$ in $I$. (It is proved later that if $f$ has an antiderivative on~$I$,
    then for each $k$ in ${\NN}$ the function $f$ has a $k$-th order antiderivative on~$I$.)

\V

        (3) The process of calculating an antiderivative of a given function $f$ is called
    {\bf antidifferentiation}\IndBD{antiderivatives}{antidifferentiation} of~$f$.


\VV

             \subsection{\small{\bf Examples}}
            \label{ExampE45.40}

\V

        (1) Consider the following short `Table of Derivatives' which were among those obtained above.

\V

        \begin{displaymath}
        \begin{array}{||c|c|c||}
\hline
        \mbox{Function} & \mbox{Derivative} & \mbox{Restrictions} \\
\hline

            C  &     0       & C \,=\, \mbox{constant function} \\
               &             &             \\
        A\,x  &  A & A \,=\, \mbox{constant}\\
               &             &             \\
        A\,x^{2}  &  2\,A\,x & A \,=\, \mbox{constant}\\
               &             &             \\
        {\displaystyle \frac{A}{x}} & {\displaystyle -\frac{A}{x^{2}}} & x \,\,{\neq}\,\, 0;\, A \,=\, \mbox{constant} \\
               &             &             \\
        A\,\sqrt{x} & {\displaystyle \frac{A}{2\,\sqrt{x}}} & x\,>\,0;\,A \,=\, \mbox{constant}  \\
                       &             &             \\
\hline
        \end{array}
        \end{displaymath}

        Tables such as this are normally read `left-to-right': in each row, the first column says which function is to be differentiated,
    while the second column gives the result of that process. In contrast, by reading this table `right-to-left',
    one obtains a `table of antiderivatives'. For instance, the third row implies that one antiderivative of the function $3\,x$ is $3\,x^{2}/2$;
    simply set $A \,=\, 3/2$. Of course there are infinitely many other antiderivatives; for example, $3\,x^{2} + 7$ is one such.

        Likewise, the fourth row, read right-to-left,
    implies that one antiderivative of the function $1/x^{2}$ is the function $-1/x$; simply choose $A \,=\, -1$.
    Of course, this is not the only such antiderivative. For instance, let
        \begin{displaymath}
        F(x) \,=\, \left\{
        \begin{array}{cl}
        {\displaystyle 3-\frac{1}{x}}  & \mbox{if $x\,<\,0$} \\
                                       &                     \\
        {\displaystyle 17-\frac{1}{x}} & \mbox{if $x\,>\,0$}
        \end{array}
                                \right.
        \end{displaymath}
    It is easy to verify that $F'(x) \,=\, 1/x^{2}$ for every $x \,\,{\neq}\,\, 0$.
    Note that the function $F$ is {\em not} obtained from the initial antiderivative $-1/x$ by adding a single constant throghout its domain.


\V

        (2) (Build-It-Yourself Example) Let $F$ be any standard function from calculus. Differentiate $F$, and let $f$ be the resulting function.
    Then $F$ is an antiderivative of $f$, as is $F+C$ for any constant~$C$.

\V

        (3) Let $f:{\RR} \,{\rightarrow}\, {\RR}$ be given by the rule
        \begin{displaymath}
        f(x) \,=\, \frac{1}{(1+u^{2})^{3/2}} \mbox{ for all $x$ in ${\RR}$}
        \end{displaymath}


        \underline{Claim} The function $F:{\RR} \,{\rightarrow}\, {\RR}$ given by the rule
        \begin{displaymath}
        F(x) \,=\, \frac{x}{\sqrt{1+x^{2}}} \mbox{ for all $x$ in~${\RR}$},
        \end{displaymath}
    is an antiderivative of~$f$.

        To {\em prove} that this claim is correct is not hard: differentiate the function $F$, using the rules of differentiation, and simpify algebraically.
    The harder question to answer is this: Where did the formula for $F$ come from?

\V

        (4) Let $f(x) \,=\, |x|$ for all $x$ in ${\RR}$. Then $f(x) \,=\, x$ if $x\,\,{\geq}\,\,0$, and $f(x) \,=\, -x$ if $x\,<\,0$.
    On the open interval $(0,+{\infty})$ the function $f$ has many antiderivatives;
    namely, any function on $(0,+{\infty})$ of the form $G(x) \,=\, {\displaystyle \frac{x^{2}}{2}+C_{1}}$, where $C_{1}$ is constant.
    Likewise $f$ has infinitely many antiderivatives on the interval $(-{\infty},0)$,
    namely functions of the form $H(x) \,=\, {\displaystyle -\frac{x^{2}}{2}+C_{2}}$.
    To get an antiderivative defined on all of ${\RR}$, choose the constants $C_{1}$ and $C_{2}$ so that $\lim_{x{\nearrow}0} H(x) \,=\, \lim_{x{\searrow}0} G(x)$.
    This simply requires $C_{1} \,=\, C_{2}$, so let us make the simplest choice, namely $C_{1} \,=\, C_{2} \,=\, 0$.
    Then define $F:{\RR} \,{\rightarrow}\, {\RR}$ by the rule
        \begin{displaymath}
        F(x) \,=\, \left\{
        \begin{array}{ccrl}
        H(x) & \,=\, & -x^{2}/2 & \mbox{if $x\,<\,0$} \\
          0  & \,=\, &          & \mbox{if $x\,=\,0$} \\
        G(x) & \,=\, &  x^{2}/2 & \mbox{if $x\,>\,0$}
        \end{array}
                        \right.
        \end{displaymath}
    This is the same function that appears in Example~\Ref{ExampE20.85} above, where it is shown that $F'$ is the absolute-value function.

\VV



             \subsection{\small{\bf Remarks}}
            \label{RemrkE45.70}
\V

\hspace*{\parindent}(1) The use of the word `antidifferentiation', to indicate the process opposite to the process of `differentiation', seems reasonable;
    likewise for naming the result of that process an `antiderivative'. Indeed, the `antidifferentiation/antiderivative' terminology
    seems to be universal in the modern calculus texts.

    However, for most of the centuries since calculus was first developed the words used instead of `antidifferentiation' and `antiderivative' were 
    `integration' and `integral'; some authors used the word `primitive' instead `integral', and sometimes the word `indefinite' is used before `integral'.

        Why is it important for you, the modern reader, to know this? Because the `integration/integral' terminology for these concepts is still widely used;
    thus if you encounter it, you need to know what it means. Furthermore, although the use of the `antiderivative' {\em terminology} ,
    the {\em notation} used in connection with this concept remains stuck in the eighteenth century.
    More precisely, the way one writes the statement `$F+C$ is the general antiderivative of $f$' in mathematical symbols is this:
        \begin{displaymath}
        F(x)+C \,=\, \int\, f(x)\,dx
        \end{displaymath}
    In the expression ${\displaystyle \int\, f(x)\,dx}$ the symbol ${\displaystyle \int}$ is called the {\bf integral sign},
    the function $f(x)$ is called the {\bf integrand}, and the `arbitrary constant' $C$ is called the {\bf constant of integration}.
    Even in those elementary-calculus textbooks which consistently use the `antiderivative' terminology,
     the section in which one learns how to compute antiderivatives is normally titled something like `Techniques of Integration',
    not `Techniques of Antidifferentiation'.

        Most students in a calculus course find the sections on `Techniques of Integration' to be -- by far -- the most difficult part of the course.
    Fortunately, in analysis textbooks the custom is to spend little time on the techniques of computing antiderivatives,
    and more on the theoretical aspects. We follow that custom in {\ThisText}.

\V

        (2) There is good reason to assert that the fundamental goal of elementary single-variable calculus is this:

\VA

    \h {\em Given a function $f$, to find its derivative;
    and given a function $g$, to find its (general) antiderivative.}

\VA

\noindent Indeed, this is the significance of Chapter Quote~{1} at the start of this chapter, namely 

\VA

        \h `{\em 6accd{\ae}13eff7i3l9n4o4qrr4s8t12vx}'.

\VA

\noindent Isaac Newton, one of the cofounders of calculus in the seventeenth century, used these letters as an anagram to disguise the following statement:

\begin{quotation}
{\footnotesize
      {\em Data {\ae}quatione quotcunque fluentes quantitates involvente, fluxiones invenire: et vice vers\^{a}}.
}%EndFootNoteSize
\end{quotation}

    That is,
\begin{quotation}
{\footnotesize 
    \em Given an equation involving any number of fluent quantities, to find the fluxions: and vice versa.
}%EndFootNoteSize
\end{quotation}

\noindent In Newton's terminology, a `fluxion' is the rate of change (with respect to time) of a quantity; that is, a derivative;
    and a `fluent' is the quantity whose change produces a give fluxion; that is, an antiderivative.
    Thus, according to Newton the purpose of calculus is to take derivatives and find antiderivatives.

}%\EndSkip First E45.30
%----------------------



                \section{{\bf Computational Rules for Derivatives}}
                \label{SectE30}\IndB{ZZ Sections}{\Ref{SectE30} Computational Rules}
        
\VV

        Examples given above illustrate how to compute the derivative directly from its definition in some simple cases.
    If one needed the direct use of that definition in general, then calculus would be a much more difficult, and less useful, tool.
    The next several theorems summarize rules for differentiating functions which allow one to use such simple cases to compute many other derivatives without referring to that definition.
        Throughout these theorems, $I$ is an open interval in ${\RR}$, and $c$ is a point of $I$.

\V

            \subsection{\small{\bf Theorem}}
            \label{ThmE30.20}

\V

\hspace*{\parindent}(a) ({\bf Constant-Factor Rule for Differentiation})\IndBD{differentiation rules}{constant-factor rule for differentiation}
Suppose that $f:I \,{\rightarrow}\, {\RR}$ is differentiable at $c$.
    Let $h \,=\, k{\cdot}f$ for some number $k$; that is, $h(x) \,=\, k{\cdot}(f(x))$ for all $x$ in $I$. Then $h$ is also differentiable at $c$, and $h'(c) \,=\, k{\cdot}f'(c)$.

\V

        (b) ({\bf Sum and Difference Rules for Differentiation})
    \IndBD{differentiation rules}{sum and difference rules for differentiation}
Suppose that $f,g:I \,{\rightarrow}\, {\RR}$ are differentiable at $c$.
    Let $h_{1} \,=\, f+g$; that is, $h_{1}(x) \,=\, f(x)+g(x)$ for all $x$ in $I$. Then $h_{1}$ is also differentiable at $c$, and $h_{1}'(c) \,=\, f'(c)+g'(c)$.
    Likewise, if $h_{2} \,=\, f-g$, then $h_{2}$ is differentiable at $c$, and $h_{2}'(c) \,=\, f'(c)-g'(c)$.

\V

            %\subsection{\small{\bf Corollary} (Linear-Combination Rule for Differentiation)}\IndB{differentiation rules}{linear combination rule for differentiation}
            \label{CorE30.30}%

\V

        (c) ({\bf Linear-Combination Rule for Differentiation})\IndBD{differentiation rules}{linear-combination rule for differentiation}
    Suppose that $f_{1}, f_{2},\,{\ldots}\,f_{m}:I \,{\rightarrow}\, {\RR}$ are all differentiable at $c$. Let $h:I \,{\rightarrow}\, {\RR}$ be a {\bf linear combination} of these functions;
    that is, there exist constants $k_{1}$, $k_{2}$,\,{\ldots}\,$k_{m}$ such that
        \begin{displaymath}
        h(x) \,=\, k_{1}{\cdot}(f_{1}(x)) + k_{2}{\cdot}(f_{2}(x)) + \,{\ldots}\,+ k_{m}{\cdot}(f_{m}(x)) \mbox{ for all $x$ in $I$}.
        \end{displaymath}
    Then $h$ is differentiable at $c$, and
        %\begin{displaymath}
        $h'(c) \,=\, k_{1}{\cdot}f'_{1}(c) + k_{2}{\cdot}f'_{2}(c) + \,{\ldots}\,+ k_{m}{\cdot}f'_{m}(c)$.
        %\end{displaymath}

\V

        The simple proofs are left as exercises.  \Q %% EXERCISES

\V

        {\bf Remark}\,The `Linear Combination' terminology comes from the mathematical subject of `Linear Algebra'.
    In {\ThisText} one does not need to be familiar with that subject.

\VV

            \subsection{\small{\bf Theorem}}
            \label{ThmE30.40}

\V

        Suppose that $f,g:I \,{\rightarrow}\, {\RR}$ are functions, defined on an open interval $I$, which are differentiable at a point $c$ of $I$.

\V

        (a) ({\bf Product Rule for Differentiation})\IndB{differentiation rules}{product rule for differentiation}
    Let $P \,=\, f{\cdot}g$ be the product of $f$ and $g$; that is, $P(x) \,=\, f(x){\cdot}g(x)$ for all $x$ in $I$. Then $P$ is differentiable at $c$, and
        \begin{displaymath}
        P'(c) \,=\, f'(c)g(c) + f(c)g'(c).
        \end{displaymath}

\V


        (b) ({\bf Quotient Rule for Differentiation})\IndB{differentiation rules}{quotient rule for differentiation} Suppose, also, that for all $x$ in $I$ one has $g(x) \,\,{\neq}\,\, 0$.
    Let $Q \,=\, f/g$ be the quotient of $f$ by $g$; that is, $Q(x) \,=\, f(x)/g(x)$ for all $x$ in $I$. Then $Q$ is differentiable at $c$, and
        \begin{displaymath}
        Q'(c) \,=\, \frac{f'(c)g(c)-f(c)g'(c)}{(g(c))^{2}}.
        \end{displaymath}

        {\bf Special Case in which $f \,=\, 1$: The Reciprocal Rule for Differentiation}\IndB{differentiation rules}{reciprocal rule for differentiation}
    If $g$ is as in (b) above, then
        \begin{displaymath}
        \left(\frac{1}{g}\right)'(c) \,=\, -\frac{g'(c)}{(g(c))^{2}}.
        \end{displaymath}

\V

        \underline{Proof}

\V

        (a) By a clever use of the `Add-and-Subtract Trick' one can write
        \begin{displaymath}
        \frac{P(x)-P(c)}{x-c} \,=\, \frac{f(x)g(x)-f(c)g(c)}{x-c} \,=\, \frac{f(x)g(x) - f(c)g(x) + f(c)g(x) - f(c)g(c)}{x-c}
    \,=\,
        \end{displaymath}
        \begin{displaymath}
        \left(\frac{f(x)-f(c)}{x-c}\right)g(x) + f(c)\left(\frac{g(x)-g(c)}{x-c}\right) \h ({\ast})
        \end{displaymath}
    It follows, from the differentiability hypotheses on $f$ and $g$ at~$c$, combined with the theorem that differentiability at $c$ implies continuity there,
    that the first term on the right side of $({\ast})$ approaches $f'(c)g(c)$ as $x$ approaches~$c$.
    Likewise, the second term on the right side of $({\ast})$ approaches $f(c)g'(c)$. The desired result now follows easily.


\V

        (b) This is can be proved using a similar `Add-and-Subtract Trick'; the details are left as an exercise. \Q % EXERCISE

\V

%%%
\begin{quotation}
{\footnotesize \underline{\Note}\IndB{\notes}{on the Product Rule for Derivatives} (on the statement and proof of Part~(a))
    The statement and proof of the Product Rule given above are both standard in calculus texts. In particular,
    the statement of the formula for the derivative of the product function $P$ is given first, while the proof involves an unmotivated application of the `Add-and-Subtract Trick'.
    This {\Note} tries to show how these steps might have arisen.

        First of all, the statement of the formula for $P'$ in reality came out of the proof. Indeed, one does not use that statement in carrying out the proof. %\\\\\

        First, note that simply writing down the definition of $P'(c)$ leads one to the expression
        \begin{displaymath}
        P(x) - P(c) \,=\, f(x)g(x) - f(c)g(c) \h ({\ast}{\ast})
        \end{displaymath}
    All that is known about $f$ and $g$ is that they are both differentiable at $c$, which facts involve the differences $f(x) - f(c)$ and $g(x) - g(c)$.
    The expression $f(x)g(x) - f(c)g(c)$ on the right side of Equation~$({\ast}{\ast})$ does not factor in a way that brings, say, $g(x) - g(c)$ into play.
    However, one may note that the difference $f(x)g(x) - f(x)g(c)$ {\em does} factor nicely as $f(x)(g(x) - g(c))$,  by the Distributive Law from arithmetic.
    Then it is natural to consider how this `nicer' expression compares with the right side of $({\ast}{\ast})$, obtaining
        \begin{displaymath}
        f(x)g(x) - f(c)g(c) \,=\, f(x)g(x) - f(x)g(c) + f(x)g(c) - f(c)g(c),
        \end{displaymath}
    as in the proof above. Fortunately, the expression $f(x)g(c) - f(c)g(c)$ can itself be factored as $(f(x) - f(c))g(c)$,
    so now the quantity of real interest, $f(x)g(x) - f(c)g(c)$, has been written as the sum of {\em two} terms
    that both can be factored in ways which allow the introduction of the differentiability hypotheses on $f$ and~$g$.
    The proof then contiues as above.

}%EndFootNoteSize
\end{quotation}
%##



\VV

        The preceding rules show how differentiation behaves when combined with the standard algebraic operations,
    such as addition and multiplication, and the standard names assigned to these rules relate directly to the names of these operations.
    The next rule describes how differentiation behaves when combined with the operation of composition,
    so it would seem reasonable to call it something like `The Composition Rule for Differentiation'.
    In reality, however, for historical reasons it is called the `Chain Rule'.

\V

            \subsection{\small{\bf Theorem} (Chain Rule for Differentiation)}\IndBD{differentiation rules}{chain rule for differentiation}
            \label{ThmE30.70}

\V

        Suppose that $h:I \,{\rightarrow}\, {\RR}$ is a function, defined on an open interval $I$, such that $h$ can be expressed on $I$ as the composition $h \,=\, g{\circ}f$
    of real-valued functions $g$ and $f$. (This implies that $f$ is defined on $I$ and that $g$ is defined on the image $f[I]$.)
    Assume that $f$ is differentiable at a point $c$ of $I$, and that $g$ is differentiable at the corresponding point $d \,=\, f(c)$.
    (The latter fact implies that $g$ is defined on some open interval containing~$d$.) Then the composition $h \,=\, g{\circ}f$ is differentiable at $c$, and one has
        \begin{displaymath}
        h'(c) \,=\, g'(d){\cdot}f'(c); \mbox{ that is, }
    (g{\circ}f)'(c) \,=\, g'(f(c)){\cdot}f'(c).
        \end{displaymath}

\V

        \underline{Preliminary Discussion} In elementary calculus the usual first step to convince students of the naturalness of this Rule is to note that
        \begin{displaymath}
        \frac{h(x)-h(c)}{x-c} \,=\, \frac{g(f(x))-g(f(c))}{x-c} \,=\, 
    \left(\frac{g(f(x))-g(f(c))}{f(x)-f(c)}\right) {\cdot}
    \left(\frac{f(x)-f(c)}{x-c}\right) \h ({\ast})
        \end{displaymath}
    when $x{\in}I$ and $x \,\,{\neq}\,\, c$. Next, one notes that as $x$ approaches $c$, the second factor on the right side of Equation~$({\ast})$ approaches $f'(c)$.
    Likewise, as $x$ approaches $c$ the quantity $y \,=\, f(x)$ approaches $d \,=\, f(c)$ (by Theorem~\Ref{ThmE20.30}, `Diffeerentiability implies Continuity'),
    hence the first factor on the right side of Equation~$({\ast})$ approaches $g'(d)$. This argument can be found,
    for example, in Cauchy's famous {\em Le\c{c}ons sur le calcul differential}.

        As modern texts point out, however, this argument has a major gap: it assumes that if $x \,\,{\neq}\,\, c$, then one can divide by $f(x)-f(c)$;
    more precisely, it assumes that $f(x) \,\,{\neq}\,\, f(c)$ for all $x \,\,{\neq}\,\, c$, at least when $x$ is sufficiently near $c$.
    Apparently this gap went unnoticed by many authors for a long time. For instance, as is alluded to in Chapter Quote~(3) at the start of this chapter, 
    the famous book `A Course in Pure Mathematics', by G.~H. Hardy, did not fix this gap until its fourth edition in~$1925$.
    (Indeed, the first correct proofs apparently appeared only in the $1870$s; see the article by H.~S. Carslaw in Volume~$29$ (1923) of the {\em Bulletin of the American Mathematical Society}.)
    Modern calculus texts usually provide a rigorous, but sophisticated, proof of a rather different style.

        The proof given below is based mainly on Equation~$({\ast})$, but it handles the `major gap' issue by using the `sequential' characterization of 
    `${\displaystyle \lim_{x \,{\rightarrow}\, c}}$' given in Theorem~\Ref{ThmD50.25}.

\V

         {\bf Proof} Let ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,x_{k},\,{\ldots}\,)$
    be a sequence of numbers in $I$, with $x_{k} \,\,{\neq}\,\, c$, such that ${\displaystyle \lim_{k \,{\rightarrow}\, {\infty}} x_{k} \,=\, c}$.
    Let $y_{k} \,=\, f(x_{k})$, so that $\lim_{k \,{\rightarrow}\, {\infty}} y_{k} \,=\, d$, by the continuity of $f$ at~$c$.
    To simplify the notation, for each index $k$ set
        \begin{displaymath}
        r_{k} \,=\, \frac{h(x_{k})-h(c)}{x_{k}-c} \,=\, \frac{g(y_{k})-g(d)}{x_{k}-c}.
        \end{displaymath}
    In light of Part~(c) of Theorem~\Ref{ThmD50.25}, it suffices to show that for every sequence ${\xi}$ as above one has
    $\lim_{k \,{\rightarrow}\, {\infty}} r_{k} \,=\, g'(d){\cdot}f'(c)$.


        Divide the indices $k$ into two disjoint subsets $A$ and $B$ of ${\NN}$:
        \begin{displaymath}
        A \,=\, \{k{\in}{\NN}: y_{k} \,\,{\neq}\,\, d\} \mbox{ and }
        B \,=\, \{k{\in}{\NN}: y_{k} \,=\, d\}
        \end{displaymath}
    Note that ${\NN} \,=\, A\,{\cup}\,B$, so that at least one of the subsets $A$ or $B$ must be infinite. There are three cases to consider:

\VA

        \h Case (i)\,\,$A$ is infinite and $B$ is finite;

        \h Case (ii)\, $B$ is infinite and $A$ is finite;

        \h Case (iii) $A$ and $B$ are both infinite.

\VA

        Suppose first that Case~(i) holds. Then for all sufficiently large $k$ one can write the following analog of Equation~$({\ast})$ above:
        \begin{displaymath}
        r_{k} \,=\, \frac{g(f(x_{k}))-g(f(c))}{x_{k}-c} \,=\, 
    \left(\frac{g(y_{k})-g(d)}{y_{k}-d}\right) {\cdot}
    \left(\frac{f(x_{k})-f(c)}{x_{k}-c}\right) \h ({\ast}{\ast})
        \end{displaymath}
    (Recall that $y_{k} \,=\, f(x_{k})$ and that $d \,=\, f(c)$, so in this case for all sufficiently large $k$ one has $y_{k}-d \,\,{\neq}\,\, 0$.)
    Since $\lim_{k \,{\rightarrow}\, {\infty}} y_{k} \,=\, d$ it follows that the first factor
    on the right side of Equation~$({\ast}{\ast})$ approaches $g'(d)$ as $k$ approaches infinity.
    Similarly, the second factor on the right side  of $({\ast}{\ast})$ approaches $f'(c)$ as $k$ approaches infinity.
    The Product Rule for Limits then implies that the left side of $({\ast}{\ast})$ also approaches a finite limit, namely $g'(d){\cdot}f'(c)$.
    That is, ${\displaystyle \lim_{k \,{\rightarrow}\, {\infty}} r_{k} \,=\, g'(d){\cdot}f'(c)}$, as required.

        Suppose next that Case (ii) holds. In this case one has $f(x_{k})-f(c) \,=\, 0$ for all sufficiently large~$k$,
    so that one certainly has $\lim_{k \,{\rightarrow}\, {\infty}} r_{k} \,=\, 0$.
    Likewise one also has ${\displaystyle \lim_{k \,{\rightarrow}\, {\infty}} \frac{f(x_{k}) - f(c)}{x_{k}-c} \,=\, 0}$.
    But since $f$ is, by hypothesis, differentiable at~$c$, it follows that $f'(c) \,=\, 0$
    and thus $\lim_{k \,{\rightarrow}\, {\infty}} r_{k} \,=\, 0 \,=\, g'(d){\cdot}f'(c)$, as required.

        Finally, suppose that Case (iii) holds. Apply Case~(i) to the subsequence of the sequence
    ${\rho} \,=\, (r_{1}, r_{2},\,{\ldots}\,)$ corresponding to the set $A$ of indices;
    likewise, apply Case~(ii) to the subsequence of ${\rho}$ corresponding to the set~$B$.
    It then follows that both of these sequences of ${\rho}$ converge to the same limit, namely $g'(d) {\cdot} f'(c)$.
    By the Generalized Odd/Even Limit Theorem, it follows that the sequence ${\rho}$ itself converges to the same limit, $g'(d){\cdot}h(c)$, as required.

\V

        {\bf Remark} Some alternate proofs of the Chain Rule are outlined in the exercises.

%% EXERCISES Include the ones referred to.

\VV

        

        The proof given above can be modified to show the following variation of the standard Chain Rule.


\VV


            \subsection{\small{\bf Theorem} (Modified Chain Rule for Differentiation)}\IndBD{differentiation rules}{modified chain rule for differentiation}
            \label{ThmE30.75}

\V

        Suppose that $h:I \,{\rightarrow}\, {\RR}$ is a function, defined on an open interval $I$, and that $c$ is a point of $I$ such that $h'(c)$ exists.
    Suppose further that $h$ can be expressed on $I$ as the composition $h \,=\, g{\circ}f$ of real-valued functions $g$ and $f$.

\V

    (a)\,Assume that $g$ is differentiable at the point $d \,=\, f(c)$ and that $g'(d) \,\,{\neq}\,\, 0$.
    (Recall that this implies that $g$ is defined on some open interval containing~$d$.)
    Also assume that $f$ is continuous at~$c$. Then $f$ is also differentiable at~$c$, and $f'(c) \,=\, h'(c)/g'(d)$.

\V

        (b)\,Similarly, assume instead that $f$ is differentiable at $c$, with $f'(c) \,\,{\neq}\,\, 0$, and that $g$ is continuous at~$d$.
    Then $g$ is differentiable at $d$ and $g'(d) \,=\, h'(c)/f'(c)$.

\V

        {\bf Proof} 

\V

         (a)\,As in the proof of the preceding theorem, let ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,x_{k},\,{\ldots}\,)$
    be a sequence of numbers in $I$, with $x_{k} \,\,{\neq}\,\, c$, such that ${\displaystyle \lim_{k \,{\rightarrow}\, {\infty}} x_{k} \,=\, c}$,
    and let $y_{k} \,=\, f(x_{k})$. As before, the hypothesis that $f$ is continuous at $c$ implies $\lim_{k \,{\rightarrow}\, {\infty}} y_{k} \,=\, d$.
    Set ${\displaystyle z_{k} \,=\, \frac{f(x_{k}) - f(c)}{x_{k} - c}}$. It suffices to show that for each such sequence ${\xi}$
    the corresponding sequence ${\zeta} \,=\, (z_{1}, z_{2},\,{\ldots}\,)$ converges to~$h'(c)/g'(d)$.

        As in the preceding proof, divide the indices $k$ into two disjoint subsets $A$ and $B$ of ${\NN}$:
        \begin{displaymath}
        A \,=\, \{k{\in}{\NN}: y_{k} \,\,{\neq}\,\, d\} \mbox{ and }
        B \,=\, \{k{\in}{\NN}: y_{k} \,=\, d\}
        \end{displaymath}
    Once again, ${\NN} \,=\, A\,{\cup}\,B$, so that at least one of the subsets $A$ or $B$ must be infinite, and there are three cases to consider:

\VA

        \h Case (i)\,\,$A$ is infinite and $B$ is finite;

        \h Case (ii)\, $B$ is infinite and $A$ is finite;

        \h Case (iii) $A$ and $B$ are both infinite.

\VA

        Suppose first that Case (i) holds, so that $y_{k}-d \,\,{\neq}\,\, 0$ if $k$ is large enough. The hypothesis $g'(d) \,\,{\neq}\,\, 0$ then implies that
        \begin{displaymath}
        y_{k}-d \,\,{\neq}\,\, 0 \mbox{ and }
        \frac{g(y_{k}) - g(d)}{y_{k}-d} \,\,{\neq}\,\, 0 \mbox{ for all sufficiently large $k$}.
        \end{displaymath}
    For all such $k$ one then has
        \begin{displaymath}
        \frac{h(x_{k}) - h(c)}{x_{k} -c} \,=\, \frac{g(y_{k}) - g(d)}{x_{k} - c}
     \,=\, 
        \left(\frac{g(y_{k}) - g(d)}{y_{k} - d}\right){\cdot}\left(\frac{y_{k} - d}{x_{k}-c}\right),
        \end{displaymath}
    and thus
        \begin{displaymath}
        \frac{f(x_{k}) - f(c)}{x_{k} - c}
     \,=\, 
        \left(\frac{h(x_{k}) - h(c)}{x_{k} - c}\right)\left/\left(\frac{g(y_{k}) - g(d)}{y_{k}-d}\right)\right.
        \end{displaymath}
    As $k$ approaches infinity, the numerator on the right approaches $h'(c)$, while the demominator on the right approaches the nonzero number $g'(d)$.
    Thus the Quotient Rule for Limits implies that the fraction on the left approaches a finite limit, namely $h'(c)/g'(d)$.

        Now suppose that Case~(ii) holds. Then for all sufficiently large $k$ one has $h(x_{k}) \,=\, h(c)$,
    so that ${\displaystyle \lim_{k \,{\rightarrow}\, {\infty}} \frac{h(x_{k}) - h(c)}{x_{k}-c} \,=\, 0}$.
    By the hypothesis that $h$ is differentiable at $c$ it follows that $h'(c) \,=\, 0$.
    But likewise in this Case one has $f(x_{k}) \,=\, f(c)$ for sufficiently large~$k$, so that ${\displaystyle \lim_{k \,{\rightarrow}\, {\infty}} \frac{f(x_{k})-f(c)}{x_{k}-c} \,=\, 0}$. Once again, this fraction approaches a finite limit, which again equals $0 \,=\, h'(c)/g'(d)$.

        Finally, suppose that Case (iii) holds. Apply Case~(i) to the subsequence of ${\zeta}$ which corresponds to the indices in~$A$,
    and apply Case~(ii) to the subsequence of ${\zeta}$ which corresponds to the indices in~$B$.
    From what has just been shown, both of these subsequences converge to the same number, namely $h'(c)/g'(d)$ (which of course must equal~$0$).
    By the Generalized Odd/Even Limit Theorem, it follows that the sequence ${\zeta}$ converges to $h'(c)/g'(d)$, and the desired result follows.

\V

        (b) The proof of this part involves a simple modification of the preceding one, and is left as an exercise.

\VV

        The next result applies the Modified Chain Rule to inverse functions.

\V


            \subsection{\small{\bf Theorem} (Inverse-Function Differentiation Rule)}
            \label{ThmE30.77}\IndB{differentiation rules}{inverse-function differentiation rule}

\V

        Suppose that $f:I \,{\rightarrow}\, {\RR}$ is a continuous strictly monotonic function on an open interval $I$.
    Let $J \,=\, f[I]$ be the image of $I$ under $f$, so that by Theorem~\Ref{ThmD25.55C} $J$ is also an open interval,
    $f$ is a bijection of $I$ onto $J$, and $f$ has an inverse function $g \,=\, f^{-1}:J \,{\rightarrow}\, I$ which is continuous on~$J$.
    Suppose in addition that $f$ is differentiable at a point $c$ of $I$, and that $f'(c) \,\,{\neq}\,\, 0$. Let $d \,=\, f(c)$.
    Then $g$ is differentiable at $d$, and
        \begin{equation}
        \label{EqnE.70}
        g'(d) \,=\, \frac{1}{f'(c)} \,=\, \frac{1}{f'(f^{-1}(d))}.
        \end{equation}

\V

        {\bf Proof}\, Let $h \,=\, g{\circ}f \,=\, f^{-1}{\circ}f$ on $I$, so that $h$ is the identity function on~$I$.
    In particular, $h'(c) \,=\, 1$. Part~(b) of the Modified Chain Rule then implies that $g$ is differentiable at $d$ and that $g'(d) \,=\, h'(c)/f'(c) \,=\, 1/f'(c)$, as claimed. \Q

%---------------- Old Inverse Fcn Derivative
\StartSkip{
        Suppose that $g:J \,{\rightarrow}\, {\RR}$ is differentiable and strictly monotonic on an open interval $J$, and let $I \,=\, g[J]$.
    Suppose further that for each $y$ in $J$ one has $g'(y) \,\,{\neq}\,\, 0$. Then $I$ is also an open interval, $g:J \,{\rightarrow}\, I$ is a bijection, 
    and the corresponding inverse function $f \,=\, g^{-1}:I \,{\rightarrow}\, J$ is differentiable on~$I$.
    More precisely, suppose that $y{\in}J$ and that $x \,=\, f^{-1}(y)$, so that $y \,=\, f(x)$. Then
        \begin{equation}
        \label{EqnE.70}
        (f'(x) \,=\, \frac{1}{g'(y)} \,=\, \frac{1}{g'(g^{-1}(y))}.
        \end{equation}
    That is, $(g^{-1})(y)' \,=\, 1/(g'{\circ}g^{-1})(y)$ for each $y$ in~$J$.

\V

        \underline{Proof} The facts that $I$ is an open interval, that $g:J \,{\rightarrow}\, I$,
    and that $f:I \,{\rightarrow}\, J$ is continuous on~$I$, follow directly from Theorem~\Ref{ThmD25.55C}.
    Now let $h \,=\, g{\circ}f \,=\, g{\circ}g^{-1}:I \,{\rightarrow}\, I$, so that $h(x) \,=\, x$ for all $x$ in~$I$.
    By Example~\Ref{ExampE20.70}~(1) above, the function $h$ is differentiable on~$I$, and by hypothesis one has $g'(y) \,\,{\neq}\,\, 0$ if $y{\in}J$.
    The desired result now follows easily from the Modified Chain Rule. \Q
}%EndSkip
%------------------- Old Inverse Fcn Derivative

\VV

        What follows are some concrete applications of the preceding general differentiation rules.%\\\\

\V

            \subsection{\small{\bf Examples}} 
            \label{ExampE30.55}

\V

        {\bf Remark}\,The first two examples below were obtained directly from the definition of the derivative in Example~\Ref{ExampE20.70}.
    The point here is that they can also be obtained from their simplest cases using the computational rules derived above.

\V

\hspace*{\parindent}(1) Suppose that $k$ is a nonnegative integer, and let $f_{k}:{\RR} \,{\rightarrow}\, {\RR}$ be the `$k$-th-Power Function'. That is,
        \begin{displaymath}
        f_{k}(x) \,=\, x^{k} \mbox{ for all $x$ in ${\RR}$}.
        \end{displaymath}
    Then $f_{k}$ is differentiable at all points of ${\RR}$, and one has
        \begin{equation}
        \label{EqnE.25A}
        f_{k}'(x) \,=\, k\,x^{k-1} \mbox{ for all $x$ in ${\RR}$}.
        \end{equation}
        \underline{Note}: It is understood that, in the current context, the expression $x^{0}$ is treated as the constant~$1$, even for $x \,=\, 0$.
    In elementary calculus one is also taught that the expression $0^{0}$ is an `indeterminant form' and has no definite meaning.
    These interpretations are in conflict, but in practice this normally causes no confusion.

        In light of the preceding Note, the case $k \,=\, 0$ reduces to the fact that a constant on ${\RR}$ function has derivative~$0$ at each point.
    If $k \,=\, 1$, then the formula reduces to $f_{1}'(x) \,=\, 1{\cdot}x^{0} \,=\, 1$, which was shown in Rxample~\Ref{ExampE20.70}~(1).
    Now suppose that the formula holds for a given natural number $k$, and note that $f_{k+1}(x) \,=\, f_{k}(x){\cdot}f_{1}(x)$.
    Then the Product Rule for Derivatives implies that $f_{k+1}$ is also differentiable on ${\RR}$, and that
        \begin{displaymath}
        f_{k+1}(x) \,=\, f_{k}'(x){\cdot}f_{1}'(x) + f_{k}(x){\cdot}f_{1}'(x)
     \,=\, 
        (k\,x^{k-1}){\cdot}x + x^{k}{\cdot}(1) \,=\, (k+1)\,x^{k},
        \end{displaymath}
    as required. The desired result follows by the Principle of Mathematical Induction.

\V

        (2) Suppose that $k$ is a {\em negative} integer.
    Let $f_{k}:{\RR}{\setminus}\{0\} \,{\rightarrow}\, {\RR}$ be given by
        \begin{displaymath}
        f_{k}(x) \,=\, x^{k} \mbox{ for all $x \,\,{\neq}\,\, 0$}.
        \end{displaymath}
    Then $f_{k}$ is differentiable at all points of ${\RR}{\setminus}\{0\}$, and one has
        \begin{equation}
        \label{EqnE.25B}
        f_{k}'(x) \,=\, k\,x^{k-1} \mbox{ for all $x \,\,{\neq}\,\, 0$}.
        \end{equation}
    Once again the verification is obtained using the Principle of Mathematical Induction,
    starting with the case $k \,=\, -1$ carried out in Example~\Ref{ExampE20.70}~(4).


\V

        (3) Let $n$ be a positive integer, and define the function $f_{1/n}:(0,+{\infty}) \,{\rightarrow}\, {\RR}$
    by the rule that for each $x\,>\,0$ is $f_{1/n}(x)$ is $x^{1/n}$, the positive $n$-th root of~$x$.
    Thus, $f_{1/n}$ is the inverse on $(0,+{\infty})$ of the function $f_{n}$ described above, and by Theorem~\Ref{ThmD25.55C} $f_{1/n}$ is continuous there.
    Since $f_{n}'(x) \,\,{\neq}\,\, 0$ if $x\,>\,0$, it now follows from Theorem~\Ref{ThmE30.77} that $f_{1/n}$ is differentiable on $(0,{\infty})$,
    and that
        \begin{displaymath}
        f_{1/n}'(x) \,=\, \frac{1}{f_{n}'(x^{1/n})} \,=\, 
        \frac{1}{n\,x^{(n-1)/n}} \,=\, \frac{1}{n}\,x^{-1+1/n} \mbox{ for $x\,>\,0$}.
        \end{displaymath}

\V

        (4) Suppose that $r$ is a rational number and $h_{r}:(0,{\infty}) \,{\rightarrow}\, {\RR}$
    is the function given by the rule $h_{r}(x) \,=\, x^{r}$; see Example~\Ref{ExampD30.110}~(2).
    Express $r$ in the form $r \,=\, m/n$, where $m$ and $n$ are integers and $n\,>\,0$, so that $h_{r} \,=\, f_{m}{\circ}f_{1/n}$ on~$(0,+{\infty})$.
    It follows from the regular Chain Rule that $h_{r}$ is differentiable on $(0,+{\infty})$, and that for each $x\,>\,0$ one has
        \begin{displaymath}
        h_{r}'(x) \,=\, f_{m}'(x^{1/n}){\cdot}f_{1/n}'(x)
     \,=\, 
        m\,(x^{1/n})^{m-1}{\cdot}\left(\frac{1}{n}\right)(x^{1/n -1})
     \,=\, 
        \frac{m}{n}\,x^{(m-1)/n}{\cdot}x^{-1+1/n}
     \,=\, 
        \frac{m}{n}\,x^{-1+m/n}
     \,=\, 
        r\,x^{r-1}.
        \end{displaymath}

\VV

        It is convenient for ease of future reference to include the following generalization of the preceding examples.

\V

             \subsection{\small{\bf Corollary} (The Extended Power Rule for Differentiation -- Rational Case)}
            \label{CorE40.130}\IndB{differentiation rules}{extended power rule -- Rational Case}

\V

        Suppose that $f:I \,{\rightarrow}\,Y$ is differentiable on the open interval~$I$ and with values in a nonempty subset $Y$ of~${\RR}$.
    Let $r$ be a rational number, and suppose that $Y$ is such that $(f(x))^{r}$ is defined for each $x$ in~$I$.
    More precisely:

\VA

        \h (1) If $r$ is a nonnegative integer, then $Y \,{\subseteq}\, {\RR}$.

        \h (2) If $r$ is a negative integer, then $Y \,{\subseteq}\, {\RR}\,{\setminus}\,\{0\}$.

        \h (3) If $r$ is not an integer, then $Y \,{\subseteq}\,(0,{\infty})$.

\VA

\noindent Define $g:I \,{\rightarrow}\, {\RR}$ by the rule $g(x) \,=\, (f(x))^{r}$ for each $x$ in~$I$.
    Then $g'(x) \,=\, r\,(f(x))^{r-1}\,f'(x)$ for each $x$ in~$I$.

\V
        The simple proof is left as an exercise. \Q %EXERCISE

\V


        \underline{Remark} There is an extension of this result which also holds for exponents $r$ which are irrational. This is discussed later.

\VV

            \subsection{\small{\bf Examples}}
            \label{ExampE30.57}

\V

\hspace*{\parindent}(1) Let $h:{\RR}{\setminus}\{3\} \,{\rightarrow}\, {\RR}$ be given by the rule
        \begin{displaymath}
        h(x) \,=\, \frac{5x-17}{7x-21} \mbox{ for all $x \,\,{\neq}\,\, 3$}.
        \end{displaymath}
    This function is the ratio of linear functions $f$ and $g$, where
        \begin{displaymath}
        f(x) \,=\, 5x-17 \mbox{ and } g(x) \,=\, 7x-21 \mbox{ for all $x$}.
        \end{displaymath}
    Note that $f'(x) \,=\, 5$ and $g'(x) \,=\, 7$ for all $x$. It thus follows that $h$ is differentiable at all points of its domain, and that the Quotient Rule for Derivatives can be used to get
        \begin{displaymath}
        h'(x) \,=\, \frac{5{\cdot}(7x-21) - (5x-17){\cdot}7}{(7x-21)^{2}} \h ({\ast})
        \end{displaymath}
    This answer is correct as it stands. However, by noting that
        \begin{displaymath}
        5{\cdot}(7x-21) - (5x-17){\cdot}7 \,=\, (35x - 105)  - (35x - 119)  \,=\, 14,
        \end{displaymath}
     one sees that the answer can be simplified to
        \begin{displaymath}
        h'(x) \,=\, \frac{14}{(7x-21)^{2}} \h ({\ast}{\ast})
        \end{displaymath}

\V

        (2) Let $h$ be the same function as in the preceding example, but now compute $h''(x)$.
    Of course this requires simply differentiating the function $h'$ obtained above.
    By combining the Constant-Factor and Reciprocal Rules for Derivatives to the function $h'$, one gets (using the form of $h'$ given in Equation~$({\ast}{\ast})$)
        \begin{displaymath}
        h''(x) \,=\, - \frac{2{\cdot}14{\cdot}7}{(7x-21)^{3}} \,=\, -\frac{196}{(7x-21)^{3}}
        \end{displaymath}
    Note that this calculation is quicker, and less prone to errors, than the corresponding one based on Equation~$({\ast})$ above.

\V

        (3) (a) Suppose that $p:{\RR} \,{\rightarrow}\, {\RR}$ is a polynomial function on ${\RR}$;
    that is, there are constants $c_{0}$, $c_{1}$,\,{\ldots}\,$c_{k}$ such that $p(x) \,=\, c_{k}x^{k} + c_{k-1}x^{k-1} + \,{\ldots}\,+c_{1}x + c_{0}$ for all $x$ in ${\RR}$.
    Then $p$ is differentiable on ${\RR}$; more precisely, $p'$ is the polynomial given by
        \begin{displaymath}
        p'(x) \,=\, k\,a_{k}x^{k-1} + (k-1)\,a_{k-1}\,x^{k-2} + \,{\ldots}\, +2\,a_{2}\,x + a_{1}.
        \end{displaymath}
    By an obvious inductive argument one sees that $p$ is a $C^{{\infty}}$ function on ${\RR}$,
    and that for each $m \,=\, 1,2,\,{\ldots}\,$ the function $p^{(m)}$ is a polynomial of degree at most $k-m$.
    In particular, $p^{(m)}(x) \,=\, 0$ for all $x$ in ${\RR}$ if $m\,\,{\geq}\,\,k+1$.

\V

        (b) Suppose that $f$ is a rational function; that is, there exist polynomial functions $p$ and $q$, with $q$ not the zero polynomial,
    such that $f(x) \,=\, p(x)/q(x)$ for all $x$ in ${\RR}$ such that $q(x) \,\,{\neq}\,\, 0$.
    The domain of $f$ is an open set $U$ of the form $U \,=\, {\RR}{\setminus}S$,
    where $S$ is the set of all $x$ in ${\RR}$ such that $q(x) \,=\, 0$.
    (One knows from high-school algebra that $S$ is a finite set; it may be empty.)
    Then $f$ is differentiable on $U$, and one has
        \begin{displaymath}
        f'(x) \,=\, \frac{g'(x)h(x) - f(x)g'(x)}{\left(g(x)\right)^{2}} \mbox{ for all $x$ in $U$}.
        \end{displaymath}
    It then follows from Part~(a) of this example that $f'$ is itself a rational function on the open set $U$.
    Once again, one can use induction to conclude that $f$ is of class $C^{{\infty}}$ on $U$, and that each derivative $f^{(m)}$ is a rational function on~$U$.

\VV

%%%
\begin{quotation}
{\footnotesize \underline{\Notes}\IndB{\notes}{on calculus pedagogy, Part~1} (on calculus pedagogy, Part~1)
(1) Calculus teachers are familiar with the phenomenon of students taking an examination who ask `Do we need to simplify our answers?'.
    The usual answer from teachers is `Yes, you do'.

        Of course, this raises the touchy issue of `How simple is simple enough?' A Justice-Stewart type of response that `You'll know it when you see it' (see Chapter Quote~(1) for Chapter~\Ref{ChaptA}) is probably inadequate.
    Indeed, sometimes one form of the answer is  better (i.e., `simpler') for one purpose, while a different form of the answer is better for a different purpose.
    For instance, the expressions ${\displaystyle \frac{2}{1-x^{2}}}$ and ${\displaystyle \frac{1}{1+x} + \frac{1}{1-x}}$ both represent the same function for $x \,\,{\neq}\,\,  \,{\pm}\, 1$.
    The former expression is `simpler' if the issue is to determine where the function takes on the value $2$,
    while the latter expression is `simpler' if the the issue is to compute the fifth derivative of the function.

        Likewise, in the case of Example~(1) above, failure to simplify the expression $5{\cdot}(7x-21) - (5x-17){\cdot}7$ to $14$ would not make the answer `wrong'.
    However, it would make it harder to compute $h''(x)$, in that errors would be much more likely to enter.

\V

        (2) Many calculus students avoid using the `Constant Factor Rule' for differentiation,
    preferring to use instead the Product Rule. For example, such a student might compute the derivative of the function $y \,=\, 5\,x^{3}$ as follows:
        \begin{displaymath}
        y' \,=\, 5'{\cdot}x^{3} + 5{\cdot}(x^{3})' \,=\, 0{\cdot}x^{3} + 5{\cdot}3{\cdot}x^{2} \,=\, 15\,x^{2}.
        \end{displaymath}
    This is perfectly legal, albeit inefficient. But there is a deeper problem: it happens more often than one might expect that the student will write
    $5' \,=\, 1$, or maybe $5' \,=\, 5$, instead of $5' \,=\, 0$. If the calculation in question is just one step in, say, a max/min problem,
    the student may have turned a problem intended to have a quick and easy solution into one in which much valuable exam time is wasted getting the wrong answer or no answer at all.
    Similarly, students often avoid direct application of the Reciprocal Rule for differentiation to compute $(1/g)' \,=\, -g'/g^{2}$ and instead use the Quotient Rule $(1'{\cdot}g - 1{\cdot}g')/g^{2}$, opening the real possibility of introducing the error $1' \,=\, 1$.
}%EndFootNoteSize
\end{quotation}
%##

    
\VV

%----------------- Extended Power Rule Integers
\StartSkip{

            \subsection{\small{\bf Corollary} (The Extended Power Rule for Differentiation -- Integer Case)}\IndB{differentiation rules}{extended power rule -- integer case}
            \label{CorE30.80}

        Suppose that $f:I \,{\rightarrow}\, {\RR}$ is differentiable at each point of an open interval~$I$.

\V

        (a) Let $m$ be a nonnegative integer, and let $F:I \,{\rightarrow}\, {\RR}$ be given by the rule
        \begin{displaymath}
        F(x) \,=\, \left(f(x)\right)^{m} \mbox{ for every $x$ in $I$}.
        \end{displaymath}
    Then $F$ is differentiable at every point of $I$, and one has
        \begin{equation}
        \label{EqnE.65A}
        F'(x) \,=\, m(f(x))^{m-1}{\cdot}f'(x) \mbox{ for all $x$ in $I$}.
        \end{equation}

\V

        (b) Suppose, in addition, that there is an open subinterval $J$ of $I$ such that $f(x) \,\,{\neq}\,\, 0$ when $x{\in}J$.
    Let $k$ be a negative integer, and define $G:J \,{\rightarrow}\, {\RR}$ by the rule
        \begin{displaymath}
        G(x) \,=\, \left(f(x)\right)^{k} \mbox{ for all $x$ in $J$}.
        \end{displaymath}
    Then $G$ is differentiable at each point of $J$, and one has
        \begin{equation}
        \label{EqnE.65B}
        G'(x) \,=\, k\left(f(x)\right)^{k-1}{\cdot}f'(x) \mbox{ for each $x$ in $J$}.
        \end{equation}

\V

        \underline{Proof}

\V

\hspace*{\parindent}(a) Let $g(u) \,=\, u^{m}$ for all $u$ in ${\RR}$.
    Clearly $F \,=\, g{\circ}f$. By Part~(a) of Corollary~\Ref{CorE30.55} the function $g$ is differentiable at each point of ${\RR}$,
    and $g'(u) \,=\, mu^{m-1}$ for all $x$.
    Thus, the Chain Rule can be applied to show that the composition $F \,=\, g{\circ}f$ is differentiable at each point of $I$, and that
        \begin{displaymath}
        F'(x) \,=\, g'(f(x)){\cdot}f'(x) \,=\, m(f(x))^{m-1}{\cdot}f'(x) \mbox{ for all $x$ in $I$},
        \end{displaymath}
    as claimed.

\V

        (b) This part follows in much the same way by combining the Chain Rule with Part~(b) of Corollary~\Ref{CorE30.55}.
    The details are left as an exercise. \Q
}%\EndSkip
%--------------------- Power Rule Integeers

\VV
%----------------Z Cut the function with discontinuous derivative entirely
\StartSkip{
                \section{{\bf Derivatives with Discontinuities}}
                \label{SectE35}\IndB{ZZ Sections}{\Ref{SectE35} Derivatives with Discontinuities}

\VV

        The main goal of this section is to construct examplse of functions $F$ which are differentiable on ${\RR}$ but not $C^{1}$ on~${\RR}$.
    (The task of constructing for each $k$ in ${\NN}$ an example of a function which is $k$-times differentiable on ${\RR}$,
    but not $C^{k}$ on ${\RR}$, is left to the exercises.) % EXERCISES

        The functions $F$ obtained here involve a sequence of `bumps' which converge to the point of discontinuity of $F'$.
    These `bumps' become successively thinner and shorter, and it may seem that the construction is unduly complicated.
    However in Section~\Ref{SectE50} it is shown that discontinuites of derivatives are of necessity complicated.
    (Recall from Remark~\Ref{RemrkD20.50}~(2) that to say $F'$ has a discontinuity at a number $c$ requires that $F'$ be defined at~$c$;
    that is, the orginal function $F$ must be differentiable at~$c$.)

\V

            \subsection{\small{\bf Preliminary Example} (Strictly $C^{1}$ Bump Functions)}\IndBD{derivatives}{bump functions -- strictly $C^{1}$}
            \label{ExampE30.85B}
\V

        Let $g:{\RR} \,{\rightarrow}\, {\RR}$ be given by the rule
        \begin{displaymath}
        g(x) \,=\, \left\{
        \begin{array}{ll}
        0 & \mbox{if $x\,\,{\leq}\,\,0$} \\
        x^{2} & \mbox{if $x\,>\,0$}
        \end{array}
                            \right.
        \end{displaymath}
    It is clear that $g$ is continuous at every point of~${\RR}$, even at~$0$. It is also clear that one has
        \begin{displaymath}
        g'(x) \,=\, \left\{
        \begin{array}{cl}
        0 & \mbox{if $x\,\,{\leq}\,\,0$} \\
        2\,x & \mbox{if $x\,>\,0$}
        \end{array}
                            \right. \h ({\ast})
        \end{displaymath}
    Thus it remains to determine differentiability of $g$ at~$c \,=\, 0$. One has
        \begin{displaymath}
        \frac{g(x) - g(0)}{x-0} \,=\, \left\{
        \begin{array}{cl}
        0 & \mbox{if $x\,<\,0$} \\
        x^{2}/x \,=\, x & \mbox{if $x\,>\,0$}
        \end{array}
                            \right.
        \end{displaymath}
    One sees that both the left-hand derivative at $0$ and the right-hand derivative at $0$ of $g$ exist and equal~$0$.
    Thus by Theorem~\Ref{ThmE20.25A} it follows that $g'(0)$ exists and also equals~$0$.
    It is then clear from~$({\ast})$ that $g'$ is continuous on~${\RR}$, even at~$0$; that is, $g$ is~$C^{1}$ on~${\RR}$.
    However, one also computes that $g''(x) \,=\, 0$ if $x\,<\,0$ and $g''(x) \,=\, 2$ if $x\,>\,0$, so $g$ is not $C^{2}$ on~${\RR}$;
    indeed, it is not even twice differentiable on~${\RR}$. Thus, $g$ is {\em strictly} $C^{1}$ on~${\RR}$.

\VV


        Now let let $h:{\RR} \,{\rightarrow}\, {\RR}$ be given by the formula $h(x) \,=\, g(1-x)$ for each $x$ in~${\RR}$, where $g$ is described above.
    That is,
        \begin{displaymath}
        h(x) \,=\, \left\{
        \begin{array}{ll}
        (1-x)^{2} & \mbox{if $x\,<\,1$} \\
          0         & \mbox{if $x\,\,{\geq}\,\,1$}
        \end{array}
                            \right.
        \end{displaymath}
    Then it follows from the results just obtained for~$g$, combined with the Chain Rule,
    that $h'(x) \,=\, -g'(1-x)$ for each $x$ in~${\RR}$. In particular, $h$ is also strictly $C^{1}$ on~${\RR}$.

        Consider now the function $f:{\RR} \,{\rightarrow}\, {\RR}$ given by the rule $f(x) \,=\, g(x)\,h(x)$ for each $x$ in~${\RR}$.
    From the known facts about the factors $g$ and~$h$, it is clear that $f$ is a strictly $C^{1}$ function on~${\RR}$ given by the formula
        \begin{displaymath}
        f(x) \,=\, \left\{
        \begin{array}{ll}
       ( x\,(1-x))^{2} & \mbox{if $0\,<\,x\,<\,1$} \\
          0         & \mbox{if $x\,\,{\leq}\,\,0 \mbox{ or }x\,\,{\geq}\,\,0$}
        \end{array}
                            \right.
        \end{displaymath}
    Furthermore, it is clear that $f$ assumes the minimum value $0$ for ${\RR}$ at every number $x$ such that $x\,\,{\leq}\,\,0$ or $x\,\,{\geq}\,\,1$.
    By the result of Example~\Ref{ExampB30.80}, it also follows that $f$ assumes its maximum value for ${\RR}$ at $x \,=\, 1/2$ (and nowhere else),
    and that this maximum value is $1/16$. More precisely, it is pointed out in that example that
    ${\displaystyle x\,(1-x) \,=\, \frac{1}{4} - \left(x-\frac{1}{2}\right)^{2}}$,
    so that ${\displaystyle f(x) \,=\, \left(\frac{1}{4} - \left(x-\frac{1}{2}\right)^{2}\right)^{2}}$ if $0\,<\,x\,<\,1$.
    Geometrically speaking, it is easy to see that the graph $y \,=\, f(x)$, in the $x\,y$-plane,
    of the function $f$ consists of the horizontal half lines $\{(x,y): y \,=\, 0 \mbox{ if $x\,\,{\leq}\,\,0$}\}$
    and $\{(x,y): y \,=\, 0$ \mbox{ if $x\,\,{\geq}\,\,0$}\}, together with a positive `bump' of maximum height $1/4^{2} \,=\, 1/16$ on the interval $0\,<\,x\,<\,1$.

        It is convenient to normalize the construction above so that the height of the `bump' equals~$1$:
    simply replace $f$ by $\hat{f}:{\RR} \,{\rightarrow}\, {\RR}$, given by the rule $\hat{f}(x) \,=\, 16\,f(x)$, where $f$ is defined as above.

\V

        One can use the preceding result to associate an analogous normalized strictly $C^{1}$ function with every closed bounded interval $[a,b]$ in ${\RR}$.
    Indeed, the familiar linear function ${\varphi}_{[a,b]}:{\RR} \,{\rightarrow}\,{\RR}$,
    given by the rule ${\varphi}_{[a,b]}(t) \,=\, (1-t)a+tb$ for $t$ in ${\RR}$, is strictly increasing and maps $[0,1]$ bijectively onto~$[a,b]$.
    Its strictly increasing linear inverse ${\varphi}_{[a,b]}^{-1}:{\RR} \,{\rightarrow}\, {\RR}$
    is then given by the formula ${\varphi}_{[a,b]}^{-1}(x) \,=\, {\displaystyle \frac{x-a}{b-a}}$; it maps $[a,b]$ bijectively onto $[0,1]$.
    It is easy to see that the composition $\hat{f}{\circ}{\varphi}_{[a,b]}^{-1}$ is a strictly $C^{1}$ function on ${\RR}$
    which equals $0$ at all $x$ such that $x\,\,{\leq}\,\,a$ or $x\,\,{\geq}\,\,b$,
    and which has a positive `bump' for all $x$ such that $a\,<\,x\,<\,b$. Furthermore, the `bump' has a maximum height of~$1$,
    and this maximum occurs at the midpoint $x \,=\, (a+b)/2$ of the interval $[a,b]$.

\V

        It is convenient to formally summarize the preceding results.

\V

            \subsection{\small{\bf Definition}}
            \label{DefE35.110}\IndBD{functions}{bump functions, strictly $C^{1}$}

\V

        Let $k$ be a natural number.

\V

        (1) The {\bf standard normalized strictly $C^{1}$ bump function supported on the interval $[0,1]$}
    is the function $B_{[0,1]}:{\RR} \,{\rightarrow}\, {\RR}$ given by the rule
        \begin{displaymath}
        B_{[0,1]}(x) \,=\, 
            \left\{
        \begin{array}{cl}
              0        & \mbox{if $x\,\,{\leq}\,\,0$ or $x\,\,{\geq}\,\,1$} \\
    16\,x^{2}(1-x)^{2} & \mbox{if $0\,<\,x\,<\,1$}
        \end{array}
                        \right.
        \end{displaymath}


\V

        (2) More generally, let $a$ and $b$ be real numbers such that $a\,<\,b$. 
    Then the {\bf standard normalized strictly $C^{1}$ bump function supported on the interval $[a,b]$} is the function $B_{[a,b]}:{\RR} \,{\rightarrow}\, {\RR}$ given by the rule
        \begin{displaymath}
        B_{[a,b]}(x) \,=\,\left(B_{[0,1]}{\circ}{\varphi}^{-1}_{[a,b]}\right)(x) \,=\,  B_{[0,1]}\left(\frac{x-a}{b-a}\right) \mbox{ for all $x$ in ${\RR}$}.
        \end{displaymath}

\VV

       {\bf Remark}\, The strictly $C^{1}$ bump functions described above can be combined to form functions
    which are differentiable at each point of~${\RR}$, but whose derivatives fail to be continuous at one or more points of~${\RR}$.
    In order to construct such examples, it is useful to study the derivatives of the $C^{1}$ bump functions in more detail.

\V

            \subsection{\small{\bf Lemma}}
            \label{LemmaE35.120}

\hspace*{\parindent}(a) Let $M_{[0,1]}$ denote the maximum value of the function $\left|B'_{[0,1]}\right|$ on~${\RR}$. Then $3/2\,\,{\leq}\,\,M_{[0,1]}\,\,{\leq}\,\,16$.

\V

        (b) More generally, if $[a,b]$ is a closed bounded interval in ${\RR}$, and $M_{[a,b]}$ is the maximum value of $\left|B'_{[a,b]}\right|$ on~${\RR}$,
    then ${\displaystyle \frac{3}{2\,(b-a)}\,\,{\leq}\,\,M_{[a,b]}\,\,{\leq}\,\,\frac{16}{b-a}}$.

\V

        {\bf Proof} (a) To save some writing, set $F \,=\, B_{[0,1]}$. It was proved above that $F'$ is continuous on ${\RR}$
    and that $F'(x) \,=\, 0$ for each $x$ in ${\RR}$ such that $x\,\,{\leq}\,\,0$ or $x\,\,{\geq}\,\,1$.
    Since $F'$ is continuous on $[0,1]$, it follows from the Extreme-Value Theorem that $|F'|$ does have a maximum value $M_{[0,1]}$ on $[0,1]$.
    To estimate it, note that on the interval $[0,1]$ one has
        \begin{displaymath}
        F(x) \,=\, 16\,x^{2}\,(1-x)^{2} \,=\, 16\,x^{2} -32\,x^{3} + 16\,x^{4},
        \end{displaymath}
    hence
        \begin{displaymath}
        F'(x) \,=\, 32\,x - 96\,x^{2} + 64\,x^{3}
    \,=\,
        32\,x\,(1-3\,x + 2\,x^{2}) \,=\, 32\,x\,(x-1)\,(x-1/2)
        \end{displaymath}
    Since $|x|\,\,{\leq}\,\,1$, $|x-1|\,\,{\leq}\,\,1$ and $|x-1/2|\,\,{\leq}\,\,1/2$ for all $x$ in the interval~$[0,1]$,
it follows that
    $|F'(x)|\,\,{\leq}\,\,32{\cdot}1{\cdot}1{\cdot}(1/2) \,=\, 16$ for all $x$ in~${\RR}$. In particular, $M_{[0,1]}\,\,{\leq}\,\,16$.
    Likewise, since $|F'(1/4)| \,=\, 32{\cdot}(1/4){\cdot}(3/4){\cdot}(1/4) \,=\, 3/2$, it follows that $M_{[0,1]}\,\,{\geq}\,\,3/2$.

\V

        (b) By definition one has
        \begin{displaymath}
        B_{[a,b]}(x) \,=\, \left(F{\circ}{\varphi}^{-1}_{[a,b]}\right)(x) \mbox{ for all $x$ in ${\RR}$},
        \end{displaymath}
 where $F$ is the function in Part~(a). It then follows from the Chain Rule that
        \begin{displaymath}
        (B_{[a,b]})'(x) \,=\,\left(F{\circ}{\varphi}^{-1}_{[a,b]}\right)'(x) \,=\, F'\left(\frac{x-a}{b-a}\right){\cdot}\frac{1}{b-a}.
        \end{displaymath}
    The desired result now follows from Part~(a). \Q

%------------------- A Alternate approach to preceding lemma using Fermat's Test
\StartSkip{
            \subsection{\small{\bf Lemma}}
            \label{LemmaE35.120}

\V

\hspace*{\parindent}(a) Let $M_{[0,1]}$ denote the maximum value of the function $\left|B'_{[0,1]}\right|$ on~${\RR}$. Then $M_{[0,1] \,=\, 16\sqrt{3}/9$}$.

\V

        (b) More generally, if $[a,b]$ is a closed bounded interval in ${\RR}$, then the maximum value
    $M_{[a,b]}$ of the function $\left|B_{[a,b]}\right|$ is $16\sqrt{3}/(9(b-a)) \,=\, M_{[0,1]}/(b-a)$.

\V

        {\bf Proof}\,

\V

        (a) For convenience let us set $F \,=\, B_{[0,1]}$.
    It was proved above that $F'$ is continuous on ${\RR}$ and that $F'(x) \,=\, 0$ for each $x$ in ${\RR}$ such that $x\,\,{\leq}\,\,0$ or $x\,\,{\geq}\,\,1$.
    Since $F'$ is continuous on $[0,1]$, it follows from the Extreme-Value Theorem that $F'$ does have a maximum value and a minimum value on $[0,1]$.
    Since the function $F'$ is differentiable on the open interval $(0,1)$, the First-Derivative Test for Extrema implies that $F'$ must assume its extreme values for the interval $[0,1]$ either at one of the endpoints $0$ or $1$,
    or at a point $x$ such that $0\,<\,x\,<\,1$, at which one has $F''(x) \,=\, 0$.
    On the interval $(0,1)$ one has
        \begin{displaymath}
        F(x) \,=\, 16\,x^{2}\,(1-x)^{2} \,=\, 16\,x^{2} -32\,x^{3} + 16\,x^{4},
        \end{displaymath}
    hence
        \begin{displaymath}
        F'(x) \,=\, 32\,x - 96\,x^{2} + 64\,x^{3}
    \mbox{ and thus }
        F''(x) \,=\, 32 - 192\,x + 192\,x^{2} \,=\, 32\,(1 - 6\,x + 6\,x^{2})
        \end{displaymath}
    It is a straight-forward application of the Quadratic Formula from high-school algebra to verify that $F''(x) \,=\, 0$ if, and only if,
    $x \,=\,  {\displaystyle \frac{1}{2}\,{\pm}\, \frac{\sqrt{3}}{6}}$; note that both these values of $x$ lie in the open interval~$(0,1)$.
    One computes that
        \begin{displaymath}
        F'(0) \,=\, F'(1) \,=\, 0, \mbox{ while } 
        F'\left(\frac{1}{2} - \frac{\sqrt{3}}{6}\right) \,=\, \frac{16\sqrt{3}}{9} \mbox{ and } F'\left(\frac{1}{2} + \frac{\sqrt{3}}{6}\right) \,=\, -\frac{16\sqrt{3}}{9}
        \end{displaymath}
    The desired result follows.

\V

        (b) By definition one has
        \begin{displaymath}
        B_{[a,b]}(x) \,=\, \left(F{\circ}{\varphi}^{-1}_{[a,b]}\right)(x) \mbox{ for all $x$ in ${\RR}$},
        \end{displaymath}
    where $F$ is the function discussed in the proof of Part~(a). It then follows from the Chain Rule that
        \begin{displaymath}
        (B_{[a,b]})'(x) \,=\,\left(F{\circ}{\varphi}^{-1}_{[a,b]}\right)'(x) \,=\, F'\left(\frac{x-a}{b-a}\right){\cdot}\frac{1}{b-a}.
        \end{displaymath}
    The desired result now follows from Part~(a). \Q

}%\EndSkip
%------------------- A

\VV

        If one were to ask a student in a real analysis course, familiar with elementary calculus,
    whether there exists a funtion which is differentiable on an open interval $I$ but whose derivative is not continuous at each point of~$I$,
    there are two likely answers:

\VA

         \h (1)\, `No, because if such a function existed, my calculus teacher would have given an example'; or

        \h (2)\,`Yes, because I've already seen plenty of simple functions which have simple discontinuities.'

\VA

        The correct answer is given by the next example, and is `Yes'; but for deeper reasons than one might expect.

\V


            \subsection{\small{\bf Example of a Differentiable Function whose Derivative has a Discontinuity}}
            \label{ExampE35.130}

\V

        For each $m \,=\, 1,2,\,{\ldots}\,$, let ${\displaystyle f_{m} \,=\, B_{\left[\frac{1}{m+1},\frac{1}{m}\right]}}$.
    Note that $f_{m}$ is $C^{1}$ on~${\RR}$, as is any constant multiple of this function.

        \underline{Comment} Note that $1/m - 1/(m+1) \,=\, 1/(m(m+1))$, so by Part~(b) Lemma~\Ref{LemmaE35.120},
    one sees that the maximum value $M_{[a,b]}$ of $|f'_{m}|$ on ${\RR}$ satisfies
        \begin{displaymath}
        \frac{3}{2}\,m(m+1)\,\,{\leq}\,\,M_{[a,b]}\,\,{\leq}\,\, 16\,m\,(m+1)
        \end{displaymath}

\V

        Define $G:{\RR} \,{\rightarrow}\, {\RR}$ by the rule
        \begin{displaymath}
        G(x) \,=\, \left\{
        \begin{array}{cl}
           0     & \mbox{if $x\,<\,0$ or $x\,>\,1$} \\
                 &                                  \\
        {\displaystyle p_{m}f_{m}(x)} & \mbox{if ${\displaystyle \frac{1}{m+1}\,<\,x\,\,{\leq}\,\,\frac{1}{m}}$ for some $m \,=\, 1,2,\,{\ldots}\,$, where each $p_{m}\,>\,0$ is a constant to be determined} \\
                 & \\
            0    & \mbox{if $x \,=\, 0$}
        \end{array}
                                        \right.
        \end{displaymath}
    It is easy to see that the function $G$ is $C^{1}$ at each point $c$ of ${\RR}$, with the possible exception of $c \,=\, 0$.
    Indeed, it is clear that the function $G$ is constant on the open interval $(-{\infty},0)$, hence smooth at each $c$ such that $c\,<\,0$.
    Next, note that for each $m$ in ${\NN}$ the restriction of $G$ to the open interval $(1/(m+1),+{\infty})$
    is precisely the (finite) sum $p_{m}\,f_{m} + p_{m-1}\,f_{m-1} + \,{\ldots}\, + p_{1}\,f_{1}$
    of the restrictions to that interval of the $C^{1}$ functions
    $p_{m}\,f_{m}$, $p_{m-1}\,f_{m-1}$, \,{\ldots}\, $p_{1}\,f_{1}$, and therefore is also $C^{1}$ on the interval $(1/(m+1),+{\infty})$.
    It follows that the restriction of $G$ to the open interval $(0,+{\infty})$ is also $C^{1}$ on $(0,+{\infty})$, hence $C^{1}$ at each $c\,>\,0$.

    To check for differentiability of $G$ at $c \,=\, 0$, let $x$ be any nonzero number and consider the corresponding difference quotient at~$c \,=\, 0$:
        \begin{displaymath}
        \frac{G(x)-G(0)}{x-0} \h ({\ast})
        \end{displaymath}

    \h (i)\, Suppose $x\,<\,0$. Then the quotient in~$({\ast})$ reduces to $(0-0)/(x-0) \,=\, 0$ for all such $x$,
    so the left-hand derivative of $G$ at $0$ exists, and equals~$0$. Thus, to get $G$ to be differentiable at $c \,=\, 0$,
    one needs to chose the constants $p_{m}$ so that the right-hand derivative at~$0$ exists and also equals~$0$.

        \h (ii) Suppose $x\,>\,0$. Without loss of generality assume that $0\,<\,x\,\,{\leq}\,\,1$,
    and thus, by the Principle of Archimedes, $x{\in}(1/(m+1),1/m]$ for some $m$ in~${\NN}$.
    In that case, since $0\,\,{\leq}\,\,f_{m}(x)\,\,{\leq}\,\,1$ and $1/(m+1)\,<\,x$, one has $1/x\,<\,m+1$
        \begin{displaymath}
        0\,\,{\leq}\,\,\frac{G(x)-G(0)}{x-0}
    \,=\,
        p_{m}\,\frac{f_{m}(x)}{x}
    \,\,{\leq}\,\,
        p_{m}{\cdot}\frac{1}{1/(m+1)}
    \,=\,
        p_{m}\,(m+1).
        \end{displaymath}
    It now follows easily, using the Squeeze Theorem for Sequences, that if the constants $p_{m}$
    are chosen so that $\lim_{m \,{\rightarrow}\, {\infty}} p_{m}\,(m+1) \,=\, 0$, the right-hand derivative of $G$ at $0$ exists and equals~$0$.
    Thus $G'(0)$ exists and equals~$0$.


        In order to ensure that the function $G'$ is not continuous at $c \,=\, 0$, one must be more specific about the choices of the constants~$p_{m}$.
    For example, set $p_{m} \,=\, 1/(m\,(m+1))$. Then by the `Comment' made above one sees that
    the maximum value of $|G'|$ on an interval $[1/(m+1),1/m]$ is bounded above by $16$;
    in particular, $\lim_{m \,{\rightarrow}\, {\infty}} p_{m}\,(m+1) \,=\, 0$, so $G$ is differentiable at~$0$.
    Furthermore, by the same `Comment' there  exists $x_{m}$ in the interval $[1/(m+1), 1/m]$ such that $|G'(x_{m})|\,\,{\geq}\,\,3/2$ for each index~$m$.
    It is clear that $\lim_{m \,{\rightarrow}\, {\infty}} x_{m} \,=\, 0$, but it is not the case that $\lim_{m \,{\rightarrow}\, {\infty}} G'(x_{m}) \,=\, 0$.
    Thus, with this choice of $p_{m}$ the function $G$ is differentiable on ${\RR}$ but not $C^{1}$ on~${\RR}$.

    The preceding choice of $p_{m}$ produces a {\em bounded} function~$G'$. If, instead, one chooses $p_{m} \,=\, 1/(\sqrt{m}\,(m+1))$,
    for instance, then the corresponding $G'$ is actually unbounded at~$0$.

\V

        {\bf Remark}\,The construction carried out above is quite complicated; a simpler construction is provided later using trigonometric functions.
    The reason for providing such examples now s to justify the importance of the main theorem of the next section.
}%EndSkip
%------------------- Z



%---------------- B
\StartSkip{

            \subsection{\small{\bf Example of a Differentiable Function whose Derivative has a Discontinuity}}
            \label{ExampE35.130}

\V

        For each $m \,=\, 1,2,\,{\ldots}\,$, let ${\displaystyle f_{m} \,=\, B_{[\frac{1}{m+1},\frac{1}{m}}}$.
    Note that $f_{m}$ is $C^{1}$ on~${\RR}$, as is any constant multiple of this function.

        \underline{Comment} Note that $1/m - 1/(m+1) \,=\, 1/(m(m+1))$, so by Part~(b) Lemma~\Ref{LemmaE35.120},
    one sees that the maximum and minimum values of $f_{m}'$ are $ \,{\pm}\, {\displaystyle \frac{16\sqrt{3}m(m+1)}{9}}$.

\V

        Define $G:{\RR} \,{\rightarrow}\, {\RR}$ by the rule
        \begin{displaymath}
        G(x) \,=\, \left\{
        \begin{array}{cl}
           0     & \mbox{if $x\,<\,0$ or $x\,>\,1$} \\
                 &                                  \\
        {\displaystyle p_{m}f_{m}(x)} & \mbox{if ${\displaystyle \frac{1}{m+1}\,<\,x\,\,{\leq}\,\,\frac{1}{m}}$ for some $m \,=\, 1,2,\,{\ldots}\,$, where $p_{m}\,>\,0$ is a constant to be determined} \\
                 & \\
            0    & \mbox{if $x \,=\, 0$}
        \end{array}
                                        \right.
        \end{displaymath}
    It is easy to see that the function $G$ is $C^{1}$ at each point $c$ of ${\RR}$, with the possible exception of $c \,=\, 0$.
    Indeed, it is clear that the function $G$ is constant on the open interval $(-{\infty},0)$, hence smooth at each $c$ such that $c\,<\,0$.
    Next, note that for each $m$ in ${\NN}$ the restriction of $G$ to the open interval $(1/(m+1),+{\infty})$
    is precisely the sum of the restrictions to that interval of the $C^{1}$ functions
    $p_{m}\,f_{m} + p_{m-1}\,f_{m-1} + \,{\ldots}\, + p_{1}\,f_{1}$, and therefore is also $C^{1}$ on the interval $(1/(m+1),+{\infty})$.
    It follows that the restriction of $G$ to the open interval $(0,+{\infty})$ is also $C^{1}$ on $(0,+{\infty})$, hence $C^{1}$ at each $c\,>\,0$.

    To check for differentiability of $G$ at $c \,=\, 0$, let $x$ be any nonzero number and consider the corresponding difference quotient at~$c \,=\, 0$:
        \begin{displaymath}
        \frac{G(x)-G(0)}{x-0} \h ({\ast})
        \end{displaymath}


    \h (i), Suppose $x\,<\,0$. Then the quotient in~$({\ast})$ reduces to $(0-0)/(x-0) \,=\, 0$ for all $x\,<\,0$,
    so the left-hand derivative of $G$ at $0$ exists, and equals~$0$. Thus, to get $G$ to be differentiable at $c \,=\, 0$,
    one needs to chose the constants $p_{m}$ so that the right-hand derivative at~$0$ exists and also equals~$0$.

        \h (ii) Suppose $x\,>\,0$. Without loss of generality assume that $0\,<\,x\,\,{\leq}\,\,1$,
    and thus, by the Principle of Archimedes, $x{\in}(1/(m+1),1/m]$ for some $m$ in~${\NN}$.
    In that case, since $0\,\,{\leq}\,\,f_{m}(x)\,\,{\leq}\,\,1$ and $1/(m+1)\,<\,x$, one has
        \begin{displaymath}
        0\,\,{\leq}\,\,\frac{G(x)-G(0)}{x-0} \,=\, p_{m}\,\frac{f_{m}(x)}{x}\,\,{\leq}\,\, p_{m}{\cdot}\frac{1}{1/(m+1)} \,=\, p_{m}\,(m+1).
        \end{displaymath}
    It now follows easily, using the Squeeze Theorem for Sequences, that if the constants $p_{m}$
    are chosen so that $\lim_{m \,{\rightarrow}\, {\infty}} p_{m}\,(m+1) \,=\, 0$, the right-hand derivative of $G$ at $0$ exists and equals~$0$.
    Thus $G'(0)$ exists and equals~$0$.

        In order to ensure that the function $G'$ is not continuous at $c \,=\, 0$, one must be more specific about the choices of the constants~$p_{m}$.
    Indeed, by the `Comment' made above one sees that the maximum value of $G'$ on an interval $[1/(m+1),1/m]$ is
    ${\displaystyle p_{m}\,\frac{16\sqrt{3}m(m+1)}{9}}$. Choose, for example, $p_{m} \,=\, 1/(m\,(m+1))$,
    so that $\lim_{m \,{\rightarrow}\, {\infty}} p_{m}\,(m+1) \,=\, 0$. Then there exists a point $x_{m}$ in that interval such that $G'(x_{m}) \,=\, 16\,\sqrt{3}/9$,
    so $\lim_{m \,{\rightarrow}\, {\infty}} x_{m} \,=\, 0$ but $\lim_{m \,{\rightarrow}\, {\infty}} G'(x_{m}) \,\,{\neq}\,\, G'(0)$.
    With this choice it is clear that $G'$ is discontinuous at~$0$, but is still bounded on~${\RR}$.
    If, instead, one chooses $p_{m} \,=\, 1/(\sqrt{m}\,(m+1))$, for instance, then the corresponding $G'$ is actually unbounded at~$0$.
}%\EndSkip
%----------------- B


%----------------- C
\StartSkip{
        Let $k$ be a natural number, and let $g_{k}:{\RR} \,{\rightarrow}\, {\RR}$ be given by the rule
        \begin{displaymath}
        g_{k}(x) \,=\, \left\{
        \begin{array}{ll}
        0 & \mbox{if $x\,\,{\leq}\,\,0$} \\
        x^{k+1} & \mbox{if $x\,>\,0$}
        \end{array}
                            \right.
        \end{displaymath}
    It is clear that $g_{k}$ is continuous at every point of~${\RR}$, even at~$0$. It is also clear that for each $j \,=\, 1, \,{\ldots}\,k+1$ one has
        \begin{displaymath}
        g_{k}^{(j)}(x) \,=\, \left\{
        \begin{array}{cl}
        0 & \mbox{if $x\,\,{\leq}\,\,0$} \\
        (k+1)\,k\,\,{\ldots}\,(k+2-j)\,x^{k+1-j} & \mbox{if $x\,>\,0$}
        \end{array}
                            \right. \h ({\ast})
        \end{displaymath}
    (In the special case $j \,=\, k+1$ the factor $x^{k+1-j}$ in the second line becomes~$x^{0} \,=\, 1$, since $x\,>\,0$, and the coefficient is $(k+1)!$.)
    Thus it remains to determine differentiability of $g_{k}^{(j)}$ at~$0$. Consider, for instance, the case $j \,=\, 1$. One has
        \begin{displaymath}
        \frac{g_{k}'(x) - g_{k}'(0)}{x-0} \,=\, \left\{
        \begin{array}{cl}
        0 & \mbox{if $x\,<\,0$} \\
        (k+1)\,x^{k} & \mbox{if $x\,>\,0$}
        \end{array}
                            \right.
        \end{displaymath}
    One sees that both the left-hand derivative at $0$ and the right-hand derivative at $0$ of $g_{k}$ exist and equal~$0$.
    Thus by Theorem~\Ref{ThmE20.25A} it follows that $g_{k}'(0)$ exists and also equals~$0$.
    It is then clear from~$({\ast})$ that $g_{k}'$ is continuous on~${\RR}$, even at~$0$.
    Similar arguments work for all $j$ such that $1\,\,{\leq}\,\,j\,\,{\leq}\,\,k$
    to show that $g_{k}^{(j)}$ is defined at all points, even at~$0$, where its value is~$0$,
    and that $g_{k}^{(j)}$ is continuous on~${\RR}$. In other words, $g_{k}$ is $C^{k}$ on~${\RR}$.

        The argument fails, however, when $j \,=\, k+1$. Indeed, in this case one gets from~$({\ast})$ that
        \begin{displaymath}
        \frac{g_{k}^{(k)}(x) - g_{k}^{(k)}(0)}{x-0}
     \,=\, \left\{
        \begin{array}{cl}
        0 & \mbox{if $x\,<\,0$} \\
        (k+1)! & \mbox{if $x\,>\,0$}
        \end{array}
                        \right.
        \end{displaymath}
    In particular, the left-hand limit at~$0$ of the expression $g_{k}^{(k)}(x)-g_{k}^{(k)}(0))/(x-0)$ is~$0$,
    while the corrresponding right-hand limit is $(k+1)! \,\,{\neq}\,\, 0$. It now follows,
    from Theorem~\Ref{ThmE20.25A} again, that $g_{k}^{(k)}$ is {\em not} differentiable at~$0$.
    Since $g_{k}$ is $C^{k}$ on ${\RR}$ but not $C^{k+1}$ on~${\RR}$, it is strictly $C^{k}$ on~${\RR}$, as required.

\VV

        \underline{Note} There is a subtle difference between the statement `$f$ is $C^{k}$ on $I$'
    and the statement `$f$ is $k$-times differentiable on $I$' (see Part~(4) of Definition~\Ref{DefE20.80} above).
    Namely, the former statement requires the $k$-th derivative $f^{(k)}$ to exist and be continuous at each point of $I$,
    while the latter statement requires only the existence of $f^{(k)}$ but not its continuity.
    Notice that the existence of $f^{(k)}$ on $I$ for some $k\,\,{\geq}\,\,1$ automatically ensures, by Theorem~\Ref{ThmE20.30},
    the continuity of $f^{(j)}$ for $0\,\,{\leq}\,\,j\,<\,k$.
    The explicit requirement of this continuity of $f^{(j)}$ for such $j$ in the statement of the definition of `$f$ is $C^{k}$' is thus not really needed;
    it is included to simplify the discussion later on.


        Let $k$ be a natural number, and let $g_{k}$ be the function constructed in Example~\Ref{ExampE20.85} above, and let $h_{k}:{\RR} \,{\rightarrow}\, {\RR}$
    be given by the formula $h_{k}(x) \,=\, g_{k}(1-x)$ for each $x$ in~${\RR}$. That is,
        \begin{displaymath}
        h_{k}(x) \,=\, \left\{
        \begin{array}{ll}
        (1-x)^{k+1} & \mbox{if $x\,<\,0$} \\
          0         & \mbox{if $x\,\,{\geq}\,\,0$}
        \end{array}
                            \right.
        \end{displaymath}
    Then it follows from the results of that example, combined with repeated use of the Chain Rule,
    that $h_{k}^{(j)}(x) \,=\, (-1)^{j}g_{k}^{(j)}(1-x)$ for each $x$ in~${\RR}$. In particular, $h_{k}$ is also strictly $C^{k}$ on~${\RR}$.

        Consider now the function $f_{k}:{\RR} \,{\rightarrow}\, {\RR}$ given by the rule $f_{k}(x) \,=\, g_{k}(x)\,h_{k}(x)$ for each $x$ in~${\RR}$.
    From the known facts about the factors $g_{k}$ and~$h_{k}$, it is clear that $f_{k}$ is a strictly $C^{k}$ function on~${\RR}$ given by the formula
        \begin{displaymath}
        f_{k}(x) \,=\, \left\{
        \begin{array}{ll}
        x^{k+1}\,(1-x)^{k+1} & \mbox{if $0\,<\,x\,<\,1$} \\
          0         & \mbox{if $x\,\,{\leq}\,\,0 \mbox{ or }x\,\,{\geq}\,\,0$}
        \end{array}
                            \right.
        \end{displaymath}
    Furthermore, it is clear that $f_{k}$ assumes the minimum value $0$ for ${\RR}$ at every number $x$ such that $x\,\,{\leq}\,\,0$ or $x\,\,{\geq}\,\,1$.
    By the result of Example~\Ref{ExampB30.80}, and the usual order properties of real numbers,
    it also follows that $f_{k}$ assumes its maximum value for ${\RR}$ at $x \,=\, 1/2$ (and nowhere else), and that this maximum value is $1/4^{k+1}$.
    Viewing the function $f_{k}$ in terms of its graph $y \,=\, f_{k}(x)$ in the $x\,y$-plane,
    it is easy to see that the graph of the function $f_{k}$ consists of the horizontal half lines
    $\{(x,y): y \,=\, 0 \mbox{ if $x\,\,{\leq}\,\,0$}\}$ and $\{(x,y): y \,=\, 0$ \mbox{ if $x\,\,{\geq}\,\,0$}\},
    together with a `bump' of maximum height $1/4^{k+1}$ on the interval $0\,<\,x\,<\,1$.

        It is convenient to normalize the construction above so that the height of the `bump' equals~$1$:
    simply replace $f_{k}$ by $\hat{f}_{k}:{\RR} \,{\rightarrow}\, {\RR}$, given by the rule $\hat{f}_{k}(x) \,=\, 4^{k+1}f_{k}(x)$,
    where $f_{k}$ is defined as above.

\V

        One can use the preceding result to associate an analogous normalized strictly $C^{k}$ function with every closed bounded interval $[a,b]$ in ${\RR}$.
    Indeed, the familiar linear function ${\varphi}:{\RR} \,{\rightarrow}\,{\RR}$,
    given by the rule ${\varphi}(t) \,=\, (1-t)a+tb$ for $t$ in ${\RR}$, is strictly increasing and maps $[0,1]$ bijectively onto~$[a,b]$.
    Its strictly increasing linear inverse ${\varphi}^{-1}:{\RR} \,{\rightarrow}\, {\RR}$
    is then given by the formula ${\varphi}^{-1}(x) \,=\, {\displaystyle \frac{x-a}{b-a}}$; it maps $[a,b]$ bijectively onto $[0,1]$.
    It is easy to see that the composition $\hat{f}_{k}{\circ}{\varphi}^{-1}$ is a strictly $C^{k}$ function on ${\RR}$
    which equals $0$ at all $x$ such that $x\,\,{\leq}\,\,a$ or $x\,\,{\geq}\,\,b$,
    and which has a positive `bump' for all $x$ such that $a\,<\,x\,<\,b$. Furthermore, the `bump' has a maximum height of~$1$,
    and this maximum occurs at the midpoint $x \,=\, (a+b)/2$ of the interval $[a,b]$.

\V

        It is convenient to formally summarize the preceding results.

\V

            \subsection{\small{\bf Definition} (Strictly $C^{k}$ Bump Functions)}
            \label{DefE35.110}\IndBD{functions}{bump functions, strictly $C^{k}$}

\V

        Let $k$ be a natural number.

\V

        (1) The {\bf standard normalized strictly $C^{k}$ bump function supported on the interval $[0,1]$}
    is the function $\hat{B}^{[k]}_{[0,1]}:{\RR} \,{\rightarrow}\, {\RR}$ given by the rule
        \begin{displaymath}
        \hat{B}^{[k]}_{[0,1]}(x) \,=\, 
            \left\{
        \begin{array}{cl}
              0        & \mbox{if $x\,\,{\leq}\,\,0$ or $x\,\,{\geq}\,\,1$} \\
    4^{k+1}(1-x)^{k+1}x^{k+1} & \mbox{if $0\,<\,x\,<\,1$}
        \end{array}
                        \right.
        \end{displaymath}


\V

        (2) More generally, let $a$ and $b$ be real numbers such that $a\,<\,b$. 
    Then the {\bf standard normalized strictly $C^{k}$ bump function supported on the interval $[a,b]$} is the function $\hat{B}^{[k]}_{[a,b]}:[a,b] \,{\rightarrow}\, {\RR}$ given by the rule
        \begin{displaymath}
        \hat{B}^{[k]}_{[a,b]}(x) \,=\, \hat{B}^{[k]}_{[0,1]}\left(\frac{x-a}{b-a}\right) \mbox{ for all $x$ in ${\RR}$}.
        \end{displaymath}

\V

            \subsection{\small{\bf Remark}}
            \label{RemrkE35.115}

        Let $k$ be a natural number. Then it is clear that 
    $\hat{B}^{[k]}_{[a,b]}(x) \,=\, \left(\hat{B}^{[0]}_{[a,b]}(x)\right)^{k+1} \mbox{ for all $x$ in $[a,b]$}$.
    It is also clear that $\hat{B}^{[k]}_{[a,b]}$ is $C^{{\infty}}$ at all points except $a$ and $b$,
    and that it is $C^{k}$ -- but not $C^{k+1}$ -- at those two points.

\VV

        The strictly $C^{k}$ bump functions described above can be combined to form functions
    which are $k$-times differentiable at each point of~${\RR}$, but fail to be $C^{k}$ at one or more points of~${\RR}$.
    We consider only the case $k \,=\, 1$ here. Thus, we construct a function which is differentiable at each point of

\V

            \subsection{\small{\bf Lemma}}
            \label{LemmaE35.120}

\V

\hspace*{\parindent}(a) The maximum value of the derivative $\left(\hat{B}^{[1]}_{[0,1]}\right)'$ is $16\sqrt{3}/9$;
    likewise, the minimum value of this derivative is $-16\sqrt{3}/9$.

\V

        (b) More generally, if $[a,b]$ is a closed bounded interval in ${\RR}$, then the maximum value of
    $\left(\hat{B}^{[1]}_{[a,b]}\right)'$ is $16\sqrt{3}/(9(b-a))$, while the minimum value of this derivative is $-16\sqrt{3}/(9(b-a))$.

\V

        {\bf Proof}\,

\V

        (a) For convenience let $f \,=\, \hat{B}^{[1]}_{[0,1]}$.
    It is clear that $f'$ is continuous on ${\RR}$ and that $f'(x) \,=\, 0$ for each $x$ in ${\RR}$ such that $x\,\,{\leq}\,\,0$ or $x\,\,{\geq}\,\,1$.
    Since $f'$ is continuous on $[0,1]$ it follows from the Extreme-Value Theorem that $f'$ does have a maximum value on $[0,1]$.
    Since $f''$ is defined on $(0,1)$, one can apply the First-Derivative Test for Extrema to the function $f'$,
    together with the continuity of $f'$, to conclude that $f'$ must assume its extreme values for the interval $[0,1]$ either at one of the endpoints $0$ or $1$, or at a point $x$ such that $0\,<\,x\,<\,1$ at which $f''(x) \,=\, 0$.
    It is a straight-forward exercise to verify that $f''(x) \,=\, 0$ if, and only if, $x \,=\,  {\displaystyle \frac{1}{2}\,{\pm}\, \frac{\sqrt{3}}{6}}$;
    clearly both these values of $x$ lie in the open interval~$(0,1)$.
    In addition, one easily computes that
        \begin{displaymath}
        f'(0) \,=\, f'(1) \,=\, 0, \mbox{ while } 
        f'\left(\frac{1}{2} + \frac{\sqrt{3}}{6}\right) \,=\, -\frac{16\sqrt{3}}{9} \mbox{ and } f'\left(\frac{1}{2} - \frac{\sqrt{3}}{6}\right) \,=\, \frac{16\sqrt{3}}{9}
        \end{displaymath}
    The desired result follows.

\V

        (b) By definition one has
        \begin{displaymath}
        \hat{B}^{1}_{[a,b]}(x) \,=\, f\left(\frac{x-a}{b-a}\right) \mbox{ for all $x$ in ${\RR}$},
        \end{displaymath}
    where $f$ is the function discussed in the proof of Part~(a). It then follows from the Chain Rule that
        \begin{displaymath}
        ({\hat{B}}^{1}_{[a,b]})'(x) \,=\, \frac{d\,}{dx}\, f\left(\frac{x-a}{b-a}\right) \,=\, f'\left(\frac{x-a}{b-a}\right){\cdot}\frac{1}{b-a}.
        \end{displaymath}
    The desired result now follows from Part~(a). \Q

\V

        The next result uses bump functions to construct a phenomenon which is normally not encountered in elementary calculus;
    namely, an example of a function which is differentiable on an interval $I$ but whose first derivative fails to be continuous at some point of~$I$.
    We actually provide a class of such functions; some of them are bounded on $I$, some are not.

\V

            \subsection{\small{\bf Examples}}
            \label{ExampE35.130}

\V

        For each $m \,=\, 1,2,\,{\ldots}\,$, let ${\displaystyle f_{m} \,=\, \hat{B}^{1}_{[\frac{1}{m+1},\frac{1}{m}}}$.

        \underline{Comment} Note that $1/m - 1/(m+1) \,=\, 1/(m(m+1))$, so by Part~(b) Lemma~\Ref{LemmaE35.120},
    one sees that the maximum and minimum values of $f_{m}'$ are $ \,{\pm}\, {\displaystyle \frac{16\sqrt{3}m(m+1)}{9}}$,
    and these extreme values of $f'_{m}$ occur inside the interval $[1/(m+1),1/m]$.

\V

        (1) Define $g:{\RR} \,{\rightarrow}\, {\RR}$ by the rule
        \begin{displaymath}
        g(x) \,=\, \left\{
        \begin{array}{cl}
           0     & \mbox{if $x\,\,{\leq}\,\,0$ or $x\,>\,1$} \\
        \frac{1}{m^{3/2}}f_{m}(x) & \mbox{if ${\displaystyle \frac{1}{m+1}\,<\,x\,\,{\leq}\,\,\frac{1}{m}}$ for some $m \,=\, 1,2,\,{\ldots}\,$}.
        \end{array}
                                        \right.
        \end{displaymath}
    It is easy to see from Theorem~\Ref{ThmE35.20} above that the function $g$ is $C^{1}$ at each point $x$ of ${\RR}$,
    with the possible exception of $x \,=\, 0$.
    For $x \,\,{\neq}\,\, 0$ consider the corresponding difference quotient at~$0$:
        \begin{displaymath}
        \frac{g(x)-g(0)}{x-0} \h ({\ast})
        \end{displaymath}


    \h (i), Suppose $x\,<\,0$. Then the quotient in~$({\ast})$ reduces to $(0-0)/(x-0) \,=\, 0$ for all $x\,<\,0$, so the left-hand derivative of $g$ at $0$ exists, and equals~$0$.

        \h (ii) Suppose $x\,>\,0$. Since the object of interest is $g'(0)$, without loss of generality one may assume that $0\,<\,x\,\,{\leq}\,\,1$,
    and thus $x{\in}(1/(m+1),1/m]$ for some $m \,=\, 1,2,\,{\ldots}\,$.
    In that case, since $0\,\,{\leq}\,\,f_{m}(x)\,\,{\leq}\,\,1$ and $1/(m+1)\,<\,x$, one has
        \begin{displaymath}
        0\,\,{\leq}\,\,\frac{g(x)-g(0)}{x-0} \,=\, \frac{1}{m^{3/2}}\frac{f_{m}(x)}{x}\,\,{\leq}\,\, \frac{1}{m^{3/2}}{\cdot}\frac{1}{1/(m+1)} \,=\, \frac{m+1}{m^{3/2}} \,=\, 
    \frac{1}{m^{1/2}} + \frac{1}{m^{3/2}}
        \end{displaymath}
    It now follows easily, using the Squeeze Theorem, that the right-hand derivative of $g$ at $0$ exists and equals~$0$.
    Thus $g'(0)$ exists and equals~$0$.


        In contrast, the function $g'$ is {\em not} continuous at~$0$.
    Indeed, by the `Comment' made above one sees that the maximum value of $g'$ on an interval $[1/(m+1),1/m]$ is ${\displaystyle \frac{16\sqrt{3}m(m+1)}{9m^{3/2}}}$, which clearly diverges to $+{\infty}$ as $m$ increases; that is, $g'$ is actually unbounded above on any open interval containing $0$, so $g'$ certainly cannot be continuous at $0$.
    (A similar argument shows that $g'$ is unbounded below near~$0$.)

\V

        (2) If, in the previous example, one replaces the factor $1/m^{3/2}$ by $1/m^{2}$,
    one gets a function $g$ which is differentiable on ${\RR}$ and even has {\em bounded} first derivative on ${\RR}$, but still fails to be continuous at~$0$.
    Note that in this case ${\sup}\,\{g'(x):x{\in}{\RR}\} \,=\, 16\sqrt{3}/9$, but $g'$ never actually takes on this value; a similar statement holds for ${\inf}\,\{g'(x):x{\in}{\RR}\}$.
}%\EndSkip
%----------------------------------- C

\VV

                \section{{\bf Significance of the First Derivative}}
                \label{SectE40}\IndB{ZZ Sections}{\Ref{SectE40} Significance of the First Derivative}

\VV

        As one knows from elementary calculus, the derivative $f'$ of a function $f$ on an interval
    contains a great deal of information about the behavior of $f$ on that interval.
    Much of that information is obtained through a study of the sign of the derivative; that is, by determining whether $f'$ is positive or negative.

        \underline{Note} The approach followed in this section and the next differs substantially from that found in standard calculus texts.
    Indeed, the approach here is much closer to the original treatment of Lagrange in the eighteenth century,
    but with the gaps in his arguments filled in.

\VV


            \subsection{\small{\bf Theorem} (First-Derivative Test for the Nonexistence of Extrema at a Point)}\IndBD{derivatives}{sign of the derivative at a point}
            \label{ThmE20.50}

\V

        Let $f:[a,b] \,{\rightarrow}\, {\RR}$ satisfy $f'(x_{0}) \,\,{\neq}\,\, 0$ at a point $x_{0}$ of a closed interval~$[a,b]$.
    If $x_{0}$ is either of the endpoints $a$ or $b$, then $f'(x_{0})$ is the appropriate one-sided derivative.

\V

        (a) Suppose that $a\,\,{\leq}\,\,x_{0}\,<\,b$. If $f'(x_{0})\,>\,0$, then $f$ does not have a maximum for $[a,b]$ at~$x_{0}$,
    while if $f'(x_{0})\,<\,0$, then $f$ does not have a minimum for $[a,b]$ at~$x_{0}$.

\V

        (b) Suppose that $a\,<\,x_{0} \,\,{\leq}\,\, b$. If $f'(x_{0})\,>\,0$, then $f$ does not have a minimum for $[a,b]$ at~$x_{0}$,
    while if $f'(x_{0})\,<\,0$, then $f$ does not have a maximum for $[a,b]$ at~$x_{0}$.

\V

        (c) Suppose that $a\,<\,x_{0}\,<\,b$. Then $f$ has neither a maximum nor a minimum for $[a,b]$ at~$x_{0}$.

\V

        {\bf Proof}\, (a) Note that
        \begin{displaymath}
        f'(x_{0}) \,=\, f'_{+}((x_{0})) \,=\, \lim_{x{\searrow}x_{0}} \frac{f(x) - f(x_{0})}{x-x_{0}}.
        \end{displaymath}
    In particular, the fraction $(f(x)-f(x_{0}))/(x-x_{0})$ has the same sign as $f'(x_{0})$ provided $x\,>\,x_{0}$ is sufficiently near~$x_{0}$.
    In this case one has $x-x_{0}\,>\,0$, and it follows that $f(x)-f(x_{0})$ has the same sign as $f'(x_{0})$.
    For such $x$ one then has $f(x)\,>\,f(x_{0})$ if $f'(x_{0})\,>\,0$, while $f(x)\,<\,f(x_{0})$ if $f'(x_{0})\,<\,0$.
    In the former case $f'(x_{0})$ cannot be the maximum value of $f$ on $[a,b]$,
    while in the latter case it cannot be the minimum value of $f$ on~$[a,b]$, as claimed.

\V

        (b) A similar proof applies.

\V

        (c) This follows immediately from Parts (a) and~(b). \Q


\VV

        {\bf Remark}\,Part~(c) of the preceding result is often phrased in the following equivalent form:

\V

            \subsection{\small{\bf Corollary} (First-Derivative Test for Extrema)}\IndBD{derivatives}{first-derivative test for extrema}
            \label{CorE20.60}

        Suppose that $f:I \,{\rightarrow}\, {\RR}$ is defined on an open interval $I$ and differentiable at a point $c$ of $I$.
    If $f$ assumes an extreme value (i.e., maximum or minimum) for $I$ at $c$, then $f'(c) \,=\, 0$.

        Alternate phrasing: A {\em necessary} condition for such $f$ to assume an extreme value at $c$ is that $f'(c) \,=\, 0$.

\V

            \subsection{\small{\bf Remarks}}
            \label{RemrkE20.65}

\V

\hspace*{\parindent}(1) The name `First-Derivative Test for Extrema' given to the preceding corollary is completely standard in calculus texts.
    \IndBD{functions}{first-derivative test for extrema} Unfortunately, the name is also confusing,
    especially to students, since in fact one can {\em never} use this `test' -- by itself --
    to conclude that a function has a maximum or minimum value at a particular interior point:
    it is a {\em necessary}, but not a {\em sufficient},
    condition for this to occur. Many students in elementary calculus have lost points on examinations because of this confusion.

        In contrast, it makes good sense to call Theorem~\Ref{ThmE20.50} the `First-Derivative Test for the Nonexistence of Extrema at a Point'.
    \IndBD{functions}{first-derivative test for nonexistence of extrema} Indeed, this  result {\em does}
    provide sufficient conditions for a function to not have a extremum at a point. 

\V

        (2) A point $c$ in an open interval $I$ at which $f'(c) \,=\, 0$ is called a {\bf critical point of~$\Bfm{f}$}.
    \IndBD{functions}{critical point of a differentiable function}

        \underline{Note} Some texts say that $c$ is a critical point of $f$ if either $f$ is differentiable at $c$ and $f'(c) \,=\, 0$,
    or $f$ is defined, but not differentiable, at~$c$. We avoid this usage in {\ThisText}.

\VV

        In real-life mathematics the subject of `Differential Calculus' is concerned primarily with functions
    that are differentiable at each point of an interval, and not just at isolated points. The remainder of this section is devoted primarily to such functions.

\VV

            \subsection{\small{\bf Theorem} (Significance of the Sign of the First Derivative on an Interval)}
            \label{ThmE40.30}\IndBD{derivatives}{sign of the derivative on an interval}

        Let $f:X \,{\rightarrow}\, {\RR}$ be a real-valued function with domain $X \,{\subseteq}\, {\RR}$,
    and suppose that $f'(x)$ is defined, and nonzero, at each point $x$ of an interval $I \,{\subseteq}\, X$. Then:

\V

        (a) The derivative function $f'$ is of constant sign on~$I$. More precisely, either $f'(x)\,>\,0$ for all $x$ in~$I$, or $f'(x)\,<\,0$ for all $x$ in~$I$;
    equivalently, if $a$ and $b$ are numbers in the interval~$I$, then the nonzero numbers $f'(a)$ and $f'(b)$ are of the same sign.

\V

        (b) If $f'(x)\,>\,0$ for all $x$ in $I$, then the original function $f$ is strictly increasing on~$I$.
    Likewise, if $f'(x)\,<\,0$ for all $x$ in~$I$, then $f$ is strictly decreasing on~$I$.
    In either case, the image $J \,=\, f[I]$ of the interval $I$ under $f$ is an interval of the same type as~$I$:
    either both are open intervals, both are closed intervals, or both are half-open intervals.
    Furthermore, the map $f:I \,{\rightarrow}\, J$ is a bijection.

\V

        {\bf Proof} (a) Let $a$ and $b$ be points of $I$ such that $a \,\,{\neq}\,\, b$;
    without lose of generality, suppose that they are labelled so that $a\,<\,b$.
    Since $f$ is differentiable on the interval~$I$, by Theorem~\Ref{ThmE20.30} it is certainly continuous on the closed and bounded subinterval~$[a,b]$.
    Thus, by the Extreme-Value Theorem, it must assume both a maximum and a minimum value for $[a,b]$ in~$[a,b]$.
    The hypothesis that $f'(x)$ is never $0$ for $x$ in the interval~$I$, combined with Part~(c) of the preceding theorem, implies that these extreme values cannot be assumed at an interior point of~$[a,b]$.
    If $f'(a)\,>\,0$, then by Part~(a) of that theorem it follows that that this maximum cannot occur at~$a$, so it must occur at $b$, and nowhere else.
    Likewise, the minimum of $f$ on $[a,b]$ must occur at $a$, and nowhere else. In particular, $f(a)\,<\,f(b)$.
    It then follows from Part~(b) of the same theorem that $f'(b)$ cannot be negative, so one must have $f'(b)\,>\,0$ as well.

        A similar argument shows that if $f'(a)\,<\,0$ then $f'(b)\,<\,0$. In either case, $f'(a)$ and $f'(b)$ must have the same sign, as claimed.

\V

        (b) Assume, to be definite, that $f'(x)\,>\,0$ for all $x$ in~$I$, and again let $a$ and $b$ be any numbers in $I$ such that $a\,<\,b$.
    Then from the proof of Part~(a) just given, it follows that $f(a)\,<\,f(b)$. Since $a$ and $b$ are arbitrary numbers in $I$ with $a\,<\,b$, it follows that $f$ is strictly increasing on~$I$, as claimed.

        If, instead, $f'(x)\,<\,0$ for every $x$ in~$I$, apply what was just proved to the function $g \,=\, -f$.

        The remaining claims, that the image set $J$ is an interval of the same tpe as~$I$,
    and that $f:I \,{\rightarrow}\, J$ is a bijection of $I$ onto~$J$, follow from Theorem~\Ref{ThmD25.55C}.

\V

        {\bf Remark} The proofs just given are different from -- and more direct than -- the proofs of the same results found in nearly every calculus text.
    Indeed, those proofs are based on the so-called `Mean-Value Theorem' (see below), a result that students find difficult to understand. The genesis of the proofs here is a paper by L.~Bers; see [BERS~1967].

\VV

        The next result is a simple application of Part~(a) of the preceding result. It can be of considerable use in analysis, but is often treated as a mere curiosity.
    Standard texts in elementary calculus rarely even state it; indeed, apparently it went unobserved until Darboux proved it around~$1875$.
    
\V

            \subsection{\small{\bf Theorem} (The Intermediate-Value Theorem for Derivatives)}\IndBD{derivatives}{intermediate-value theorem for derivatives}
            %\label{ThmE50.10}
            \label{ThmE40.35}

\V

        Suppose that $f$ is differentiable at each point of an open interval $I$ in~${\RR}$.
    Let $m \,=\, {\inf}\,\{f'(x): x{\in}I\}$ and $M \,=\, {\sup}\,\{f'(x): x{\in}I\}$; we allow the possibilities $m \,=\, -{\infty}$ and $M \,=\, +{\infty}$.
    If $p$ is a number such that $m\,<\,p\,<\,M$, then there exists a number $c$ in $I$ such that $f'(c) \,=\, p$.

\V

        \underline{Proof} (by contradiction):  Suppose that there is $p$ such that $m\,<\,p\,<\,M$ but there is no $c$ in $I$ which satisfies $f'(c) \,=\, p$.
    By the Approximation Properties for infimum and supremum, there exist numbers $q$ and $r$ in the set $S \,=\, \{f'(x): x{\in}I\}$ such that $q\,<\,p\,<\,r$.
    By definition of the set~$S$, this implies that there exist numbers $a$ and $b$ in $I$ such that $f'(a) \,=\, q$ and $f'(b) \,=\, r$,
    hence for which $f'(a)\,<\,p\,<\,f'(b)$. Define $g:I \,{\rightarrow}\, {\RR}$ by the rule $g(x) \,=\, f(x) - p\,x$ for each $x$ in~$I$.
    Since $g'(x) \,=\, f'(x)-p$, it follows from the `contradiction hypothesis' on $p$ that for each $x$ in $I$ one has $g'(x) \,\,{\neq}\,\, 0$.
    It then follows from Part~(a) of Theorem~\Ref{ThmE40.30} that either $g'(x)\,>\,0$ for all $x$ in~$I$, or $g'(x)\,<\,0$ for all $x$ in~$I$.
    That is, either (i)\, $f'(x)\,>\,p$ for all $x$ in~$I$; or (ii) $f'(x)\,<\,p$ for all $x$ in~$I$.
    Statement~(i) contradicts the fact that $q\,<\,p$: set $x \,=\, a$. Likewise, Statement~(ii) contradicts the fact that $p\,<\,r$: set $x \,=\, b$. \Q

\VV


            \subsection{\small{\bf Remarks}} 
            \label{RemrkE40.35B}

\V

\hspace*{\parindent} (1) If $f'$ is constant on~$I$, then $m \,=\, M$, so that the hypothesis, `$p$ is anumber such that $m\,<\,p\,<\,M$', of the claimed implication is never true,
    hence in this case the claim itself is automatically true.
    Of course if $f'$ is {\em not} constant on~$I$, then $m\,<\,M$ so the hypothesis $m\,<\,p\,<\,M$ holds for infinitely many values of~$p$.

\V

        (2) In light of the Bolzano Endpoint-Principles, this result can be rephrased as follows:
    Suppose that $f$ is differentiable at each point of an open interval~$I$, and let $J$ be the image of $I$ under the derivative function~$f'$.
    Then either $J$ is a singleton set (if $f'$ is constant on~$I$), or $J$ is also an interval (if $f'$ is not constant on~$I$), although not necessarily an open one.

\V

        (3) If the function $f'$ were continuous on $I$, then the conclusion of the preceding result
    would follow directly from the standard Intermediate-Value Theorem for continuous functions. The fact that there exist functions $f$ which are differentiable on $I$,
    but for which $f'$ fails to be continuous on~$I$ -- see Remark~\Ref{RemrkE20.87}~(1) above -- shows that the current result is not trivial.

\V

\StartSkip{
        (4) Darboux's theorem also shows that a necessary condition for a function to have an antiderivative on an interval,
    it must possess the appropriate `intermediate-value' property. For example, a function which is continuous on an interval,
    except for a single jump discontinuity, cannot have an antiderivative which works over the entire interval.
} %EndSkip

\VV

        The next result provides a useful application of Theorem~\Ref{ThmE40.35}.

\V

            \subsection{\small{\bf Corollary} (The Extended Intermediate-Value Theorem for Derivatives)}\IndBD{derivatives}{intermediate-value theorem for derivatives, extended}
            \label{CorE40.35C}

\V


        Suppose that $f$ and $g$ are differentiable on an open interval $I$. Assume, in addition, that $g'(x)\,>\,0$ for each $x$ in~$I$.
    Let $Q(x) \,=\, f'(x)/g'(x)$ for each $x$ in~$I$.
    Suppose that $c$ and $d$ are numbers in~$I$ such that $c\,<\,d$ and $Q(c) \,\,{\neq}\,\, Q(d)$.
    Then for each number $p$ between $Q(c)$ and $Q(d)$ there exists a number $q$ between $c$ and $d$ such that $Q(q) \,=\, p$;
    that is, ${\displaystyle \frac{f'(q)}{g'(q)}\,=\,p}$.

\V

        {\bf Proof} The hypothesis on $g'$ implies that the image of $I$ under $g$ is an open interval $J$ and that $g$ has an inverse $g^{-1}$ on~$J$.
    Let $h(u) \,=\, (f{\circ}g^{-1})(u)$ for all $u$ in~$J$. By the Chain Rule and the rule for the derivative of an inverse, one has
        \begin{displaymath}
        h'(u) \,=\, f'(g^{-1}(u)){\cdot}(g^{-1})'(u) \,=\, \frac{f'(g^{-1}(u))}{g'(g^{-1}(u))}.
        \end{displaymath}
    Since every number $x$ in $I$ can be expressed in exactly one way in the form $x \,=\, g^{-1}(u)$ with $u$ in~$J$,
    it follows that for each $x$ in $I$ one has $f'(x)/g'(x) \,=\, h'(u)$ for some $u$ in~$J$.
    The desired result now follows by applying the usual IVT-D, Theorem~2, to the function~$h'$ on the open interval~$J$.


\VV

        With a little more work, one can also refine Theorem~\Ref{ThmE40.30}.

\V

            \subsection{\small{\bf Theorem}}
            \label{ThmE40.40}

\V

\hspace*{\parindent}(a) Suppose that $f'(x)\,\,{\geq}\,\,0$ at each point $x$ of some open interval $I$ in ${\RR}$. Then $f$ is monotonic up on $I$.

\V

        (b) Likewise, suppose that $f'(x)\,\,{\leq}\,\,0$ at each point $x$ of some open interval $I$ in ${\RR}$. Then $f$ is monotonic down on $I$

\V

        (c) In both Part~(a) and in Part~(b), suppose that there exist numbers $x_{1}$ and $x_{2}$ in $I$, with $x_{1}\,<\,x_{2}$,
    such that $f(x_{1}) \,=\, f(x_{2})$. Then $f$ is constant on the closed interval~$[x_{1},x_{2}]$.

\V

        (d) Supppose, in contrast, that $g$ is a function such that $g'(x)$ exists at each point $x$ of the interval~$I$,
    but $g'$ is {\em not} of constant sign on $I$; that is, there exist points $x_{1}$ and $x_{2}$ in $I$ such that $g'(x_{1})\,<\,0$ and $g'(x_{2})\,>\,0$.
    Then $g$ is {\em not} monotonic, nor is $g$ one-to-one, on~$I$.

\V

        \underline{Proof}

\V

        (a) Let $k$ be any natural number, and let $g_{k}:I \,{\rightarrow}\, {\RR}$ be given by the rule $g_{k}(x) \,=\, x/k + f(x)$ for each $x$ in~$I$. 
    Then $g_{k}$ is differentiable on~$I$, and $g'(x) \,=\, 1/k+f'(x)\,\,{\geq}\,\,1/k\,>\,0$ since $f'(x)\,\,{\geq}\,\,0$.
    It follows from Part~(b) of Theorem~\Ref{ThmE40.30} that $g_{k}$ is strictly increasing on~$I$.
    In particular, if $x_{1}$ and $x_{2}$ in $I$ satisfy $x_{1}\,<\,x_{2}$, then $g_{k}(x_{2})-g(x_{1})\,>\,0$. This fact can be written as
        \begin{displaymath}
        \frac{x_{2} - x_{1}}{k} + (f(x_{2})-f(x_{1}))\,>\,0 \mbox{ for all 
${\varepsilon}\,>\,0$}.
        \end{displaymath}
    Let the index $k$ approach $+{\infty}$ and use standard limit laws for sequences to get $f(x_{2})-f(x_{1})\,\,{\geq}\,\,0$;
    that is, $f(x_{1})\,\,{\leq}\,\,f(x_{2})$, as claimed.
    

\V

        (b) Apply the results of Part~(a) to the function $-f$.

\V

        (c) This is an obvious property of functions which are monotonic on an interval.

\V

        (d) Without loss of generality assume that $x_{1}\,<\,x_{2}$. By the Extreme-Value Theorem,
    the continuous function $g$ assumes both its maximum and minimum values for the interval $[x_{1},x_{2}]$ somewhere on $[x_{1}, x_{2}]$.
    Since, by hypothesis, one has $g'(x_{1})\,<\,0$ and $g'(x_{2})\,>\,0$, it follows from Parts~(a) and~(b) of Theorem~\Ref{ThmE20.50}
    that the minimum value cannot occur at either $x_{1}$ or $x_{2}$, and thus must occur at some point $c$ such that $x_{1}\,<\,c\,<\,x_{2}$.
    In particular, one has $g(x_{1})\,>\,g(c)$ and $g(c)\,<\,g(x_{2})$. The former inequality implies that $g$
    is not monotonic up on the subinterval $[x_{1},c]$, while the latter implies that $g$ is not monotonic down on~$[c,x_{2}]$.
    It follows that $g$ is certainly not monotonic on the full interval~$I$.

        It is now an easy exercise to show that there must exist points $u_{1}$ and $u_{2}$, with $x_{1}\,<\,u_{1}\,<\,c\,<\,u_{2}\,<\,x_{1}$,
    such that $g(u_{1}) \,=\, g(u_{2})$, so that $g$ is not one-to-one on~$I$, as claimed. \Q 

%% EXERCISE

\V

            \subsection{\small{\bf Corollary}}
            \label{CorE40.50}

\V

        Suppose that $f$ satisfies the equation $f'(x) \,=\, 0$ for all points of an open interval $I$ in ${\RR}$. Then $f$ is constant on $I$.
    (Of course, the converse is also true: if $f$ is constant on~$I$, then $f'(x) \,=\, 0$ for all $x$ in~$I$.)

\V

        \underline{Proof} Note that the hypothesis implies that $f'(x)\,\,{\geq}\,\,0$ for all $x$ in $I$,
    and that $f'(x)\,\,{\leq}\,\,0$ for all $x$ in $I$.
    Thus by Part~(a) of the preceding theorem it follows that $f$ is monotonic up on $I$,
    while by Part~(b) of that theorem it follows that $f$ is monotonic down on $I$. The only way this can happen is if $f$ is constant on $I$. \Q

\V

            \subsection{\small{\bf Corollary}}
            \label{CorE40.65}

\V

        Let $I$ be an open interval, and suppose that $f,g:I \,{\rightarrow}\, {\RR}$ are differentiable at each point of $I$.

\V

        (a) Assume that $f'(x)\,\,{\leq}\,\,g'(x)$ for all $x$ in the open interval $I$. Then
        \begin{displaymath}
        f(x_{2})-f(x_{1})\,\,{\leq}\,\,g(x_{2})-g(x_{1})
        \end{displaymath}
    for all $x_{1}$, $x_{2}$ in $I$ such that $x_{1}\,\,{\leq}\,\,x_{2}$.

\V

        (b) Assume further that there exist $x_{1}$ and $x_{2}$ in the interval $I$, with $x_{1}\,<\,x_{2}$,
    such that $f(x_{2})-f(x_{1}) \,=\, g(x_{2})-g(x_{1})$. Then $g - f$ is constant on the closed interval $[x_{1},x_{2}]$;
    equivalently, $f' \,=\, g'$ on~$[x_{1},x_{2}]$. In particular, if in addition there exists at least one number $c$ in $[x_{1},x_{2}]$
    such that $f(c) \,=\, g(c)$, then $B \,=\, 0$ and $f \,=\, g$ on~$[x_{1},x_{2}]$.

\V

        (c) The corresponding results obtained by replacing the inequality `$\,\,{\leq}\,\,$' throughout (a) and (b) with `$\,\,{\geq}\,\,$' are also true.

\V

        \underline{Proof} Apply Theorem~\Ref{ThmE40.40} and Corollary~\Ref{CorE40.50} to the function $h \,=\, g-f$,
    then transpose terms in the obvious way. \Q

\VV

\StartSkip{

        It is useful to separately reformulate some of the preceding results in terms of the important concept of `antiderivative'.

\V

            \subsection{\small{\bf Corollary}}
            \label{CorE40.65A}

\V

        Suppose that $h:I \,{\rightarrow}\, {\RR}$ is a function which has an antiderivative $H$ on an open interval~$I$.


\V

        (a) Assume that $h(x)\,\,{\geq}\,\,0$ for all $x$ in~$I$. If $x_{1}$ and $x_{2}$ are points of $I$ such that $x_{1}\,<\,x_{2}$,
    then $H(x_{1})\,\,{\leq}\,\,H(x_{2})$. Furthermore, one has $H(x_{1}) \,=\, H(x_{2})$ if, and only if, $H$ is constant on the interval~$[x_{1},x_{2}]$.
    A similar result holds if, instead, $h(x)\,\,{\leq}\,\,0$ for all $x$in~$I$.

\V

        (b) Every antidrivative of $h$ on $I$ is of the form $H+C$ for some constant function~$C$.
    In particular, if $H_{1}$ and $H_{2}$ are antiderivatives of $h$ on $I$, then for every $x_{1}$ and $x_{2}$
    in $I$ one has $H_{2}(x_{2}) - H_{2}(x_{1}) \,=\, H_{1}(x_{2})-H_{1}(x_{1})$.

\V

        (c) If $c$ is any point in the interval~$I$, then there exists a unique antiderivative $H$ of $h$ on $I$ such that $H(c) \,=\, 0$.
    More precisely, $H$ is given by the rule $H(x) \,=\, \hat{H}(x) - \hat{H}(c)$ for each $x$ in $I$,
    where $\hat{H}$ can be {\em any} antiderivative of $h$ on~$I$.

\V

        The simple proof is left as an exercise. \Q

\V

            \subsection{\small{\bf Remarks}}
            \label{RemrkE40.65AB}

\V

        (1) It is clear that Part (c) of the previous corollary can be strengthened to say that if $B$ is any number,
    then there is a unique antiderivative $H$ of $h$ on $I$ such that $H(c) \,=\, B$;
    indeed, simply set ${H}(x) \,=\, \hat{H}(x) - \hat{H}(c) + B$ for each $x$ in~$I$, where $\hat{H}$ is any antiderivative of $h$ on~$I$.
    For our purposes the simplest case $B \,=\, 0$ is good enough.

\V

        (2) The antiderivative $H \,=\, \hat{H} - \hat{H}(c)$ described above in Part~(c) is called the {\bf $\Bfm{c}$-antiderivative of $\Bfm{h}$ on~$\Bfm{I}$}.
    More generally, if $n$ is any natural number, then the {\em $\Bfm{n}$-th order $\Bfm{c}$-antiderivative of $\Bfm{h}$ on~$\Bfm{I}$}
    is the (necessarily unique) $k$-th order antiderivative $H$ of $h$ on $I$ such that $H^{(j)}(c) \,=\, 0$ for each $j \,=\, 0, 1, \,{\ldots}\,n-1$.
    \IndBD{antiderivatives}{$n$-th order $c$-antiderivative}.

\V

            \subsection{\small{\bf Examples}}
            \label{ExampE40.65B}

\V

\hspace*{\parindent}(1) Let $A$ be a real number and let $h:I \,{\rightarrow}\, {\RR}$ be the corresponding constant function given by $h(x) \,=\, A$ for all $x$ in an open interval~$I$. Then for each $c$ in $I$
    the unique antiderivative of $H$ on $I$ which equals $0$ at $c$ is given by $H(x) \,=\, A\,x - A\,c \,=\, A\,(x-c)$ for all $x$ in~$I$.

\V

        (2) The (unique) antiderivative of the function $H$, obtained above, which equals $0$ at $c$
    is given by the formula ${\displaystyle A\,\left(\frac{(x-c)^{2}}{2}\right)}$ for all~$x$ in~$I$.

\V

        (3) More generally, if one repeats this process, namely finding the antiderivative vanishing at $c$ of the most recently found function,
    a total of $k$ times, where $k$ can be any natural number, the result is the function given by ${\displaystyle A\,\left(\frac{(x-a)^{k}}{k\,!}\right)}$.

\VV

            \subsection{\small{\bf Remarks}}
            \label{RemrkE40.65C}

\V

\hspace*{\parindent}(1) Part (b) of Corollary~\Ref{CorE40.65A} shows that if $H$ is an antiderivative of a function $h$ on an open interval~$I$,
    then the value of $H$ at any individual point of $I$ has no intrinsic connection with $h$; indeed, this value can be any number.
    However, {\em differences} of the form $H(x_{2}) - H(x_{1})$, with $x_{1}$ and $x_{2}$ in~$I$,
    {\em do} describe something intrinsicly related to~$h$; the nature of this relationship becomes clearer later.

\V

        (2) The function $H:I \,{\rightarrow}\, {\RR}$, given in Example~(3) above,
    by ${\displaystyle H(x) \,=\, \frac{A}{n!}\,(x-c)^{n}}$ is the $n$-th order $c$-antiderivative of the constant function~$A$.
} %\EndSkip

\VV

        The preceding results involve functions differentiable on an {\em open} interval.
    The next theorem provides information about what happens when endpoints are involved.

\V

            \subsection{\small{\bf Theorem}}
            \label{ThmE40.66}

\V

        Let $f:(a,b) \,{\rightarrow}\, {\RR}$ and $g:(a,b) \,{\rightarrow}\, {\RR}$ be differentiable on an open interval $(a,b)$,
    where the endpoints $a$ and $b$ satisfy $-{\infty}\,\,{\leq}\,\,a\,<\,b\,\,{\leq}\,\,+{\infty}$;
    recall that the `arrow' notation allows the possibility that $f$ and $g$ are defined at points outside
    the open interval~$(a,b)$ (at least if $a$ or $b$ is finite).

\V

        \underline{Case 1}\, Suppose that the endpoints $a$ and $b$ are finite and that $f$ and $g$ are also defined, and continuous, on the closed interval~$[a,b]$.

        \h (a) If $f'(x)\,<\,g'(x)$ for all $x$ in the open interval~$(a,b)$, then for each $x_{1}$ and $x_{2}$ in $[a,b]$
    such that $x_{1}\,<\,x_{2}$ one has $f(x_{2}) - f(x_{1})\,<\,g(x_{2}) - g(x_{1})$.

\V

        \h (b) If, instead, $f'(x)\,\,{\leq}\,\,g'(x)$ for all $x$ in $(a,b)$, then $f(x_{2})-f(x_{1})\,\,{\leq}\,\,g(x_{2})-g(x_{1})$
    for all $x_{1}$, $x_{2}$ in $[a,b]$ such that $x_{1}\,<\,x_{2}$. Furthermore, if there exist $x_{1}$ and $x_{2}$ in $[a,b]$,
    with $x_{1}\,<\,x_{2}$, such that $f(x_{2}) - f(x_{1}) \,=\, g(x_{2}) - g(x_{1})$,
    then there exists a constant $C$ such that $f(x) \,=\, g(x)+C$ for all $x$ in the closed interval $[x_{1},x_{2}]$.
    In particular, if in addition there exists at least one number $c$ in $[x_{1},x_{2}]$ such that $f(c) \,=\, g(c)$,
    then $C \,=\, 0$ and $f(x) \,=\, g(x)$ for {\em all} $x$ in $[x_{1},x_{2}]$.

\V

        \underline{Case 2}\, Suppose that the left endpoint $a$ is finite and that $h$ is continuous at~$a$.
    Then the conclusions of Case~(1) remain valid if the closed interval $[a,b]$ is replaced throughout that case by the half-open interval~$[a,b)$.
    Similarly, suppose that the right endpoint $b$ is finite and that $h$ is continuous at~$b$.
    Then the conclusions of Case~(1) remain valid if the closed interval $[a,b]$ is replaced throughout that case by the half-open interval~$(a,b]$.

\V

        \underline{Proof}

\V

        \underline{Case 1} When $a\,<\,x_{1}\,<\,x_{2}\,<\,b$ in Parts (a) and (b), the desired results reduce to the case of open intervals obtained above.
    Now let $x_{1}$ approach $a$ from the right and let $x_{2}$ approach $b$ from the left, and use the continuity hypothesis for~$h$.
    To obtain Part~(c), apply Parts~(a) and~(b) to $h \,=\, g-f$.

\V

        \underline{Case 2} The proof is similar (and slightly easier), and is left as an exercise. \Q

\V

        \underline{Remark} Assuming that $h$ is differentiable at the interior points of an interval and merely continuous at the endpoints may seem unduly fussy.
    However, there are many important functions for which these hypotheses are appropriate, for instance, the function $f$ given by
        \begin{displaymath}
        f(x) \,=\, \sqrt{1-x^{2}} \mbox{ for $-1\,\,{\leq}\,\,x\,\,{\leq}\,\,1$}.
        \end{displaymath}
    This function, which arises in the description of the unit circle $x^{2}+y^{2} \,=\, 1$ in ${\RR}^{2}$,
    is continuous on the closed interval $[-1,1]$ and differentiable on the open interval $(-1,1)$; but it is {\em not} differentiable at either endpoint.

\VV

                \section{{\bf Antiderivatives}} 
                \label{SectE45}\IndB{ZZ Sections}{\Ref{SectE45} Antiderivatives}

\V

        In elementary calculus the first important procedure is that of `differentiation': given a function~$F$, determine its derivative~$f \,=\, F'$.
    Students normally have little difficulty becoming proficient at this procedure because of the presence of the computation rules of differentiation discussed above.

        Of equal importance in elementary calculus, but considerably more difficult for students, is the reverse procedure:
    given a function $f$, determine a function $F$ of which $f$ is the derivative. The present section studies this reverse procedure in some depth.

\V

             \subsection{\small{\bf Definition} (Antiderivatives)}
            \label{DefE45.30}\IndB{antiderivatives}{definition}

        Suppose that $f:I \,{\rightarrow}\, {\RR}$ is function defined on an open interval $I$ in ${\RR}$.

\V

        (1) A function $F:I \,{\rightarrow}\, {\RR}$ is said to be {\bf an antiderivative of $f$ on $I$} provided $F'(x) \,=\, f(x)$ for all $x$ in $I$.
    If such $F$ exists, then one also says that $f$ is {\bf antidifferentiable} on $I$.

\V

        (2) More generally, let $k$ be a positive integer. A function $G:I \,{\rightarrow}\, {\RR}$ is said to be a {\bf \IndBB{antiderivatives}{$k$-th order antiderivative} of $f$ on $I$} provided $G^{(k)}(x) \,=\, f(x)$ for all $x$ in $I$.

\V

        (3) The process of calculating an antiderivative of a given function $f$ is called {\bf antidifferentiation} of~$f$.


\VV

             \subsection{\small{\bf Examples}}
            \label{ExampE45.40}

\V

\hspace*{\parindent}(1) Let $f:{\RR} \,{\rightarrow}\, {\RR}$ be given by $f(x) \,=\, 3\,x^{2}$ for all~$x$.
    Then is is clear that $F$, given by $F(x) \,=\, x^{3}$, is an antiderivative of $f$. Why is it so clear?
    Because we have differentiated $x^{3}$ before (for instance as the special case $k \,=\, 3$ of Example~\Ref{ExampE20.70}) and found the result to be~$3\,x^{2}$.
    It is also clear from the simplest differentiation rules that if $C$ is any constant, then the function $G$, given for all $x$ by $G(x) \,=\, x^{3} + C$, is also an antiderivative of~$f$.

\V

        (2) More generally, l et $F$ be any standard function from calculus. Differentiate $F$, and let $f$ be the resulting function.
    Then $F$ is an antiderivative of $f$, as is $F+C$ for any constant~$C$.

\V

        (3) Let $f:{\RR} \,{\rightarrow}\, {\RR}$ be given by the rule
        \begin{displaymath}
        f(x) \,=\, \frac{1}{(1+x^{2})^{3/2}} \mbox{ for all $x$ in ${\RR}$}
        \end{displaymath}


        \underline{Claim} The function $F:{\RR} \,{\rightarrow}\, {\RR}$ given by the rule
        \begin{displaymath}
        F(x) \,=\, \frac{x}{\sqrt{1+x^{2}}} \mbox{ for all $x$ in~${\RR}$},
        \end{displaymath}
    is an antiderivative of~$f$.

        To {\em prove} that this claim is correct is not hard: differentiate the function $F$, using the rules of differentiation, and simpify algebraically.
    The harder question to answer is this: Where did the formula for $F$ come from?

\VV

        (3) Let $f(x) \,=\, |x|$ for all $x$ in ${\RR}$. Then $f(x) \,=\, x$ if $x\,\,{\geq}\,\,0$, and $f(x) \,=\, -x$ if $x\,<\,0$.
    On the open interval $(0,+{\infty})$ the function $f$ has many antiderivatives; namely, any function on $(0,+{\infty})$ of the form $G(x) \,=\, {\displaystyle \frac{x^{2}}{2}+C_{1}}$,
    where $C_{1}$ is constant. Likewise $f$ has infinitely many antiderivatives on the interval $(-{\infty},0)$,
    namely functions of the form $H(x) \,=\, {\displaystyle -\frac{x^{2}}{2}+C_{2}}$.
    To get an antiderivative defined on all of ${\RR}$, choose the constants $C_{1}$ and $C_{2}$ so that $\lim_{x{\nearrow}0} H(x) \,=\, \lim_{x{\searrow}0} G(x)$.
    This simply requires $C_{1} \,=\, C_{2}$, so let us make the simplest choice, namely $C_{1} \,=\, C_{2} \,=\, 0$.
    Then define $F:{\RR} \,{\rightarrow}\, {\RR}$ by the rule
        \begin{displaymath}
        F(x) \,=\, \left\{
        \begin{array}{ccrl}
        H(x) & \,=\, & -x^{2}/2 & \mbox{if $x\,<\,0$} \\
          0  & \,=\, &          & \mbox{if $x\,=\,0$} \\
        G(x) & \,=\, &  x^{2}/2 & \mbox{if $x\,>\,0$}
        \end{array}
                        \right.
        \end{displaymath}
    This is the same function that appears in Example~\Ref{ExampE20.85} above, where it is shown that $F'$ is the absolute-value function.

\VV

        As is indicated in Example~(1) above, the use of the indefinite article `an' in the definition of `an antiderivative' is needed:
    If $F$ is an antiderivative of $f$ on an interval $I$, then for every constant function $C$ the function $F+C$ is also an antiderivative of $F$ on $I$.
    The next result shows that this is the only ambiguity possible for first-order antiderivatives.

\V

             \subsection{\small{\bf Theorem}}
            \label{ThmE45.50}

\V

        Suppose that $F_{1}$ and $F_{2}$ are antiderivatives of $f$ on an open interval $I$.
    Then there exists a constant function $C$ such that $F_{2}(x) \,=\, F_{1}(x) + C$ for all $x$ in $I$.

    Equivalently: The function $F_{2}-F_{1}$ is constant on $I$.

\V

        \underline{Proof} Note that, by Theorem~\Ref{ThmE30.20}, the function $F_{2}-F_{1}$ is differentiable on $I$, and
        \begin{displaymath}
        (F_{1}-F_{2})'(x) \,=\, F_{1}'(x)-F_{2}'(x) \,=\, f(x)-f(x) \,=\, 0
        \end{displaymath}
    for all $x$ in $I$.
    It then follows from Corollary~\Ref{CorE40.50} that $F_{2}-F_{1}$ is constant on $I$, as required. \Q

\V

        \underline{Notes} (1) Because of the preceding result, if $F$ is a particular antiderivative of a given function $f$ on an open interval $I$,
    then it is common to refer to the expression $F+C$ in which $C$ is an `arbitrary constant', 
    as {\bf the \IndBB{antiderivatives}{general} antiderivative of $f$ on $I$}.

\V

        (2) The corresponding ambiguity for higher-order antiderivatives is more complicated, and is considered later; see Theorem~\Ref{ThmE45.110}.

\VV 

             \subsection{\small{\bf Corollary}}
            \label{CorE45.60}

\V

        \hspace*{\parindent}(a) Suppose that $f:I \,{\rightarrow}\, {\RR}$ is a function which has an antiderivative on an open interval $I$.
    Let $c$ be any point in $I$, and let $A$ be any real number.
    Then there exists a unique antiderivative $F$ of $f$ on $I$ such that $F(c) \,=\, A$.

        More precisely, if $G:I \,{\rightarrow}\, {\RR}$ is any antiderivative of $f$ on $I$, then the unique $F$ with this property is given by
        \begin{displaymath}
        F(x) \,=\, A + G(x) - G(c) \h ({\ast})
        \end{displaymath}
    In particular, if $A \,=\, 0$ then the formula reduces to
        \begin{displaymath}
        F(x) \,=\, G(x)-G(c).
        \end{displaymath}

\V

        (b) Suppose that $f:I \,{\rightarrow}\, {\RR}$ is a function defined on an open interval $I$ and that $a$ and $b$ are elements of $I$ with $a\,<\,b$.
    Suppose that there are numbers $x_{1}$, $x_{2}$,\,{\ldots}\,$x_{k}$ with $a\,<\,x_{1}\,<\,x_{2}\,<\,\,{\ldots}\,\,<\,x_{k}\,<\,b$ such that $f$ has an antiderivative on each of the subintervals $[a,x_{1}]$, $[x_{1},x_{2}]$,\,{\ldots}\,$[x_{k-1},x_{k}]$, $[x_{k},b]$.
    Then for each $A$ in ${\RR}$ there exists a unique antiderivative $F:[a,b] \,{\rightarrow}\, {\RR}$ of $f$ on $[a,b]$ such that $F(a) \,=\, A$.
\V

        \underline{Proof}

        (a) \underline{Existence} If $G$ is any antiderivative of $F$ on $I$, then clearly the function $F$ given by Equation~$({\ast})$ is also an antiderivative of $f$, since it differs from $G$ by the constant $A-G(c)$.
    Furthermore, one computes that $F(c) \,=\, A+G(c)-G(c) \,=\, A$, as required.

        \underline{Uniqueness} Suppose that $F_{1}$ and $F_{2}$ are both antiderivatives of $f$ on $I$ such that $F_{1}(c) \,=\, A$ and $F_{2}(c) \,=\, A$.
    By the preceding theorem there exists a constant function $C$ such that $F_{2}(x) - F_{1}(x) \,=\, C$ for all $x$ in $I$.
    In particular, this condition must hold when $x \,=\, c$; that is,
        \begin{displaymath}
        0 \,=\, A-A \,=\, F_{1}(c)-F_{1}(c) \,=\, C,
        \end{displaymath}
    so $C \,=\, 0$ and $F_{2} \,=\, F_{1}$ on $I$, as claimed.

\V

        (b) For notational convenience, set $x_{0} \,=\, a$ and $x_{k+1} \,=\, b$.
    By Part~(a), $f$ has a unique antiderivative $F_{1}:[x_{0},x_{1}] \,{\rightarrow}\, {\RR}$ on $[x_{0},x_{1}]$ such that $F_{1}(x_{0}) \,=\, A$.
     Then by Part~(a) again $f$ has a unique antiderivative $F_{2}:[x_{1},x_{2}] \,{\rightarrow}\, {\RR}$ on $[x_{1},x_{2}]$ such that $F_{2}(x_{1}) \,=\, F_{1}(x_{1})$.
    Continuing on this way, one obtains functions $F_{1}$, $F_{2}$,\,{\ldots}\,$F_{k+1}$ such that

        \h (i)\, for each $j \,=\, 1,2,\,{\ldots}\,k+1$, $F_{j}$ is an antiderivative of $f$ on the subinterval $[x_{j-1},x_{j}]$.

        \h (ii) $F_{1}(x_{0}) \,=\, A$; and for each $j \,=\, 1,2,\,{\ldots}\,k$, $F_{j+1}(x_{j}) \,=\, F_{j}(x_{j})$.

\noindent Now define $F:[a,b] \,{\rightarrow}\, {\RR}$ by the rule
        \begin{displaymath}
        F(x) \,=\, F_{j}(x) \mbox{ if $x{\in}[x_{j-1},x_{j}]$}.
        \end{displaymath}
    It is easy to show by Property~(ii) above that $F(x)$ is well-defined at all $x$ in $[a,b]$, even at $x \,=\, x_{j}$.
    It is also easy to show by using one-sided limits at the points $x_{j}$ that $F'(x) \,=\, f(x)$ for all $x$ in $[a,b]$; the details are left to the reader.
    The desired result now follows. \Q

\VV

             \subsection{\small{\bf Examples}}
            \label{ExampE45.65}
\V

\hspace*{\parindent}(1) Suppose that $f:{\RR} \,{\rightarrow}\, {\RR}$ is a monomial, in the sense of high-school algebra;
    that is, there is a nonzero constant $a$ and a nonnegative integer $k$ such that $f(x) \,=\, a\,x^{k}$ for all $x$ in~${\RR}$.
    (Note that if $k$ is even, then $f(-x) \,=\, f(x)$ for all $x$, while if $k$ is odd then $f(-x) \,=\, -f(x)$ for all~$x$.)
    The function $f$ has a unique antiderivative $F$ on ${\RR}$ which is also a monomial, namely the antiderivatives $F$ such that $F(0) \,=\, 0$.
    Indeed, this antiderivative is given by $F(x) \,=\, b\,x^{k+1}$, where $b \,=\, a/(k+1)$.
    Note that if $f$ is a monomial of even degree, then $F$ is a monomial of odd  degree. Similarly, if $f$ is of odd degree, then $F$ is of even degree.
   In the former case one has $f(-x) \,=\, f(x)$ and $F(-x) \,=\, -F(x)$, while in the latter case one has $f(-x) \,=\, -f(x)$ and $F(-x) \,=\, F(x)$.

\V

        (2) More generally, suppose that $I$ is an open interval of the form $(-b,b)$, where $b\,>\,0$; the case $b \,=\, +{\infty}$ is allowed.
    Recall, from high-school algebra, that a function $g:I \,{\rightarrow}\, {\RR}$ is said to be an
    {\bf even function on $\Bfm{I}$}\IndBD{functions}{even, odd functions} provided $g(-x) \,=\, g(-x)$ for all $x$ in~$I$.
    Likewise, a function $h:I \,{\rightarrow}\, {\RR}$ is said to be an {\bf odd function on $\Bfm{I}$} provided $h(-x) \,=\, -h(x)$ for all $x$ in~$I$.

        Suppose that $f:I \,{\rightarrow}\, {\RR}$ is a real-valued function with domain~$I$, and that $f$ has an antiderivative on~$I$.
    Let $F:I \,{\rightarrow}\, {\RR}$ be the unique antiderivative of $f$ on~$I$ such that $F(0) \,=\, 0$.

\VA

        \h (i)\, If $f$ is an even function on the interval~$I$, then $F$ is an odd function on~$I$.

        \h (ii) If $f$ is an odd function on~$I$, then $F$ is an even function on~$I$.

\VA

\noindent Indeed, consider the functiom $G:I \,{\rightarrow}\, {\RR}$ given by the rule $G(x) \,=\, F(-x)$ for all $x$ in~$I$.
    By the Chain Rule one sees that $G'(x) \,=\, -F'(-x) \,=\, -f(-x)$, so that $(-G)'(x) \,=\, f(-x)$.

        In Case~(i) one has $f(-x) \,=\, f(x)$, so that $(-G)'(x) \,=\, f(x)$. Since clearly $-G(0) \,=\, 0$,
    it follows that in this case one has $-G(x) \,=\, F(x)$; that is, $-F(-x) \,=\, F(x)$, which implies that $F$ is an odd function, as claimed.
    Likewise, in Case~(ii) one gets $(-G)'(x) \,=\, -f(x)$, which implies that $F(-x) \,=\, F(x)$, so that $F$ is an even function, as claimed.

\VV

             \subsection{\small{\bf Remarks}}
            \label{RemrkE45.70}
\V

\hspace*{\parindent}(1) The use of the word `antidifferentiation', to indicate the process opposite to the process of `differentiation', seems reasonable;
    likewise for naming the result of that process an `antiderivative'. Indeed, the `antidifferentiation/antiderivative' terminology
    seems to be universal in the modern calculus texts.

    However, for most of the centuries since calculus was first developed the words used instead of `antidifferentiation' and `antiderivative' were 
    `integration' and `integral'; some authors used the word `primitive' instead `integral', and sometimes the word `indefinite' is used before `integral'.

        Why is it important for you, the modern reader, to know this? Because the `integration/integral' terminology for these concepts is still widely used;
    thus if you encounter it, you need to know what it means. Furthermore, although the use of the `antiderivative' {\em terminology} ,
    the {\em notation} used in connection with this concept remains stuck in the eighteenth century.
    More precisely, the way one writes the statement `$F+C$ is the general antiderivative of $f$' in mathematical symbols is this:
        \begin{displaymath}
        F(x)+C \,=\, \int\, f(x)\,dx
        \end{displaymath}
    In the expression ${\displaystyle \int\, f(x)\,dx}$ the symbol ${\displaystyle \int}$ is called the {\bf integral sign},
    the function $f(x)$ is called the {\bf integrand}, and the `arbitrary constant' $C$ is called the {\bf constant of integration}.
    Even in those elementary-calculus textbooks which consistently use the `antiderivative' terminology,
     the section in which one learns how to compute antiderivatives is normally titled something like `Techniques of Integration',
    not `Techniques of Antidifferentiation'.

\V

        (2) There is good reason to assert that the fundamental goal of elementary single-variable calculus is this:

\VA

    \h {\em Given a function $f$, to find its derivative;
    and given a function $g$, to find its (general) antiderivative.}

\VA

\noindent Indeed, this is the significance of Chapter Quote~{1} at the start of this chapter, namely 

\VA

        \h `{\em 6accd{\ae}13eff7i3l9n4o4qrr4s8t12vx}'.

\VA

\noindent Isaac Newton, one of the cofounders of calculus in the seventeenth century, used these letters as an anagram to disguise the following statement:

\begin{quotation}
{\footnotesize
      {\em Data {\ae}quatione quotcunque fluentes quantitates involvente, fluxiones invenire: et vice vers\^{a}}.
}%EndFootNoteSize
\end{quotation}

    That is,
\begin{quotation}
{\footnotesize 
    \em Given an equation involving any number of fluent quantities, to find the fluxions: and vice versa.
}%EndFootNoteSize
\end{quotation}

\noindent In Newton's terminology, a `fluxion' is the rate of change (with respect to time) of a quantity; that is, a derivative;
    and a `fluent' is the quantity whose change produces a give fluxion; that is, an antiderivative.
    Thus, according to Newton the purpose of calculus is to take derivatives and find antiderivatives.



\VV

       The concepts of derivative and antiderivative are so intimately related that it should not be a surprise that for each fact realting to the one concept there is a corresponding fact relating to the other.
    The next result restates Corollary~\Ref{CorE40.65} in a format which handles the existence of more than one antiderivative in a simple manner.

\V

             \subsection{\small{\bf Corollary (of Corollary~\Ref{CorE40.65})}}
            \label{CorE45.75}

\V

        Suppose that $f$ and $g$ are functions which have antiderivatives on an open interval $I$ in~${\RR}$,
    and assume that $f(u)\,\,{\leq}\,\,g(u)$ for all $u$ in~$I$. Let $a$ be a point of $I$,
    and let $F$ and $G$ denote the antiderivatives of $f$ and $g$ on $I$ such that $F(a) \,=\, G(a) \,=\, 0$.


\V

      (a) If $a\,\,{\leq}\,\,x$, then      
        \begin{displaymath}
        F(x)\,\,{\leq}\,\,G(x),
        \end{displaymath}
    with equality if, and only if, $F \,=\, G$ on the set $\mbox{Seg}\,[a,x]$.

\V

        (b) If $x\,\,{\leq}\,\,a$, then
        \begin{displaymath}
        G(x)\,\,{\leq}\,\,F(x),
        \end{displaymath}
    with equality if, and only if, $F \,=\, G$ on {\mbox{Seg}\,[a,x]}.

\V

{\bf Proof} The result is simply a restatement of Corollary~\Ref{CorE40.65} \Q

\VV

             \subsection{\small{\bf Examples}}
            \label{ExampE45.75A}

\V

        Throughout these examples $I$ is an open interval in ${\RR}$, $:I \,{\rightarrow}\, {\RR}$ is a function with domain~$I$,
    $a$ and~$x$ are numbers in the interval~$I$ such that $a\,<\,x$. (The restriction to $a\,<\,x$ is to simplify the example; the case $x\,\,{\leq}\,\,a$ is considered later.)

\V

        (1) Suppose that $f'$ is defined on $I$ and that $M_{1}$ is a number such that $f'(u)\,\,{\leq}\,\,M_{1}$ for all $u$ in~$[a,x]$;
    let $g_{1}$ be the constant function such that $g_{1}(u) \,=\, M_{1}$ for all $u$ in~$I$. (The reason for the subscript will be clear soon.)
    Note that the antiderivative of $f$ on $I$ with value $0$ at $a$ is clearly $F_{1}:I \,{\rightarrow}\, {\RR}$ given by the formula $F_{1}(u) \,=\, f(u)-f(a)$.
    Likewise, the antiderivative of $g_{1}$ with value $0$ at $a$ is given by $G_{1}(u) \,=\, M_{1}\,(u-a)$.
    It follows from the preceding corollary that
        \begin{displaymath}
        f(x) - f(a)\,\,{\leq}\,\,M_{1}\,(x-a),
        \end{displaymath}
    with equality if, and only if, $f(u)-f(a) \,=\, M_{1}\,(u-a)$ for all $u$ in~$[a,x]$.
    A similar argument shows that if $m_{1}$ is a number such that $m_{1}\,\,{\leq}\,\,f'(u)$ for all $u$ in~$[a,x]$, then
        \begin{displaymath}
        m_{1}\,(x-a)\,\,{\leq}\,\,f(x) - f(a),
        \end{displaymath}
    with equality if, and only if, $f(u)-f(a) \,=\, m_{1}\,(u-a)$ for all $u$ in $[a,x]$.

\V

        (2) Now suppose that $f''$ is defined on $I$, and that $m_{2}\,\,{\leq}\,\,f''(u)\,\,{\leq}\,\,M_{2}$ for all $u$ in~$[a,x]$.
    Apply the results of the preceding example to get
        \begin{displaymath}
        m_{2}\,(x-a)\,\,{\leq}\,\,f'(x) - f'(a)\,\,{\leq}\,\,M_{2}\,(x-a),
        \end{displaymath}
    with equality on the left if, and only if, ???

\VV

%%----------------------- L
%\StartSkip{
        The following question is natural:

\VA

        \h `Under what circumstances does a given function have an antiderivative on a given interval?'.

\VA

\noindent One knows from elementary calculus many examples of functions whose antiderivatives can be computed.
    In contrast, it is easy to provide simple examples of functions $f:I \,{\rightarrow}\, {\RR}$
    which fail to have an antiderivative on an open interval~$I$. For example, if $f:{\RR} \,{\rightarrow}\, {\RR}$ is the Dirichlet function,
    then there is no function $F:{\RR} \,{\rightarrow}\, {\RR}$ such that $F'(x) \,=\, f(x)$ for all~$x$.
    This is obvious because the Dirichlet function clearly fails to possess the Intermediate-Value Property on any subinterval of~${\RR}$.
    In other words, a complete answer to this question is likely to be complicated.

\V

        The next result generalizes the method used in the `Absolute Value' example above.

\V

             \subsection{\small{\bf Theorem}}
            \label{ThmE45.125A}

\V

        Let $[a,b]$ be a closed bounded interval in~${\RR}$, and let ${\cal D} \,=\, \{(x_{0},y_{0}), (x_{1},y_{1}),\,{\ldots}\,(x_{k},y_{k})\}$
    be a set of `data points' in ${\RR}^{2}$ such that $a \,=\, x_{0}\,<\,x_{1}\,<\,\,{\ldots}\,\,<\,x_{k} \,=\, b$,
    and let $g:{\RR} \,{\rightarrow}\, {\RR}$ be the corresponding continuous piecewise-linear interpolating function through these points;
    see Example~\Ref{ExampD20.53}~(6). Then for each real number $c$ in ${\RR}$ the function $g$ has a unique antiderivative $G$ on ${\RR}$ such that $G(c) \,=\, 0$.

\V

        \underline{Proof} It suffices to show that $g$ has an antiderivative $G$ which takes on the value $0$ at $c \,=\, a \,=\, x_{0}$; the case for general choice of $c$ follows easily.

        Note that for each index $j \,=\, 1,2,\,{\ldots}\,k$, if $x$ satisfies the condition $x_{j-1}\,\,{\leq}\,\,x\,\,{\leq}\,\,x_{j}$, then by definition
        \begin{displaymath}
        g(x) \,=\, y_{j-1} + a_{j}\,(x-x_{j-1}) \mbox{ where } a_{j} \,=\, \frac{y_{j} - y_{j-1}}{x_{j} - x_{j-1}}.
        \end{displaymath}
    It is clear that, on the inverval $[x_{j-1},x_{j}]$, every function of the form $G_{j}(x) \,=\, a_{j}\,(x-x_{j-1})^{2}/2 + y_{j}\,(x-x_{j-1}) + c_{j}$, where $c_{j}$ can be any real number,
    is an antiderivative of $g$ on that interval such that $G_{j}(x_{j-1}) \,=\, c_{j}$. (At the endpoints of this interval $G_{j}'$ refers to the appropriate one-sided derivatives.)
    The remainder of the construction of the desired function~$G$ is to choose the constants
    $c_{1}$, $c_{2}$,\,{\ldots}\,$c_{k}$ so that the antiderivatives $G_{1}$, $G_{2}$,\,{\ldots}\, `fit' together properly at the points $x_{j}$; more precisely:

        \underline{The case $j \,=\, 1$} Since, as is observed above, one has $G_{1}(x_{0}) \,=\, c_{1}$, one must choose $c_{1} \,=\, 0$.

        \underline{The Case $j \,=\, 2$} Choose $c_{2}$ so that $G_{1}(x_{1}) \,=\, G_{2}(x_{1})$.
    Since $G_{2}(x_{1}) \,=\, c_{2}$, this requires that one should choose $c_{2} \,=\, G_{1}(x_{1})$.

        The case of general $j$ is carried out similarly, so that $c_{j} \,=\, G_{j-1}(x_{j})$ for each~$j$.

        Finally, define $G:[a,b] \,{\rightarrow}\, {\RR}$ by the rule that if $x_{j-1}\,\,{\leq}\,\,x\,\,{\leq}\,\,x_{j}$, then $G(x) \,=\, G_{j}(x)$.
    One can apply Theorem~\Ref{ThmE20.25A} to conclude that $G'(x) \,=\, g(x)$ for all $x$ in~$[a,b]$. \Q

\V

        {\bf Remark}\,It is easy to show, using only simple Euclidean geometry, that if $g$ is as above and $G$ is any anitderivative of $g$ on~$[a,b]$,
    then $G(b) - G(a)$ equals the signed area between the graph $y \,=\, g(x)$ and the horizontal axis for $a\,\,{\leq}\,\,x\,\,{\leq}\,\,b$.
    (`Signed area' refers to the idea that area above the horizontal axis is positive, while area below that axis is negative.)

\VV
%}%\EndSkip
%%-------------------- L



\VV

                \section{{\bf Significance of Derivatives of Higher Order}}
                \label{SectE42}\IndB{ZZ Sections}{\Ref{SectE42} Significance of Derivatives of Higher Order}

\VV

        We have just seen that first derivative $f'$ gives useful information about a given function $f$ on an interval.
    Thus it is natural to ask what information derivatives of higher order can provide.
    The key is the following generalization of the construction in Part~(c) of Corollary~\Ref{CorE40.65A}.

\V

            \subsection{\small{\bf Example}}
            \label{ExampE40.68A}

\V

        Suppose that for some natural number $n$ the function $f:I \,{\rightarrow}\, {\RR}$ is $n$-times differentiable on an open interval~$I$.
    Let $c$ be any fixed number in~$I$. Let $F_{n-1}:I \,{\rightarrow}\, {\RR}$ denote the $c$-antiderivative of $f^{(n)}$ on $I$ (see Remark~\Ref{RemrkE40.65AB}).
    Then $F_{n-1}$ is given by the formula
        \begin{displaymath}
        F_{n-1}(x) \,=\, f^{(n-1)}(x) - f^{(n-1)}(c) \mbox{ for all $x$ in~$I$}.
        \end{displaymath}
    Similarly, the $c$-antiderivative $F_{n-2}$ of $F_{n-1}$ on $I$, which of course is the second-order $c$-antiderivative of~$f^{(n)}$, is given by
        \begin{displaymath}
        F_{n-2}(x) \,=\, f^{(n-2)}(x) - f^{(n-2)}(c) - f^{(n-1)}(c)\,(x-c) \mbox{ for all $x$ in~$I$}.
        \end{displaymath}

        Continuing this way, one sees that the $n$-th order $c$-antiderivative of $f^{(n)}$ on $I$ is the function $F_{0}:I \,{\rightarrow}\, {\RR}$ given by
        \begin{displaymath}
        F_{0}(x) \,=\, f(x) - f(c) - f'(c)\,(x-a) - \frac{f''(c)}{2}\,(x-c)^{2} - \,{\ldots}\, - \frac{f^{(n-1)}}{(n-1)!} \mbox{ for all $x$ in~$I$}.
        \end{displaymath}
    Note that repeated use is made of the fact that if $g(x) \,=\, B\,(x-c)^{m}$ for some constant~$B$,
    then the $c$-antiderivative of $g$ is given by ${\displaystyle \frac{B}{m+1}\,(x-c)^{m+1}}$; see Example~\Ref{ExampE40.65B}.
    In the case at hand, the constant $B$ is of the form $f^{(k)}(c)/k!$.

\VV

        The preceding calculation brings to light, in a natural way, certain polynomials which are associated with a function $f$ having $n-1$ derivatives.

\V

            \subsection{\small{\bf Definition}}
            \label{DefE40.68B}

\V

        Let $f:I \,{\rightarrow}\, {\RR}$ be a function which is $n$-times differentiable on an open interval~$I$, where $n{\in}{\NN}$.
    Let $c$ be a point in~$I$. The {\bf Taylor polynomial of order $\Bfm{n-1}$ of a function $\Bfm{f}$ centered at~$\Bfm{c}$}, denoted $\Bfm{p_{(f;n-1;c)}}$, \IndA{Taylor polynomial at a point} is given by the rule
        \begin{displaymath}
        p_{(f;n-1;c)}(x)\,=\, f(c) + f'(c)\,(x-c) + \frac{f''(c)}{2}\,(x-c)^{2} 
    + {\ldots} + \frac{f^{(n-1)}(c)}{(n-1)!}\,(x-c)^{n-1}.
        \end{displaymath}
        The expression $f(x) \,{\approx}\, p_{(f;n-1;c)}(x)$ is called the {\bf Taylor approximation}
    \IndB{Taylor's theorem}{Taylor approximation; error} of $f(x)$ of order~$n-1$ at~$c$;
    the difference $f(x)- p_{(f;n-1;c)}(x)$ is the {\bf error} of this approximation.

\VV

        The Taylor approximation is widely used in many applied areas, mainly because it is possible to estimate the size of the corresponding error.

\V

            \subsection{\small{\bf Taylor's Theorem}}
            \label{ThmE40.69A}\IndA{Taylor's theorem}

\V

        Suppose that $f:I \,{\rightarrow}\, {\RR}$ is $n$-times differentiable on an open interval~$I$, where $n$ is a natural number.
    Let $c$ be a number in~$I$.

\V

        (a) Let $x$ be a number in $I$ such that $x\,>\,c$, and suppose that for $M_{n}$ is a quantity such that $f^{(n)}(u)\,\,{\leq}\,\,M_{n}$
    for all $u$ in the closed interval~$[c,x]$; the value $M_{n} \,=\, +{\infty}$ is allowed. Then one has
        \begin{displaymath}
        f(x)- p_{(f;n-1;c)}(x)\,\,{\leq}\,\, \frac{M_{n}}{n!}(x-c)^{n}
        \end{displaymath}
     with equality if, and only if, ${\displaystyle f(u)- p_{(f;n-1;c)}(u) \,=\,  \frac{M_{n}}{n!}(u-c)^{n}}$ for every $u$ in $[c,x]$;
    equivalently, if, and only if, $f^{(n)}(u) \,=\, M_{n}$ for all $u$ in the interval~$[c,x]$.

\V

        (b) Again let $x$ be a number in $I$ such that $x\,>\,c$, but now suppose, instead,
    that $m_{n}$ is a quantity such that $m_{n}\,\,{\leq}\,\,f^{(n)}(u)$ for all $u$ in~$[c,x]$; the value $m_{n} \,=\, -{\infty}$ is allowed. Then one has
        \begin{displaymath}
        \frac{m_{n}}{n!}\,(x-c)^{n}\,\,{\leq}\,\,f(x)- p_{(f;n-1;c)}(x),
        \end{displaymath}
    with equality if, and only if, ${\displaystyle f(u)- p_{(f;n-1;c)}(u) \,=\,  \frac{M_{n}}{n!}(u-c)^{n}}$ for every $u$ in $[c,x]$;
    equivalently, if, and only if, $f^{(n)}(u) \,=\, m_{n}$ for each $u$ in~$[c,x]$.

\V

        (c) Now let $x$ be any number in $I$ such that $x\,<\,c$, and suppose that $m_{n}$ and $M_{n}$ are quantities such that 
    $m_{n}\,\,{\leq}\,\,f^{(n)}(u)\,\,{\leq}\,\,M_{n}$ for each $u$ in the closed interval $[x,c]$;
    as usual, the values $m_{n} \,=\, -{\infty}$ and $M_{n} \,=\, +{\infty}$ are allowed. Then one has
        \begin{displaymath}
        (-1)^{n}\frac{m_{n}}{n!}\,(x-c)^{n}
    \,\,{\leq}\,\,
        (-1)^{n}\left(f(x) - p_{(f;n-1;c)}(x)\right)
    \,\,{\leq}\,\,
        (-1)^{n}\,frac{M_{n}}{n!}\,(x-c)^{n}. 
        \end{displaymath}
    Furthermore, one has equality on the left if, and only if, ${\displaystyle f(u)- p_{(f;n-1;c)}(u) \,=\,  \frac{M_{n}}{n!}(u-c)^{n}}$
    for each $u$ in $[c,x]$; equivalently, if, and only if, $f^{(n)}(u) \,=\, m_{n}$ for each $u$ in~$[c,x]$.
        Similarly, one has equality on the right if, and only if, ${\displaystyle f(u)- p_{(f;n-1;c)}(u) \,=\,  \frac{M_{n}}{n!}(u-c)^{n}}$
    for every $u$ in $[c,x]$; equivalently, if, and only if, $f^{(n)}(u) \,=\, M_{n}$ for every $u$ in~$[c,x]$.


\V

        (d) Let $n$ be a positive integer, and suppose that $f:I \,{\rightarrow}\, {\RR}$ is $n$-times differentiable on an open interval~$I$.
    Let $c$ be any number in~$I$. Then for each $x$ in $I$ with $x \,\,{\neq}\,\, c$,
    there exists a number $q$ strictly between $x$ and $c$ such that
        \begin{displaymath}
        f(x) - p_{(f;n-1;c)}(x) \,=\, \frac{f^{(n)}(q)}{n!}\,(x-c)^{n}.
        \end{displaymath}
    This last equation often is written in the equivalent form
        \begin{displaymath}
        \frac{f(x) - p_{(f;n-1;c)}(x)}{(x-c)^{n}} \,=\, \frac{f^{(n)}(q)}{n!}.
        \end{displaymath}

\V

        {\bf Proof} The simple proofs of Parts (a) and (b) are left as exercises.

\V

        (c) Let $J \,=\, \{y: -y{\in}I\}$, and define $g:J \,{\rightarrow}\, {\RR}$ by the rule $g(y) \,=\, f(-y)$ for each $y$ in~$J$. Set $b \,=\, -c$.
    It follows directly from repeated use of the Chain Rule that $g$ is $n$-times differentiable on $J$.
    More precisely, if $y$ is in $J$ and $x \,=\, -y$ is the coreesponding element of~$I$, then one has
        \begin{displaymath}
        g^{(k)}(y) \,=\, (-1)^{k}\,f^{(k)}(-y) \,=\, (-1)^{k}\,f^{(k)}(x)        \end{displaymath}
     for each $j \,=\, 0, 1, \,{\ldots}\,n$. It follows easily for such $k$, $x$ and $y$ that $g^{(k)}(y)(y-b)^{k} \,=\, f^{(k)}(c)(x-c)^{k}$, hence
        \begin{displaymath}
        g(y) - p_{g;n-1};b)(y) \,=\, f(x) - p_{f;n-1};c)(x).
        \end{displaymath}
    From the hypothesis $m_{n}\,\,{\leq}\,\,f^{(n)}(u)\,\,{\leq}\,\,M_{n}$ for $u$ in~$[x,c]$,
    and the fact that $(-1)^{n}\,(x-c)^{n} \,=\, (c-x)^{n}\,>\,0$, it then follows that
        \begin{displaymath}
        \frac{m_{n}}{n!}\,(-1)^{n}(x)\,\,{\leq}\,\,(-1)^{n}\,g^{(n)}(v)\,\,{\leq}\,\,M_{n}
    \mbox{ for each $v$ in $[b,y]$}.
        \end{displaymath}
    Apply these results, together with the results of Parts (a) and~(b), to the function $(-1)^{n}\,g$ to get the desired result.

\V

        (d) \underline{Case 1}\,Suppose first that $x\,>\,c$, and let $S \,=\, \{f^{(n)}(u): u{\in}[c,x]\}$.
    Let $\hat{m}_{n} \,=\, {\inf}\,S$ and $\hat{M}_{n} \,=\, {\sup}\,S$. It is clear that $\hat{m}_{n}$ and $\hat{M}_{n}$ can be used for $m_{n}$ and $M_{n}$,
    respectively, in Parts~(a) and~(b) above. More precisely, $\hat{m}_{n}$ is the largest value of $m_{n}$, and $\hat{M}_{n}$ is the smallest of $M_{n}$,
    that satisfy the hypotheses in Parts~(a) and~$(b)$.

        \h (i)\,\,Suppose that $\hat{m}_{n} \,=\, \hat{M}_{n}$, so that $f^{(n)}(u) \,=\, \hat{M}_{n}$ for all $u$ in~$[c,x]$.
    Then the `if' portions of the `equality' statements in~(a) and~(b) imply that
        \begin{displaymath}
         \frac{\hat{m}_{n}}{n!}\,(x-c)^{n} \,=\, f(u)- p_{(f;n-1;c)}(u) \,=\, \frac{\hat{M}_{n}}{n!}(u-c)^{n} \mbox{ for all $u$ in $[c,x]$}.
        \end{displaymath}
    In particular, in this case one can choose $q$ to be any interior pointof the interval $[c,x]$; for example, $q \,=\, (c+x)/2$ works.

        \h (ii),Suppose now that $m_{n}\,<\,M_{n}$ so that $f^{(n)}$ is not constant on the interval $[c,x]$.
    Then the `only if' portions of the same statements imply that
        \begin{displaymath}
        \frac{\hat{m}_{n}}{n!}\,(x-c)^{n}\,<\,f(x)- p_{(f;n-1;c)}(x) \,<\, \frac{\hat{M}_{n}}{n!}(u-c)^{n} \mbox{ for all $u$ in $[c,x]$}.
        \end{displaymath}
    However, the definitions of $\hat{m}_{n}$ and $\hat{M}_{n}$ here, combined with with the Intermediate-Value Theorem for Derivatives,
    implies that every number between $\hat{m}_{n}$ and $\hat{M}_{n}$ is of the form $f^{(q)}$,
    and thus every number between ${\displaystyle \frac{\hat{m}_{n}}{n!}}\,(x-c)^{n}$ and ${\displaystyle \frac{\hat{M}_{n}}{n!}}\,(x-c)^{n}$
    is of the form ${\displaystyle \frac{f^{(n)}}(q){n!}}\,(x-c)^{n}$, for some $q$ in $(c,x)$. The desired result now follows.

        \underline{Case 2}\,If, instead, one supposes that $x\,<\,c$, then the desired result follows in a similar manner from Part~(c) above;
    a key observation is that $(-1)^{n}(x-c)^{n} \,=\,(c-x)^{n}\,>\,0$.

\V

            \subsection{\small{\bf Corollary} (The Mean-Value Inequality)}
            \label{CorE40.69A}

\V

        Suppose that $f:I \,{\rightarrow}\, {\RR}$ is differentiable on an open interval~$I$. 
    Let $a$ and $b$ be numbers in $I$ such that $a\,<\,b$, and suppose that $m$ and $M$ are quantities such that $m\,\,{\leq}\,\,f'(u)\,\,{\leq}\,\,M$
    for all $u$ in the closed interval~$[a,b]$; the values $m \,=\, -{\infty}$ and  $M \,=\, +{\infty}$ are allowed. Then one has
        \begin{displaymath}
        m\,(b-a)\,\,{\leq}\,\,f(b) - f(a)\,\,{\leq}\,\,M\,(b-a),
        \end{displaymath}
     with equality on either side being an equality if, and only if, $f'$ is constant on the interval $[a,b]$.

\V

        The simple proof is left as an exercise. \Q

\VV

            \subsection{\small{\bf Remarks}}
            \label{RemrkE40.69AB}

\V

\hspace*{\parindent}(1) The name `Taylor' refers to the English mathematician Brook Taylor (1685-1731),
    although the preceding theorem is actually due to the Italian/French mathematician Joseph-Louis Lagrange (1736-1813).

\V

        (2) Many texts use the phrase `Taylor polynomial of {\em degree}~$n-1$' instead of `of {\em order}~$n-1$'.
    The problem with that terminology is that if $f^{(n-1)}(c) \,=\, 0$, then the degree of the polynomial $p_{(f;n-1;c)}$ is strictly less than~$n-1$.

\V

        (3) The error $f(x) - p_{(f;n-1); c}$ is often called the {\bf remainder} in the given Taylor approximation, and abbreviated as $R_{n-1}(x)$.
    That is, $R_{n-1}(x)$ is what `remains' when one subtracts the approximate value $p_{(f;n-1;c)}(x)$ from the exact value $f(x)$ of~$f$ at~$x$,
    so that one can write
        \begin{displaymath}
        f(x) \,=\, p_{(f;n-1);c}(x) + R_{n-1}(x).
        \end{displaymath}
    In particular, the expression ${\displaystyle f^{(n)}(q)\,(x-c)^{n}}$ appearing in Part~(d) above is called the {\bf Lagrange form} of the remainder.
    \IndBD{Taylor's Theorem}{Lagrange form of the remainder} There is a very different formulation of the same remainder,
    due to Cauchy, which must be postponed to a later chapter.

\VV

%% Lagrange `Theorie des Functions Analytique"
%%    pp. 78-79 Incorrect proof that f'(x)>0 on [a,b] implies f(b)>f(a)
%%             Mistake same as Cauchy's in C's `proof' of MVT
%%    pp. 80-85 Taylor's Theorem in terms of inequalities; more precisely,
%%              MacLaurin's version ($c \,=\, 0$).
%%
%% See also Lesson 9 of Lagrange's `Lecons sur le calcul des fonctions'

%%  NOTE Lagrange refers to a memoire of Ampere, in Tome VI 
%%       de J. de L'ecole Polytechnique   

%% Cauchy `Resume des Lecons sur le Calcul Infinitesimal'
%%    pp. 36-38 Incorrect proof of Mean-Value Inequality, which is used
%%              correctly to get the standard Mean-Value Equation, but
%%              explicitly assuming continuity of f'

%%  NOTE Cauchy refers to a memoire of Ampere, in XIII-ieme
%%       Cahier de J. de L'ecole Polytechnique  

%%  Cauchy `Lecons sur lw calcul differentials'
%%    pp. 44-50 Cauchy version of MVT via inequalities. Then versios of
%%              L'Hospital, then versions of Taylor's Theorem (all via 
%%              standard MVT.)



\VV

        The following somewhat weaker result is sufficient for many applications, and covers both the cases $x\,>\,c$ and~$x\,<\,c$.


\V

            \subsection{\small{\bf Corollary}}
            \label{CorE40.69B}

\V

        Suppose that for some number $B_{n}$ one has $|f^{(n)}(x)|\,\,{\leq}\,\,B$
    for all $x$ in some open interval $I$. Let $c$ be any point of~$I$. Then
        \begin{displaymath}
        |f(x)- p_{(f;n-1;c)}(x)|\,\,{\leq}\,\,\frac{B}{n!}|x-a|^{n} \mbox{ for all $x$ in~$I$}.
        \end{displaymath}

\V

        The simple proof is left as an exercise. \Q

\VV


        {\bf Special Case -- The Mean-Value Theorems for Derivatives}\IndBD{Taylor's Theorem}{mean-value theorems for derivatives}

\V

        The case $n \,=\, 1$ Theorem~\Ref{ThmE40.69A}
    corresponds to the simplest Taylor approximation, namely $f(x) \,{\approx}\, f(c)$.
    The corresponding inequalities from Parts~(a) and~(b) of that theorem is of the form
        \begin{displaymath}
        m_{1}\,(x-c)\,\,{\leq}\,\,f(x)-f(c)\,\,{\leq}\,\,M_{1}\,(x-c)
    \mbox{ for all $x$ in $I$ such that $x\,>\,c$},
        \end{displaymath}
    where $m_{1}$ is a lower bound for $f'$ on $[c,x]$ and $M_{1}$ is the corresponding upper bound.
    Divide by the positive quantity $x-c$ to get the equivalent formulation
        \begin{displaymath}
        m_{1}\,\,{\leq}\,\,\frac{f(x) - f(c)}{x-c}\,\,{\leq}\,\,M_{1}.
        \end{displaymath}
    In texts for elementary calculus the letters $c$ and $x$ are usually replaced by the letters $a$ and $b$, respectively,
    so that $a\,<\,b$, and the preceding result is written in the more familiar form
        \begin{displaymath}
        m_{1}\,\,{\leq}\,\,\frac{f(b) - f(a)}{b-a}\,\,{\leq}\,\,M_{1} \h ({\ast})
        \end{displaymath}
    Furthermore, one gets equality on either end of $({\ast})$ if, and only if, $m_{1} \,=\, f'(u) \,=\,M_{1}$ for each $u$ in $[a,b]$.
    Since ${\displaystyle \frac{f(b) - f(a)}{b-a} \,=\, \frac{f(a)-f(b)}{a-b}}$, it is clear that Inequality~$({\ast})$ remains valid if, instead, $b\,<\,a$.
    
        For historical reasons one refers to~$({\ast})$ as the {\bf Inequality Formulation of the Mean-Value Theorem for Derivatives};
    \IndBD{Taylor's theorem}{mean-value theorem for derivatives, inequality formulation; mean-value inequality}
    or, more  briefly, the {\bf Mean-Value Inequality}.

        Similarly, the case $n \,=\, 1$ in Part~(d) of Theorem~\Ref{ThmE40.69A} usually is written
        \begin{displaymath}
        \frac{f(b) - f(a)}{b-a} \,=\, f'(q) \mbox{ for some $u$ such that $a\,<\,u\,<\,b$}.
        \end{displaymath}
    This result is called the {\bf Equation Formulation of the Mean-Value Theorem for Derivatives}; or, more briefly, the {\bf Mean-value Equation}.
    \IndBD{Taylor's theorem}{mean-value theorem for derivatives, equation formulation; mean-value equation}

\V

        The next result is an obvious corollary of the Mean-Value Equation.

\V

            \subsection{\small{\bf Corollary} (Rolle's Theorem)}
            \label{CorE40.69C}

\V

        Suppose that $f:[a,b] \,{\rightarrow}\, {\RR}$ is differentiable on an open interval $(a,b)$
    and continuous on the corresponding closed interval $[a,b]$. Assume further that $f(a) \,=\, f(b)$.
    Then there exists a number $q$ such that $a\,<\,q\,<\,b$ such that $f'(c) \,=\, 0$.
        

\VV

        {\bf Remark} The order in which the topics are presented in the last two sections of {\ThisText}
    is essentially the reverse of the order followed in standard texts on elementary calculus.
    More precisely, such texts start the discussion with the statement and proof of Rolle's Theorem.
    This theorem is used to prove the Mean-Value Equation, which is then used to prove the results relating the sign of the derivative and monotonicity.
    In such texts the results concerning Taylor's Theorem normally occur much later, and are proved by a clever application of Rolle's Theorem.

        One way to view this is that often in mathematics there is more than one way to develop a subject,
    and that it is useful for learners to see alternate approaches. In the next {\Note} other reasons for preferring the approach used in {\ThisText} are given.
    
\VV


%%%
\begin{quotation}
{\footnotesize \underline{\Note}\IndB{\notes}{on reducing the role of Rolle's theorem} (on reducing the roll of Rolle's theorem )
    NEED TO WRITE
}%EndFootNoteSize
\end{quotation}
%##


\VV

            \subsection{\small{\bf Example}}
            \label{ExampE20.76}

\V

        In physics and engineering the Taylor approximation is used frequently to obtain useful values for processes that are subjet to known physical laws.
    For instance, the analysis of vibrating membranes leads to the following differential equation:
    $f'' + f \,=\, 0$, where $f:{\RR} \,{\rightarrow}\, {\RR}$ is a $C^{2}$ function to be determined.
    Note that this equation implies that $f$ is actually a $C^{{\infty}}$ function.
    Indeed, if written as $f'' \,=\, -f$, it implies that $f''$ is also a $C^{2}$ function,
    hence that $f$ is actually a $C^{4}$ function and that $f^{(4)} \,=\, -f^{(2)} \,=\, f$.
    Repeat this argument to get that $f$ is $C^{{\infty}}$, and that
        \begin{displaymath}
        f^{'''} \,=\, -f'; \h f^{(4)} \,=\, f; \h f^{(5)} \,=\, f'' \,=\, -f; \h
        f^{(6)} \,=\, -f'; \mbox{ and so on}.
        \end{displaymath}
    In particular, all the derivatives $f^{(n)}$ with $n\,\,{\geq}\,\,2$ can be expressed simply in terms of $f$ and~$f'$.
    It follows that for each $c$ in ${\RR}$ the coefficients of the Taylor polynomials of $f$ at $c$ are determined by the numbers $f(c)$ and $f'(c)$:
        \begin{displaymath}
        \frac{f''(c)}{2!} \,=\, -\frac{f(c)}{2!}; \h \frac{f^{(3)}(c)}{3!} \,=\, -\frac{f'(c)}{3!}; \h \frac{f^{(4)}(c)}{4!} \,=\, \frac{f(c)}{4!}; \mbox{ and so on}.
        \end{displaymath}
    To simplify this example, consider the case $c \,=\, 0$ and $f(0) \,=\, 1$, $f'(0) \,=\, 0$. One then gets
        \begin{displaymath}
        p_{(f;0;0)}(x) \,=\, p_{(f;1;0)}(x) \,=\, 1; \h
        p_{(f;2;0)}(x) \,=\, p_{(f;3;0)}(x) \,=\, 1 - \frac{x^{2}}{2}; \h
        p_{(f;4;0)}(x) \,=\, p_{(f;5;0)}(x) \,=\, 1 - \frac{x^{2}}{2} + \frac{x^{4}}{4!};
        \end{displaymath}
    and so on. To be definite, consider the case $n \,=\, 5$, so that the corresponding Taylor approximation of order $4$ takes the form
        \begin{displaymath}
        f(x) \,{\approx}\, 1 - \frac{x^{2}}{2!} + \frac{x^{4}}{4!}.
        \end{displaymath}
    This approximation is of little value since it gives no indication of how accurate it is.
    However, if one can obtain upper and lower bounds on $f^{(5)}$, then one can obtain such information.

        In this case there is a trick: multiply both sides of the original equation $f'' + f \,=\, 0$ by $f'$ to get $f''{\cdot}f + f{\cdot}f' \,=\, 0$.
    The left side of this last equation is seen to be $\frac{1}{2}\,\left((f')^{2} + f^{2}\right)'$,
    so that $(f')^{2} + f^{2}$ must be some constant~$B$. Substitute $c \,=\, 0$ into $f'$ and $f$
    to get $0^{2} + 1^{2} \,=\, B$, so that $(f'(x))^{2} + (f(x))^{2} \,=\, 1$ for all~$x$.
    In particular, one has $-1\,\,{\leq}\,\,f(x)\,\,{\leq}\,\,1$ and $-1\,\,{\leq}\,\,f'(x)\,\,{\leq}\,\,1$,
    so that $-1\,\,{\leq}\,\,f^{(n)}(x)\,\,{\leq}\,\,1$, for all~$x$. In particular, one has
        \begin{displaymath}
        -\frac{x^{5}}{5!}\,\,{\leq}\,\,f(x)-\left(1 - \frac{x^{2}}{2} + \frac{x^{4}}{4!}\right)\,\,{\leq}\,\,\frac{x^{5}}{5!} \mbox{ for all $x\,>\,0$}.
        \end{displaymath}
    If, say, $x \,=\, 1$, then this says the corresponding Taylor approximation of $f(1)$ has error less than $1/5! \,=\, 1/120\,<\,0/01$.
    By choosing larger values of $n$ one can approximate the value $f(1)$ with the value of an appropriate Taylor polynomial to any desired accuracy.
    
\V

                \subsection{{\bf Standard Form for the Taylor Remainder}}
                \label{subectE40.70}\IndB{ZZ Sections}{\Ref{SectE50} Standard Form for the Taylor Remainder}

\VV

        The `inequality' form of the remainder/error in the Taylor approximation, as given in Theorem~\Ref{ThmE40.69A},
    is not the version that one finds in most calculus texts. Instead, the following formulation is standard.

\V

            \subsection{\small{\bf Theorem}}
            \label{ThmE40.80}

\V

        Let $n$ be a positive integer, and suppose that $f:I \,{\rightarrow}\, {\RR}$ is $n$-times differentiable on an open interval~$I$.
    Let $c$ be any number in~$I$. Then for each $x$ in $I$ with $x \,\,{\neq}\,\, c$ there exists a number $u$ strictly between $x$ and $c$ such that
        \begin{displaymath}
        f(x) - p_{(f;n-1;c)}(x) \,=\, f_{^{(n)}}(u)\,(x-c)^{n}.
        \end{displaymath} 

\V

        {\bf Proof}\, Consider first the situation in which $x\,>\,c$. Let $U_{n} \,=\, \{f^{(n)}(u): u{\in}(c,x)\}$,
    and set $m_{n} \,=\, {\inf}\,U$ and $M_{n} \,=\, {\sup}\,U_{n}$.

        \underline{Case 1} Suppose that $m_{n}$ and $M_{n}$ are both finite. Then by Theorem~\Ref{ThmE40.69A} one has
        \begin{displaymath}
        \frac{m_{n}}{n!}(x-c)^{c}\,\,{\leq}\,\,f(x)- p_{(f;n-1;c)}(x)\,\,{\leq}\,\, \frac{M_{n}}{n!}(x-c)^{n}
        \end{displaymath}


    By Part~(a) of Theorem~\Ref{ThmE40.69A}

%%%
\begin{quotation}
{\footnotesize \underline{\Note}\IndB{\notes}{on the Mean-Value Theorem for Derivatives} (on the mean-value theorem for derivatives)
    The `Mean-Value Theorem' described in Theorem~\Ref{ThmE50.20} below has an interesting history.
    It was known to Lagrange in the late 18th~century, and to Cauchy in the 1820's;
    indeed, Cauchy also presented a stronger version; see Theorem~\Ref{ThmE50.40}.
    Unfortunately, none of their proofs would be considered rigorous today. In {\ThisText} we follow the usual custom of naming the original version, 
    Theorem~\Ref{ThmE50.20}, after Lagrange, and the stronger version, Theorem~\Ref{ThmE50.40}, after Cauchy.

        The Lagrange/Cauchy formulation of the original theorem can  be paraphrased as follows:

\VA

        {\em Suppose that $y \,=\, f(x)$ has continuous derivative $f'(x)$ for all $x$ in some interval $a\,\,{\leq}\,\,x\,\,{\leq}\,\,b$.
    Let $m$ be the minimum value of of $f'$ on the interval $[a,b]$, and let $M$ be the corresponding maximum value of $f'$.
    Then
        \begin{displaymath}
        m(b-a)\,\,{\leq}\,\,f(b)-f(a)\,\,{\leq}\,\,M(b-a);
    \mbox{ equivalently, the ratio } \frac{f(b)-f(a)}{b-a} \mbox{ lies in the interval $[m,M]$}.
        \end{displaymath}
}

\VA

        Stated this way, the theorem says that the amount the function $f$ changes over the interval $[a,b]$, i.e., the quantity $f(b)-f(a)$,
    depends not just on the size of the interval, i.e., on $b-a$, but also on the size of the derivative, i.e. on $m$ and $M$.

        However, there is an alternate phrasing which is shorter, and which has become the standard one.
    To arrive at it, note that since (by hypothesis) $f'$ is continuous on $[a,b]$, it follows that $[m,M]$ is the image of $[a,b]$ under $f'$;
    see Part~(b) of Corollary~\Ref{CorD25.40}.
    Thus, the conclusion of the theorem, namely that the ratio ${\displaystyle \frac{f(b)-f(a)}{b-a}}$ lies in $[m,M]$,
    is equivalent to saying that this ratio is a value of $f'$ on $[a,b]$;
    that is, there exists $c$ in $[a,b]$ such that
        \begin{displaymath}
        \frac{f(b)-f(a)}{b-a} \,=\, f'(c).
        \end{displaymath}

        This alternate formulation, which both Lagrange and Cauchy stated, has some pleasant features.
    For example, it involves a single equation, not a pair of inequalities. It also avoids the need to mentioning the extreme values $m$ and $M$ of $f'$.

        Unfortunately, these `pleasant features' are also the worst features of the alternate formulation.
    In particular, this formulation shifts the focus away from the size of $f'$ (i.e., $m$ and $M$), and towards the number $c$.
    Indeed, many students in elementary calculus think that this theorem is about determining the value of $c$;
    but of course, it is not, since one almost never needs to do that. Compare this situation
    with two other major theorems of elementary calculus which also involve the existence of a number $c$ in $[a,b]$;
    namely, the Extreme-Value Theorem and Intermediate-Value Theorem. In those theorems, finding effective methods for computing $c$ is a central issue.
    Thus, it is not surprising that students believe (albeit incorrectly) that computing $c$ explicitly is what the Mean-Value Theorem asks of them.

        Here is a summary of the proof along the line of Lagrange and Cauchy, with editorial references, to some relevant theorems in {\ThisText}, marked off by brackets:

    Note first that the maximum and minimum values $M$ and $m$ of $f'$ referred to in the statement of the theorem do exist,
    by the Extreme-Value Theorem, because of the hypothesis that $f'$ is continuous on $[a,b]$.
    Then $f'(x)-m\,\,{\geq}\,\,0$ for all $x$. That is, the function $g(x) \,=\, f(x)-m\,x$ satisfies the inequality $g'(x)\,\,{\geq}\,\,0$,
    and thus $g(b)-g(a)\,\,{\geq}\,\,0$ [see Theorem~\Ref{ThmE40.40}]. That is, $f(b)-f(a)\,\,{\geq}\,\,m\,(b-a)$.
    A similar argument shows that $f(b)-f(a)\,\,{\leq}\,\,M\,(b-a)$.

\V

        In contrast, the approach to the Mean-Value Theorem most widely used now in elementary calculus texts
    is based on the proof attributed to Ossian Bonnet in the mid~1800s. (This is the familiar proof that starts with Rolle's Theorem,
    and then introduces a cleverly-chosen auxiliary function to get the general result.)
    Indeed, in calculus one carries out this proof first and then later uses the conclusion to prove Theorem~\Ref{ThmE40.40} and Corollary~\Ref{CorE40.50}.

        The Bonnet proof is a model of mathematical elegance: it is simple, it avoids the continuity hypothesis on $f'$,
    and it gets $c$ to be an interior point. Its one weakness -- and in the context of teaching elementary calculus, this is a major weakness --
    is that it hides the fact that the size of the derivative has a bearing on how much the function can change.

\V

        The proof of the Lagrange Mean-Value Theorem given below harks back to the Lagrange/Cauchy viewpoint, in that it directly relates the ratio ${\displaystyle \frac{f(b)-f(a)}{b-a}}$ to the size of the derivative.
    However, this proof also maintains the theoretical advantages of the Bonnet approach, in that it does not require continuity of $f'$,
    and it does guarantee that the number $c$ can be chosen to be an interior point of the interval.
}%EndFootNoteSize
\end{quotation}
%##

\VV

             \subsection{\small{\bf Theorem} (The Mean-Value Theorem for Derivatives)}
            \label{ThmE50.20}

\V

        Suppose that $f:[a,b] \,{\rightarrow}\, {\RR}$ is differentiable at each point of the open interval $(a,b)$ and continuous on the closed interval $[a,b]$.

\V


        (a) (Lagrange Formulation)\IndBD{derivatives}{mean-value theorem, Lagrange form}
    Let $S$ be the set of numbers of the form $f'(x)$ for $x$ in the open interval $(a,b)$.
    Suppose that $m$ is a lower bound of the set $S$ and that $M$ is an upper bound of~$S$. Then
        \begin{equation}
        \label{IneqE.100A}
        m\,\,{\leq}\,\,\frac{f(b)-f(a)}{b-a}\,\,{\leq}\,\,M; \mbox{ equivalently, }
        m\,\,{\leq}\,\,\frac{f(a)-f(b)}{a-b}\,\,{\leq}\,\,M.
        \end{equation}
    Furthermore, if $f'$ is not constant on the interval $(a,b)$, then both of the inequalities in~\Ref{IneqE.100A} are strict;
    that is, in this case the fraction $(f(b)-f(a))/(b-a)$ equals neither $m$ nor $M$.

\V

        (b) (Standard Formulation)\IndBD{derivatives}{mean-value theorem, standard form} There exists a number $c$, with $a\,<\,c\,<\,b$, such that
        \begin{equation}
        \label{EqnE.100B}
        \frac{f(b)-f(a)}{b-a} \,=\, f'(c); \mbox{ equivalently, }
        \frac{f(a)-f(b)}{a-b} \,=\, f'(c)
        \end{equation}
    Equivalently, there exists $c$ in the open interval $(a,b)$ such that
        \begin{equation}
        \label{EqnE.100C}
        f(b)-f(a) \,=\, f'(c)\,(b-a); \mbox{ equivalently, } f(a)-f(b) \,=\, f'(c)\,(a-b)
        \end{equation}

\V

        \underline{Proof}

\V

        (a) The inequality ${\displaystyle m\,\,{\leq}\,\,\frac{f(b)-f(a)}{b-a}}$ is obviously satisfied if $m \,=\, -{\infty}$,
    and is strict, since the fraction is finite. The analogous statement is true if $M \,=\, +{\infty}$.

        Thus, it suffices to consider the case in which $m$ and $M$ are finite. Under this assumption,
    let $g:{\RR} \,{\rightarrow}\, {\RR}$ and $h:{\RR} \,{\rightarrow}\, {\RR}$ be the functions given by the formulas $g(x) \,=\, m\,x$ and $h(x) \,=\, M\,x$.
    Then $g'(x) \,=\, m$ and $h'(x) \,=\, M$for all~$x$ in~${\RR}$, so
        \begin{displaymath}
        g'(x)\,\,{\leq}\,\,f'(x)\,\,{\leq}\,\,h'(x) \mbox{ for all $x$ in~$(a,b)$}.
        \end{displaymath}
    Let $\hat{g}$, $\hat{f}$ and $\hat{h}$ be the unique antiderivatives of $g'$, $f'$ and $h'$ on $(a,b)$ which have value $0$ at~$a$;
    that is,
        \begin{displaymath}
        \hat{g}(x) \,=\, g(x) - g(a) \,=\, m,(x-a) \, \hat{f}(x) \,=\, f(x)-f(a), \mbox{ and } \hat{h}(x) \,=\, h(x)-h(a) \,=\, M\,(x-c).
        \end{displaymath}
    It then follows from Corollary~\Ref{CorE40.65} that if $x{\in}[a,b]$ then
        \begin{displaymath}
        m\,(x-a)\,\,{\leq}\,\,f(x) - f(a)\,\,{\leq}\,\,M\,(x-a) \mbox{ for all $x$ in~$(a,b)$}.
        \end{displaymath}
    In particular, if one sets $x \,=\, b$ in the preceding set of inequalities and then divide by the positive number $b-a$, one gets the desired inequality.

\V

        (b) Part (a) is valid for every choice of lower bound $m$ and upper bound~$M$ of the set~$S$, so chose $m \,=\, {\inf}\,S$ and $M \,=\, {\sup}\,S$.
    In Cases (1) and (2) above, one can choose $c$ to be any number in $(a,b)$ since,
    as is shown above, $f'(x) \,=\, {\displaystyle \frac{f(b)-f(a)}{b-a}}$ for {\em all} $x$ in $(a,b)$.
        In Case~(3), with this choice of $m$ and~$M$, the desired result follows from the Intermediate-Value Theorem for Derivatives (Theorem~\Ref{ThmE40.35}). 
    \Q

\VV

        {\bf Remarks} (1) Many texts refer to what we call the `Standard Version' here as the `Lagrange Version'.
    In fact, Lagrange stated both versions, as did Cauchy some years later. The name `Cauchy' is traditionally attached to certain generalizations; see below.

\V

        (2) In principle the most precise phrasing of Part~(a) of the preceding theorem comes from choosing $m \,=\, {\inf}\,S$ and $M \,=\, {\sup}\,S$.
    However, computing infima and suprema can be difficult, whereas finding less precise values for $m$ and $M$,
    which are still precise enough for the problem at hand, is often much simpler.
    

\VV

        In the preceding theorem we assume that $a \,\,{\neq}\,\, b$. The next slight generalization, whose proof is omitted, drops this requirement.
    Of course the formulation no longer involves division by the number~$b-a$, since this quantity might equal zero.
    This formulation is easier to generalize to multivariable calculus.

\V


             \subsection{\small{\bf Corollary} (The Mean-Value Theorem -- Segment Form)}
            \label{CorE50.25}\IndBD{derivatives}{mean-value theorem, segment form}

\V

    Suppose that $f$ is differentiable at each point of a segment $\mbox{Seg}\,[a,b]$ in ${\RR}$; we do not assume that $a\,<\,b$.
    Then there exists a number $c$ in $\mbox{Seg}\,[a,b])$ such that
        \begin{equation}
        \label{EqnE.100D}
        f(b) - f(a) \,=\, f'(c)\,(b-a) \mbox{ for some $c$ in $\mbox{Seg}\,[a,b]$}.
        \end{equation}
 

\V

        {\bf Remark} If $b \,=\, a$, then $\mbox{Seg}\,[a,b] \,=\, \{a\}$, so one must choose $c \,=\, a \,=\, b$.
    In this case, however, $b-a \,=\, 0$ and $f(b)-f(a) \,=\, 0$, so the actual value of the factor $f'(c)$ is irrelevant.

\VV

        It is traditional to single out the following special case of the Standard Mean-Value Theorem and give it a name.

\V

             \subsection{\small{\bf Theorem} (Rolle's Theorem)}
            \label{ThmE50.30}\IndBD{derivatives}{Rolle's theorem}

\V

        Suppose that $f:[a,b] \,{\rightarrow}\, {\RR}$ is differentiable at each point of the open interval $(a,b)$ and continuous on the closed interval $[a,b]$.
     Suppose, in addition, that $f(a) \,=\, f(b)$. Then there exists $c$, with $a\,<\,c\,<\,b$, such that $f'(c) \,=\, 0$.

\V

        \underline{Proof}

\V

        The hypothesis $f(a) \,=\, f(b)$ implies that ${\displaystyle \frac{f(b)-f(a)}{b-a}} \,=\, 0$.
    Now apply Part~(b) of the Standard Mean-Value Theorem.

\V

        \underline{Remark} Michel Rolle stated the preceding result in $1691$, at least for polynomial functions;
    apparently his name was not attached to it until~$1835$. The proof of his theorem given above is not the same as one finds in elementary calculus;
    for that proof, see the exercises.

% EXERCISE

\VV

        Cauchy also provided the following major generalization of the Mean-Value Theorem.

\V

             \subsection{\small{\bf Theorem} (The Cauchy Mean-Value Theorem)}
            \label{ThmE50.40}\IndBD{derivatives}{Cauchy's mean-value theorem}

\V

        Suppose that $f$ and $g$ are continuous on the closed interval $[a,b]$ and differentiable on the open interval $(a,b)$.
    Suppose, in addition, that for all $x$ in $(a,b)$ one has $g'(x) \,\,{\neq}\,\, 0$.

\V

        (a) There exists a number $p$ in $(a,b)$ such that
        \begin{equation}
        \label{EqnE.110}
        \frac{f(b)-f(a)}{g(b)-g(a)} \,=\, \frac{f'(p)}{g'(p)}.
        \end{equation}

\V


        (b) Let $M \,=\, {\sup}\,\left\{{\displaystyle \frac{f'(x)}{g'(x)}: a\,<\,x\,<\,b}\right\}$,
    and let $m \,=\, {\inf}\,\left\{{\displaystyle \frac{f'(x)}{g'(x)}: a\,<\,x\,<\,b}\right\}$. Then
        \begin{displaymath}
        m\,\,{\leq}\,\,\frac{f(b)-f(a)}{g(b)-g(a)}\,\,{\leq}\,\,M
        \end{displaymath}
    Furthermore, if the ratio $f'/g'$ is not constant on the interval $(a,b)$, then the preceding inequalities are strict.

\V

        \underline{Remark} Cauchy's proof of this result is actually along the lines of the proof given above for the Lagrange Mean-Value Theorem.
    Indeed, we could have saved space by simply stating and proving the Cauchy version,
    and then pointing out that the standard version corresponds to the special case $g(x) \,=\, x$.
        However, separating the statements of these results is customary; and doing so allows us
    to give an alternate proof which shows that the Cauchy version, which is obviously more general than the Lagrange formulation of the Mean-Value Theorem, 
    can actually be viewed as a special case of the Lagrange version.

\V

        {\bf Proof}

\V

        (a) By Theorem~\Ref{ThmE40.30}, the hypothesis that $g'$ is never $0$ implies that $g'$ is either always positive or always negative.
    Assume first that $g'(x)\,>\,0$ for all $x$ in $(a,b)$.

        By Theorem~\Ref{ThmD25.55C} it follows that the image $g[a,b]$ of $[a,b]$ under $g$ is also a closed interval $[c,d]$, with $c \,=\, g(a)$ and $d \,=\, g(b)$.
    Also the inverse $g^{-1}:[c,d] \,{\rightarrow}\, [a,b]$ is continuous on $[c,d]$, and satisfies the conditions $g^{-1}(c) \,=\, a$ and $g^{-1}(d) \,=\, b$.
    It also follows that $g$ maps the open interval $(c,d)$ onto the open interval $(a,b)$.

        Now consider the function $h:[c,d] \,{\rightarrow}\, {\RR}$ defined by the rule
        \begin{displaymath}
        h(y) \,=\, f(g^{-1}(y)) \mbox{ for all $y$ in $[c,d]$};
        \end{displaymath}
    that is, $h \,=\, f{\circ}g^{-1}$.
    It is clear from what was just stated that $h$ is continuous on $[c,d]$.
    Likewise, it is clear from the Chain Rule that $h$ is differentiable on the open interval $(c,d)$;
    more precisely, if $x{\in}(a,b)$ and $y \,=\, g(x)$, so that $x \,=\, g^{-1}(y)$, then
        \begin{equation}
        \label{EqnE.110A}
        h'(y) \,=\, f'(g^{-1}(y)){\cdot}(g^{-1})'(y) \,=\, \frac{f'(x)}{g'(x)}
        \end{equation}
        Apply Part~(b) of the Lagrange Mean-Value Theorem to the function $h$ to conclude that there exists a number $q$ in $(c,d)$ such that
        \begin{displaymath}
        \frac{h(d)-h(c)}{d-c} \,=\, h'(q) \h ({\ast})
        \end{displaymath}
    Let $p \,=\, g^{-1}(q)$. In light of Equation~\Ref{EqnE.110A} and the definition of $h$, Equation~$({\ast})$ can be written
        \begin{displaymath}
        \frac{f(b) - f(a)}{g(b) - g(a)} \,=\, \frac{f'(p)}{g'(p)},
        \end{displaymath}
    as required.

        If $g'(x)\,<\,0$ for all $x$ in $(a,b)$, then apply what was just proved to the functions $f$ and $-g$.

\V

        (b) It is clear from Equation~$EqnE.110$ above that the quantities $M$ and $m$ defined above also satisfy the conditions
    $M \,=\, {\sup}\,\left\{{\displaystyle h'(y): c\,<\,y\,<\,d}\right\}$ and $m \,=\, {\inf}\,\left\{{\displaystyle h'(y): c\,<\,y\,<\,d}\right\}$.
    The desired result now follows from Part~(a) of Theorem~\Ref{ThmE50.20}. \Q


\VV

        There is a slight generalization of the Cauchy Mean-Value Theorem which, aptly enough,
    many calculus texts call the `Generalized Mean-Value Theorem'; we include it here for sake of completeness.
    Its main advantage is that it treats the functions $f$ and $g$ more equally; in particular,
    there is no hypothesis concerning the nonvanishing of either function or their derivatives.

\V

             \subsection{\small{\bf Theorem} (The Generalized Mean-Value Theorem)}
            \label{ThmE50.45}\IndB{mean-value theorems for derivatives}{generalized form}

\V

        Suppose that $f$ and $g$ are continuous on the closed interval $[a,b]$ and differentiable on the open interval $(a,b)$.
    Then there exists a number $p$ in $(a,b)$ such that
        \begin{equation}
        \label{EqnE.110B}
        (f(b)-f(a))g'(p) \,=\, f'(p)(g(b) - g(a)).
        \end{equation}

\V

        The simple proof is left as an exercise. \Q

\VV

             \subsection{\small{\bf Remarks}}
            \label{RemrkE50.50}

\V


\hspace*{\parindent}(1) In most elementary calculus texts the Lagrange Mean-Value Theorem is used to prove those results in Section~\Ref{SectE40}
    which relate the sign of the derivative $f'$ to the changes in the values of the original function~$f$.

\V

        (2) The Lagrange Mean-Value Theorem is sometimes called the `Law of the Mean', to distinguish it from various
    `mean-value theorems for definite integrals'. In contrast, French mathematics texts often use the name
    `Theorem of Finite Changes' (or, more precisely, the French equivalent of that phrase),
    reflecting the fact that the theorem is concerned with how the derivative affects the changes in the values of the function.

\V

        (3) Many authors drop the explicit reference to `Lagrange' and use the phrase `Mean-Value Theorem' to refer to the Lagrange version, 
    Theorem~\Ref{ThmE50.20}, studied above. We often follow that usage in {\ThisText}.

\V

        (4) What we call here the Cauchy Version of the Mean-Value Theorem is called the Generalized Mean-Value Theorem by some texts.

\VV

%%%
\begin{quotation}
{\footnotesize \underline{\Note}\IndB{\notes}{on calculus pedagogy, Part~2} (on calculus pedagogy, Part~2)
 (a) Most students find the name `Mean-Value Theorem' to be obscure. To understand its roots,
    one must use some ideas from elementary calculus that have not yet been covered in {\ThisText}.
    However, since the purpose is to explain terminology, and not to prove anything,
    it seems safe enough to pause the purely logical flow of material for a moment.

        Recall from elementary integral calculus: if a variable quantity $u$ is a continuous function of $x$ for $a\,\,{\leq}\,\,x\,\,{\leq}\,\,b$,
    then one defines the {\em average value $\overline{u}$ of $u$ over the interval $[a,b]$} by the equation 
        \begin{displaymath}
        \overline{u} \,=\, \frac{1}{b-a}\,\int_{a}^{b} u\,dx \h ({\ast})
        \end{displaymath}
    Note that in subjects such as probability and statistics one frequently uses the phrase `mean value' instead of `average value'.
    In particular, suppose that $u \,=\, f'(x)$, where $y \,=\, f(x)$ is a function whose derivative $f'$ is continuous on the closed interval $[a,b]$.
    Then combining the preceding definition of `mean value' with the Fundamental Theorem of Calculus, one gets
        \begin{displaymath}
        \overline{u} \,=\, \frac{1}{b-a}\int_{a}^{b} f'(x)\,dx \,=\, 
    \frac{f(b)-f(a)}{b-a}.
        \end{displaymath}
    This equation says that the fraction ${\displaystyle \frac{f(b)-f(a)}{b-a}}$ which appears in the Lagrange Mean-Value Theorem is, in fact, a `mean value':
    it equals the average (i.e., mean) value of the function $f'$ over the interval $[a,b]$.


        The Cauchy Mean-Value Theorem has a similar interpretation in terms of average values.
    It is based on the `Change-of-Variables' formula from elementary integral calculus.

        More precisely, let us introduce a new variable $t$ by the equation $t \,=\, g(x)$, $a\,\,{\leq}\,\,x\,\,{\leq}\,\,b$.
    Assume, for simplicity, that $g'$ is continuous and positive on $[a,b]$, so that $t$ ranges over the interval $[c,d]$, where $c \,=\, g(a)$ and $d \,=\, g(b)$.
    Then $g^{-1}$ is defined and continuously differentiable on $[c,d]$, with $a \,=\, g^{-1}(c)$ and $b \,=\, g^{-1}(d)$.
    One can then write $x \,=\, g^{-1}(t)$, so that the variable $y \,=\, f(x)$ can be expressed as a function of $t$:
        \begin{displaymath}
        y \,=\, f(g^{-1}(t)), \h c\,\,{\leq}\,\,t\,\,{\leq}\,\,d.
        \end{displaymath}
    Then by the Chain Rule one has
        \begin{displaymath}
        \int \frac{dy}{dx}\,dx \,=\, \int \frac{dy}{dx}\left(\frac{dx}{dt}\,dt\right) \,=\, 
    \int \frac{dy}{dt}\,dt
        \end{displaymath}
    Thus, the Change-of-Variables Formula says
        \begin{displaymath}
        f(b)-f(a) \,=\, \int_{a}^{b} \frac{dy}{dx}\,dx \,=\, \int_{c}^{d} \frac{dy}{dt}\,dt
        \end{displaymath}
    Divide both sides by $g(b)-g(a) \,=\, d-c$ to get
        \begin{displaymath}
        \frac{f(b)-f(a)}{g(b)-g(a)} \,=\, \frac{1}{d-c} \int_{c}^{d} \frac{dy}{dt}\,dt.
        \end{displaymath}
    Thus, the ratio ${\displaystyle \frac{f(b)-f(a)}{g(b)-g(a)}}$ which appears in the Cauchy Mean-Value Theorem is the average value over the interval $[c,d]$ of the quantity $dy/dt$, where $y \,=\, f(x)$ for $x$ in $[a,b]$.

\V

        (b) The careful reader may have noticed the consistent use above of a hyphen in the phrase `mean-value theorem'.
    The reason for this may become clearer if one substitutes the equivalent phrase `average-value theorem'.
    Without the hyphen, one could interpret the phrase `average value theorem' as referring to a `value theorem' which is neither too complicated nor too easy, 
    but just `average' in difficulty; of course, that is {\em not} the intended meaning.
}%EndFootNoteSize
\end{quotation}
%##

%------------------ D
\StartSkip{
                \section{{\bf Taylor's Formula with Remainder}}
                \label{SectE60}\IndB{ZZ Sections}{\Ref{SectE60} Taylor's formula with remainder}


\VV

        Corollary~\Ref{CorE45.75} above can be used to give simple poofs of some important theorems.

        {\bf Extensions of the Mean-Value Theorem for Derivatives}

\VV

        In the `segment' formulation of the Mean-Value Theorem, as given in Equation~\Ref{EqnE.100D},
    the numbers $a$ and $b$ are thought of as the endpoints of the  `fixed'interval $[a,b]$.
    It is useful to replace the constant $b$ by a `variable' $x$ and rewrite the result in a slightly more flexible form:
        \begin{equation}
        \label{EqnE.100E}
        f(x) \,=\, f(a) + f'(c)\,(x-a) \mbox{ for some $c$ in $\mbox{Seg}\,[x,a]$}.
        \end{equation}
    In this formulation, one thinks of $a$ as a `constant' number in the domain of~$f$, while the quantity $x$ is allowed to vary.
    Thus one can think of $f(a)$ as providing an approximation, or estimate, of $f(x)$ for all $x$ near~$a$,
    and quantity $f'(c)\,(x-c)$ as the `remainder' term needed to correct the error $R_{0}(x) \,=\, f(x)-f(a)$ in the approximation $f(x) \,{\approx}\, f(a)$.
    Of course, this form of the theorem tells one only that such a number~$c$ exists, but not how to determine it.
    The Lagrange formulation, in contrast, provides the same result in a more useful form.
    Indeed it provides upper and lower bounds for for this remainder term:
        \begin{equation}
        \label{IneqE.100F}
        m\,\,{\leq}\,\,R_{0}(x)\,\,{\leq}\,\,M,
        \end{equation}
    where $m$ and $M$ are lower and upper bounds, respectively, of the set $S \,=\, \{f'(u): u{\in}\mbox{Seg}\,[a,x]\}$.

        It should come as no surprise, then, that if one has information about not just the first derivative of $f$,
    but also derivatives of higher order, then one should be able to improve this `approximation' process.

\VV

            \subsection{\small{\bf Example}}
            \label{ExampE60.10}

\V

        Suppose that $f:I \,{\rightarrow}\, {\RR}$ is {\em twice} differentiable on an open interval~$I$.
    Let $a$ and $x$ be points of the interval~$I$, and let $m$ and $M$ be lower and upper bounds of $f''$ on~$I$; that is,
        \begin{displaymath}
        m\,\,{\leq}\,\,f''(u)\,\,{\leq}\,\,M \mbox{ for all $u$ in~$I$}.
        \end{displaymath}
    To simplify this example, assume that $m$ and $M$ are both finite. Much as in the proof of Case~1 above of the Lagrange Mean-Value Theorem,
    let $g:{\RR} \,{\rightarrow}\, {\RR}$ be given by the formula $g(u) \,=\, M\,u$,
    so that $g'(u) \,=\, M$ and thus $f''(u)\,\,{\leq}\,\,g'(u)$ for all~$u$ in~$I$.
    Use Part~(a) of Corollary~\Ref{CorE40.65}, and the fact that $f'' \,=\, (f')'$, to get
        \begin{displaymath}
        f'(u) - f'(a)\,\,{\leq}\,\,g'(u) - g'(a) \,=\, M\,(u-a) \mbox{ when $u\,\,{\geq}\,\,a$}.
        \end{displaymath}
    Note that the expressions on the far left and the far right, as functions of~$u$,
    are the (unique) antiderivatives of $f''(u)$ and $g'(u)$ which equal~$0$ at~$(a)$.
    Next, let $G$ be an antiderivative of~$g$; the obvious choice is given by $G(u) \,=\, {\displaystyle M\,\frac{(u-a)^{2}}{2}}$.
    Then by the same corollary one gets
        \begin{displaymath}
        M\,\frac{(u-a)^{2}}{2}\,\,{\geq}\,\,
        \end{displaymath}
    


        Let $I$ be an open interval in~${\RR}$, and that $c$ is a point of the interval~$I$.
    Suppose that $f:I \,{\rightarrow}\, {\RR}$ is a smooth (i.e., $C^{{\infty}}$) function on~$I$.

        Recall that in ordinary language to say that two real numbers $A$ and $B$ are {\bf approximately equal}, written $A \,{\approx}\, B$,
    means that the nonnegative quantity $|B-A|$ is small. For example, in calculus one often interprets an equation such as $\lim_{x \,{\rightarrow}\, c} f(x) \,=\, f(c)$ to mean,
    speaking intuitively, that when $|x-c|$ is small, then $|f(x)-f(c)|$ is also small; that is, $f(x) \,{\approx}\, f(c)$ when $x \,{\approx}\, c$.
    Of course, vague statements like these are not precise enough for a rigorous treatment of analysis, but they do guide one's intuition.
    Since the purpose of this set of examples is to lead one to a fairly complicated definition below, the use of `approximate equality' is appropriate.

\V

        (1) From the basic properties of `continuity on an interval', one has
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\, c} f(x) \,=\, f(c).
        \end{displaymath}
    Hence, as noted above, one can write
        \begin{displaymath}
        f(x) \,{\approx}\, f(c) \mbox{ when $x \,{\approx}\, c$}.
        \end{displaymath}
    The error in this last approximation, i.e., the amount by which $f(c)$ fails to equal~$f(x)$, is the quantity $E_{0}(x) \,=\, f(x)-f(c)$.
    This last equation can be written $f(x) \,=\, f(c) + E_{0}(x)$. In the next example we improve this initial approximation of $f(x)$
    by using the derivative to estimate the error~$E_{0}(x)$.

\V

        (2)  One can use the first derivative of~$f$ to estimate the error $E_{0}(x)$ just obtained, at least when $x \,{\approx}\, c$.
    Indeed, from the definition of `derivative', together with the obvious fact that $E_{0}(c) \,=\, 0$, one has $E_{0}(x) \,=\, E_{0}(x) - E_{0}(c)$, so that
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\, c} \frac{E_{0}(x)}{x-c} \,=\, \lim_{x \,{\rightarrow}\, c} \frac{f(x)-f(c)}{x-c} \,=\, f'(c).
        \end{displaymath}
    This leads to a simple estimate for $E_{0}(x)$:
        \begin{displaymath}
        \frac{E_{0}(x)}{x-c} \,=\, \frac{f(x)-f(c)}{x-c}  \,{\approx}\, f'(c) \mbox{ and thus }
        E_{0}(x) \,{\approx}\, f'(c)\,(x-c)  \mbox{ when $x \,{\approx}\, c$}.
        \end{displaymath}
    Replace $E_{0}(x)$ in the equation $f(x) \,=\, f(c) + E_{0}(x)$ by the estimate just obtained
    to obtain a new (and, presumably, improved) approximation of~$f(x)$:
        \begin{displaymath}
        f(x) \,{\approx}\, f(c) + f'(c)\,(x-c)
        \end{displaymath}
    The error in this new approximation is $E_{1}(x) \,=\, f(x) - (f(c) + f'(c)\,(x-c))$, so that $f(x) \,=\, f(c) + f'(c)\,(x-c) + E_{1}(x)$.
    Note that $E_{1}(c) \,=\, E_{1}'(c) \,=\, 0$.

\V

        (3) In a like manner, one can use the {\em second} derivative of $f$ to estimate the error $E_{1}$ just obtained.
    Indeed,
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\, c} \frac{E_{1}(x)}{(x-c)^{2}}
     \,=\, 
        \lim_{x \,{\rightarrow}\, c} \frac{f(x) - (f(c) + f'(c)\,(x-c))}{(x-c)^{2}}
     \,=\, 
        \frac{f''(c)}{2},
        \end{displaymath}
    where the final equation comes from using L'H\^{o}pital's Rule twice. In turn, the last equation can be written as an approximation:
        \begin{displaymath}
        f(x) \,{\approx}\, f(c) + f'(c)\,(x-c) + f''(c)\,\frac{(x-c)^{2}}{2} \mbox{ when $x \,{\approx}\, c$}.
        \end{displaymath}
    The error in this approximation is $E_{2}(x) \,=\, {\displaystyle f(x) - \left(f(c) + f'(c)\,(x-c) + f''(c)\,\frac{(x-c)^{2}}{2}\right)}$.
    Note that $E_{2}(c) \,=\, E_{2}'(c) \,=\, E_{2}''(c) \,=\, 0$.

\V

        (4) In a similar way one can estimate the error $E_{2}(x)$ just obtained when $x \,{\approx}\, c$.
    Indeed, one has, by repeated use of L'H\^{o}pital's Rule,
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\, c} \frac{E_{2}(x)}{(x-c)^{3}}
     \,=\, 
        \lim_{x \,{\rightarrow}\, c} f(x) - \left(f(c) + f'(c)\,(x-c) + {\displaystyle f''(c)\,\frac{(x-c)^{2}}{2}}\right)
     \,=\, 
        \frac{f'''(c)}{3{\cdot}2}.
        \end{displaymath}
    Thus one gets $E_{2}(x) \,{\approx}\, {\displaystyle f'''(c)\,\frac{(x-c)^{3}}{2{\cdot}3}}$.

\V

        (5) Continuing this way, one sees a pattern setting up. More precisely, for each $k$ in~${\NN}$ one is led to the approximation
        \begin{displaymath}
        f(x) \,{\approx}\, f(c) + f'(c)\,(x-c) + f''(c)\,\frac{(x-c)^{2}}{2} + f'''(c)\,\frac{(x-c)^{3}}{3{\cdot}2} + \,{\ldots}\, + f^{(k)}(c)\,\frac{(x-c)^{k}}{k!}
        \end{displaymath}
    The error $E_{k}(x)$ for this approximation is given by
        \begin{displaymath}
        E_{k}(x) \,=\, f(x) - \left(f'(c)\,(x-c) + f''(c)\,\frac{(x-c)^{2}}{2} + f'''(c)\,\frac{(x-c)^{3}}{3{\cdot}2} + \,{\ldots}\, + f^{(k)}\,\frac{(x-c)^{k}}{k\,!}\right).
        \end{displaymath}
    By more applications of L'H\^{o}pital's Rule, one also gets
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\, c} \frac{E_{k}(x)}{(x-c)^{k+1}}
     \,=\, 
        \frac{f^{(k+1)}(c)}{(k+1)!}, \mbox{ hence } E_{k}(x) \,{\approx}\, f^{(k+1)}(c)\frac{(x-c)^{k+1}}{(k+1)!} \mbox{ when $x \,{\approx}\, c$}.
        \end{displaymath}

\VV

        The preceding examples suggest that the function $f$
    can be approximated near $c$ by certain polynomials whose coefficients involve the derivatives of $f$ at~$c$.
    The next definition provides the standard names of these polynomials. Note that the hypothesis used above,
    that $f$ be $C^{{\infty}}$, which was included to simplify the treatment, is weakened below.

\V

            \subsection{\small{\bf Definition} (Taylor Polynomials; the Taylor Approximation)}
            \label{DefE60.20}

        Let $f:I \,{\rightarrow}\, {\RR}$ be a $k$-times differentiable function on an open interval $I$, where $k{\in}{\NN}$.

\V

        (1) If $c$ is a point of $I$ then the {\bf Taylor Polynomial of order $k$ for $f$ about the center point $\Bfm{c}$}\IndB{functions}{Taylor polynomials}
    is the polynomial $p_{k}:{\RR} \,{\rightarrow}\, {\RR}$ given by the formula
        \begin{equation}
        \label{EqnE.120A}
        p_{k}(x) \,=\, f(c) + f'(c)(x-c) + \frac{f''(c)(x-c)^{2}}{2} + \frac{f^{(3)}(c)(x-c)^{3}}{3!} + \,{\ldots}\,+ \frac{f^{(k)}(c)(x-c)^{k}}{k!}
        \end{equation}

\V

        (2) The statement `$f(x) \,{\approx}\, p_{k}(x) \mbox{ for $x$ in $I$ near $c$}$'
    is called the {\bf Taylor Approximation of $f$ of order $k$ for $f$ near $c$}.
    The difference $E_{k}(x) \,=\, f(x) - p_{k}(x)$ is called the {\bf error} of this approximation.

\V

        (3) In the important special case in which the center point $c$ is the origin $0$, many authors use the name `Maclaurin' in place of the name `Taylor',
    and the explicit reference to $c$ is dropped. Thus the {\bf Maclaurin polynomial of order $k$ for $f$}\IndB{functions}{Maclaurin polynomials}
    is the polynomial one obtains from Equation~\Ref{EqnE.120A} by replacing $c$ with $0$ and $(x-c)$ with $x$; in other words, the polynomial
        \begin{equation}
        \label{EqnE.120B}
        p_{k}(x) \,=\, f(0) + f'(0)x + \frac{f''(0)x^{2}}{2} + \frac{f^{(3)}(0)x^{3}}{3!} + \,{\ldots}\,+ \frac{f^{(k)}(0)x^{k}}{k!}
        \end{equation}
    In {\ThisText} the `Maclaurin' terminology will normally not be used, since `Taylor polynomial about the center point $c \,=\, 0$'
    is more directly descriptive.

        \underline{Note} Brook Taylor and Colin Maclaurin were British mathematicians who were active in the early eighteenth century.

\VV

            \subsection{\small{\bf Remarks}}
            \label{RemrkE60.25}

\V


\hspace*{\parindent}(1) At times one may wish to compare the Taylor polynomials for a given function $f$ about two different choices of centers, $c_{1}$ and $c_{2}$;
    or one may wish to compare the Taylor polynomials about $c$ for two different functions $f$ and $g$.
    If no simpler notational alternatives are available, one can always resort to a more descriptive notation such as
    $p[f;c]_{k}(x)$ as a symbol for the expression on the right side of Equation~\Ref{EqnE.120A}.

\V

        (2) It is an easy consequence of the definition of the Taylor polynomials that one has $f^{(j)}(c) \,=\, p_{k}^{(j)}(c)$ for all $j \,=\, 0,1,\,{\ldots}\,k$.
    In addition, since $p_{k}$ is a polynomial of degree $k$ or less, it is clear that $p_{k}^{(k+1)}(x) \,=\, 0$ for all $x$

\V

        (3) Many authors refer to the polynomial $p_{k}$ described above as the Taylor polynomial of {\em degree} $k$ for $f$, instead of {\em order $k$}.
    It is a subtle difference; but the `order' terminology is preferable, since if $f^{(k)}(c)$ happens to equal $0$, then the degree of the polynomial $p_{k}$ is actually less than $k$.
    In other words, the `order' terminology refers to the index $k$ in the notation $p_{k}(x)$, not to the degree of the polynomial $p_{k}$.

\VV

            \subsection{\small{\bf Simple Examples}}
            \label{ExampE60.30}

\V

\hspace*{\parindent}(1) Let $f:{\RR} \,{\rightarrow}\, {\RR}$ be given by the rule $f(x) \,=\, x^{4}$ for all $x$.
    One computes
        \begin{displaymath}
        f(x) \,=\, x^{4}; \h f'(x) \,=\, 4x^{3}; \h f''(x) \,=\, 12x^{2}; \h f^{(3)}(x) \,=\, 24x; \h f^{(4)}(x) \,=\, 24; \mbox{ and}
        \end{displaymath}
        \begin{displaymath}
         f^{(k)}(x) \,=\, 0
\mbox{ for all $k$ in ${\NN}$ with $k\,\,{\geq}\,\,5$}
        \end{displaymath}

        Here are the Taylor polynomials for $f$ about two different choices of center point $c$.

        \underline{The Case $c \,=\, 0$}: One has $f(0) \,=\, f'(0) \,=\, f''(0) \,=\, f^{(3)}(0) \,=\, 0$, $f^{(4)}(0) \,=\, 24$, $f^{(k)}(0) \,=\, 0$ for $k\,\,{\geq}\,\,5$.
    Thus the corresponding Taylor polynomials are
        \begin{displaymath}
        p_{1}(x) \,=\, 0; \h p_{1}(x) \,=\, 0; \h p_{2}(x) \,=\, 0; \h p_{3}(x) \,=\, 0 \h p_{k}(x) \,=\, x^{4} \mbox{ for all $k\,\,{\geq}\,\,4$}.
        \end{displaymath}

        \underline{The Case $c \,=\, -1$}: In this case one has $f(-1) \,=\, 1$, $f'(-1) \,=\, -4$, $f''(-1) \,=\, 12$,
    $f^{(3)}(-1) \,=\, -24$, $f^{(4)}(-1) \,=\, 24$, and $f^{(k)}(-1) \,=\, 0$ for all $k\,\,{\geq}\,\,5$.
    Here are the corresponding Taylor polynomials:
        \begin{displaymath}
        p_{0}(x) \,=\, 1; \h p_{1}(x) \,=\, 1-4(x+1); \h p_{2}(x) \,=\, 1-4(x+1) + 6(x+1)^{2};
        \end{displaymath}
        \begin{displaymath}
        p_{3}(x) \,=\, 1-4(x+1) + 6(x+1)^{2} - 4(x+1)^{3}; \h p_{k}(x) \,=\, 1-4(x+1) + 6(x+1)^{2} - 4(x+1)^{3} + (x+1)^{4} \mbox{ for all $k\,\,{\geq}\,\,4$}.
        \end{displaymath}

\V

        (2) Let $f \,=\, {\exp}$ and $c \,=\, 0$. Since $f'(x) \,=\, f(x)$ for all $x$, one has $f^{(k)}(x) \,=\, f(x)$ for all $x$.
    Thus $f^{(k)}(0) \,=\, f(0) \,=\, 1$ for all $k \,=\, 0,1,2,\,{\ldots}\,$.
    Thus, the corresponding Taylor polynomials are
        \begin{displaymath}
        p_{k}(x) \,=\, 1 + x + \frac{x^{2}}{2} + \frac{x^{3}}{3!} + \,{\ldots}\, + \frac{x^{k}}{k!} \mbox{ for each $k \,=\, 0,1,2,\,{\ldots}\,$}.
        \end{displaymath}

\V

        (3) Let $f \,=\, {\sin}\,$ and $c \,=\, 0$.
    Then one computes that $f' \,=\, {\cos}\,$, $f'' \,=\, -{\sin}\,$, $f''' \,=\, -{\cos}\,$, $f^{(4)} \,=\, {\sin}\,$, and so on.
    More generally,
        \begin{displaymath}
        f^{(2k-2)} \,=\, (-1)^{k-1}{\sin}\,\, \h f^{(2k-1)} \,=\, (-1)^{k-1}{\cos}\, \mbox{ for each $k \,=\, 1,2,\,{\ldots}\,$}.
        \end{displaymath}
    Then from the usual properties of sine and cosine, one gets
        \begin{displaymath}
        f(0) \,=\, 0, \h f'(0) \,=\, 1, \h f''(0) \,=\, 0, \h f^{(3)}(0) \,=\, -1, \h f^{(4)}(0) \,=\, 0,
        \end{displaymath}
    and so on. The general rule is
        \begin{displaymath}
        f^{(2k-2)}(0) \,=\, 0, f^{(2k-1)}(0) \,=\, (-1)^{k-1} \mbox{ for $k \,=\, 1,2,\,{\ldots}\,$}.
        \end{displaymath}
    The corresponding Taylor polynomials are
        \begin{displaymath}
        p_{0}(x) \,=\, 0; \h p_{1}(x) \,=\, p_{2}(x) \,=\, x; \h p_{3}(x) \,=\, p_{4}(x) \,=\, x - \frac{x^{3}}{3!}; \,{\ldots}\,
        \end{displaymath}
    The general rule is
        \begin{displaymath}
        p_{2k-1}(x) \,=\, p_{2k}(x) \,=\, x - \frac{x^{3}}{3!} + \frac{x^{5}}{5!} - \frac{x^{7}}{7!} + \,{\ldots}\,+ (-1)^{k-1}\frac{x^{2k-1}}{(2k-1)!}
        \end{displaymath}

\VV

        The next theorem summarizes the basic conclusions of Example~\Ref{ExampE60.10} using the `Taylor polynomial' terminology.
    The proof is omitted here since it reduces to repeating the calculations carried out in that example.

\V

            \subsection{\small{\bf Theorem}}
            \label{ThmE60.35}
\V


        Let $f:I \,{\rightarrow}\, {\RR}$ be a $C^{k}$ function on an open interval $I$, where $k{\in}{\NN}$.
    Suppose that $c$ is a point of $I$, and for each $j \,=\, 0,1,\,{\ldots}\,k$ let $p_{j}$
    denote the Taylor polynomial of order $j$ for $f$ about the center point~$c$.

\V


        (a) One has
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\, c} \frac{f(x)-p_{k}(x)}{(x-c)^{k}} \,=\, 0.
        \end{displaymath}

\V

        (b) One also has
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\, c} \frac{f(x)-p_{k}(x)}{(x-c)^{k+1}} \,=\, \frac{f^{(k+1)}(c)}{(k+1)!}.
        \end{displaymath}

\V

        {\bf Remark} A more precise statement of the error in the Taylor approximation is given later (`Taylor's Formula with Remainder').

%---------------- Start 2J Material --------

\VV


        The Taylor polynomials of a function $f$ are officially defined in terms of the derivatives of $f$; see Equation~\Ref{EqnE.120A} above.
    Thus, it may appear that the only way to compute these polynomials is to carry out the indicated repeated differentiations on the function~$f$.
    However, there are alternate ways of characterizing the Taylor polynomials which sometimes are easier to use than the `official' definition;
    indeed, some of these alternate ways do not involve the calculation of any derivatives at all.
    The next result lists several such characterizations.

\V

            \subsection{\small{\bf Theorem}}
            \label{ThmE60.40}
\V

        Let $f:I \,{\rightarrow}\, {\RR}$ be a $k$-times differentiable function on an open interval $I$, where $k{\in}{\NN}$.
    Suppose that $c$ is a point of $I$, and let $p_{k}$ denote the Taylor polynomial of order $k$ for $f$ about the center point~$c$.
    Then:

        (a) The Taylor polynomial $p_{k}(x)$ is a polynomial, of degree $k$ or less, such that $p_{k}^{(j)}(c) \,=\, f^{(j)}(c)$ for all $j \,=\, 0,1,2, \,{\ldots}\, k$.

\V

        (b) Conversely, suppose that $q(x)$ is a polynomial, of degree $k$ or less, such that $q^{(j)}(c) \,=\, f^{(j)}(c)$ for all $j \,=\, 0,1,2, \,{\ldots}\, k$.
    Then $q(x) \,=\, p_{k}(x)$.

\V

        (c) Suppose that $q(x)$ is a polynomial, of degree $k$ or less, such that ${\displaystyle \lim_{x {\rightarrow} c} \frac{f(x)-q(x)}{(x-c)^{k}} \,=\, 0}$.
    Then $q(x) \,=\, p_{k}(x)$.

\V

        (d) Suppose that $q(x)$ is a polynomial of the form $q(x) \,=\, b_{0}+b_{1}\,(x-c)+ \,{\ldots}\, +b_{k}\,(x-c)^{k}$.
    Then $q(x)$ equals the Taylor polynomial $p_{k}(x)$ of $f(x)$ about the center point $c$
    if, and only if, $b_{j} \,=\, f^{(j)}(c)/n!$ for $j \,=\, 0,1, \,{\ldots}\, k$.

\V

        {\bf Proof}\,

\V

        (a) and (b) These simple proofs are left as exercises.
\V

        (c) Note that
        \begin{displaymath}
        \frac{f(x)-q(x)}{(x-c)^{k}} \,=\, \frac{f(x)-p_{k}(x)}{(x-c)^{k}} +
        \frac{p_{k}(x) - q(x)}{(x-c)^{k}} \h ({\ast})
        \end{displaymath}
    By hypothesis, the fraction on the left side of Equation~$({\ast})$ approaches $0$ as $x$ does.
    By Theorem~\Ref{ThmE60.35}, the first fraction on the right side of this equation also approaches~$0$ with~$x$.
    Thus the same must be true for the second fraction on the right. That is, one has
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\, c} \frac{p_{k}(x)-q(x)}{(x-c)^{k}} \,=\, 0
        \end{displaymath}
    However, $p_{k}(x)-q(x)$ is a polynomial of degree $k$ or less, and it is easy to see that the only way this last limit fact can hold is if $p_{k}(x)-q(x) \,=\, 0$ for all $x$.
    Thus, $q(x) \,=\, p(x)$, as claimed.

\V

        Part~(d) is a special case of the following well-known result from high-school algebra:

        `A necessary and sufficient condition for the polynomials $a_{0} + a_{1}(x-c) +  \,{\ldots}\,  + a_{k}(x-c)^{k}$ and $b_{0} + b_{1}(x-c)+ \,{\ldots}\, +b_{k}(x-c)^{k}$ to be equal is that $a_{j} \,=\, b_{j}$ for $j \,=\, 0,1, \,{\ldots}\, k$.'

\noindent This is usually proved, without using calculus, by induction on~$k$; the details are left as an exercise.

\V

            \subsection{\small{\bf Corollary}}
            \label{CorE60.50}

        Suppose that $f$ is itself a polynomial of degree $k$ or less.
    Let $c$ be any number, and let $p[f;c]_{k}$ denote the Taylor polynomial of order $k$ for $f$ centered at~$c$.
    Then $p[f;c]_{k}(x) \,=\, f(x)$, independent of the choice of center point $c$.

\V

    \underline{Proof} Apply Part~(c) of the preceding theorem with $q \,=\, f$.

\V

        \underline{Warning} The preceding corollary does {\em not} say that $p[f;c]_{j}(x)$ is independent of $c$ when $j\,<\,k$; see Example~(1) below.

\VV


            \subsection{\small{\bf More-Complicated Examples}}
            \label{ExampE60.60}

\hspace*{\parindent} 
        (1) Suppose that $f(x) \,=\, 2x^{2}-7x+5$ for all $x$ in ${\RR}$.
    Let us compute $p[f;c]_{j}(x)$ for orders $j \,=\, 0, 1, 2$ and two different center points, namely $c \,=\, 0$ and $c \,=\, 2$.
    To do this, note that
        \begin{displaymath}
        f(x) \,=\, 2x^{2}-7x+5; \h f'(x) \,=\, 4x-7; \h f''(x) \,=\, 4
        \end{displaymath}
    Thus,
        \begin{displaymath}
        f(0) \,=\, 5; \h f'(0) \,=\, -7; \h \frac{f''(0)}{2} \,=\, 2
        \end{displaymath}
    From this one gets
        \begin{displaymath}
        p[f;0]_{0}(x) \,=\, 5; \h p[f;0]_{1}(x) \,=\, 5 -7(x-0) \,=\, 5-7x;
        \end{displaymath}
        \begin{displaymath}
     p[f;0]_{2}(x) \,=\, 5 -7(x-0) + 2(x-0)^{2} \,=\, 5-7x+2x^{2} \,=\, f(x).
        \end{displaymath}
    Note that the `initial' Taylor polynomials $p[f;0]_{0}$ and $p[f;0]_{1}$ about the center point $c \,=\, 0$
    are given by the same formulas as the constant and linear terms of the of the expression $5-7x+2x^{2}$,
    which is simply the original formula for $f(x)$, but starting with the lowest powers of $x$.

        Similarly, one computes
        \begin{displaymath}
        f(2) \,=\, -1; \h f'(2) \,=\, 1; \h \frac{f''(2)}{2} \,=\, 2.
        \end{displaymath}
    Thus,
        \begin{displaymath}
        p[f;2]_{0}(x) \,=\, -1; \h p[f;2]_{1}(x) \,=\, -1 + 1{\cdot}(x-2) \,=\, x-3;
        \end{displaymath}
        \begin{displaymath}
        p[f;2]_{2}(x) \,=\, -1 + 1{\cdot}(x-2) + 2(x-2)^{2} \,=\, -1 + x -2 +2x^{2}-8x+8 \,=\, 2x^{2} - 7x + 5 \,=\, f(x).
        \end{displaymath}

        The fact that $p[f;0]_{2}(x) \,=\, p[f;2]_{2}(x)$ for all $x$ illustrates the conclusions of Corollary~\Ref{CorE60.50}.
    In contrast, the fact that $5-7x \,\,{\neq}\,\, x-3$, i.e., $p[f;0]_{1}(x) \,\,{\neq}\,\, p[f;2]_{1}(x)$,
    illustrates the warning  which follows the proof of that corollary.

\V

        (2) Here is an alternate way to analyse the quadratic function $f(x) \,=\, 2x^{2}-7x+5$ studied in the previous example.

        First note that if one sets $x \,=\, 2+(x-2)$, then $x^{2} \,=\, (2+(x-2))^{2} \,=\, 4+4(x-2)+(x-2)^{2}$, so that
        \begin{displaymath}
        f(x) \,=\, 5-7x+2x^{2} \,=\, 5-7(2+(x-2))+2(4+4(x-2)+(x-2)^{2}) \,=\, 
        \end{displaymath}
        \begin{displaymath}
    (5-14+8) + (-7+8)(x-2) + 2(x-2)^{2} \,=\, -1 + 1{\cdot}(x-2) + 2(x-2)^{2}.
        \end{displaymath}
    Note that the polynomial on the right is the same as the Taylor polynomial $p[f;2]_{2}(x)$ obtained in Example~(1) above.

        More generally, suppose that $c$ is any choice of center point.
    Note that
        \begin{displaymath}
        x \,=\, c+(x-c) \mbox{ and } x^{2} \,=\, (c+(x-c))^{2} \,=\, c^{2} + 2c(x-c) + (x-c)^{2}.
        \end{displaymath}
    Thus
        \begin{displaymath}
        f(x) \,=\, 5-7(c+(x-c)) + 2(c^{2} + 2c(x-c) + (x-c)^{2}) \,=\, 
    (5-7c+2c^{2}) + (-7+4c)(x-c) + 2(x-c)^{2}
        \end{displaymath}
    Since we already know (from the corollary) that $p_{2}[f;c] \,=\, f$, it follows from the calculation just carried out that
        \begin{displaymath}
        p[f;c]_{2}(x) \,=\, (5-7c+2c^{2}) + (-7+4c)(x-c) + 2(x-c)^{2}.
        \end{displaymath}
    By Part~(d) of Theorem~\Ref{ThmE60.40} it then follows that
        \begin{displaymath}
        5-7c+2c^{2} \,=\, f(c); \h -7+4c \,=\, f'(c); \h
    2 \,=\, \frac{f''(c)}{2}.
        \end{displaymath}
    In other words, we were able to determine the Taylor polynomials of $f$ around any center $c$ using only algebra (i.e., the $x \,=\, c+(x-c)$ trick), 
    and without taking any derivatives. Indeed, we can compute the derivatives of $f$ at the
    center point $c$ without actually computing the derived functions $f'(x)$ or $f''(x)$.

\V
        

        (3) It is shown in Theorem~\Ref{ThmB25.80}  -- long before the concept of `derivative' was introduced -- that if $x$ is any real number such that $x \,\,{\neq}\,\, 1$, then
        \begin{equation}
        \label{EqnE.130}
        1+x+x^{2}+x^{3}+ \,{\ldots}\, +x^{k} \,=\, (1-x^{k+1})/(1-x) \,=\, 
        \frac{1}{1-x} - \frac{x^{k+1}}{1-x}
        \end{equation}
    Now let $f(x) \,=\, 1/(1-x)$ for all $x\,{\neq}\,1$, and let $c \,=\, 0$.
    Then it follows, without any differentiation, that $p_{k}(x) \,=\, 1+x+x^{2}+ \,{\ldots}\, +x^{k}$.
    Indeed, let $q(x) \,=\, 1+x+x^{2}+ \,{\ldots}\, +x^{k}$.
    Clearly $q(x)$ is a polynomial of degree $k$.
    Also, Equation~\Ref{EqnE.130} implies that
        \begin{displaymath}
        q(x) \,=\, f(x) - \frac{x^{k+1}}{1-x},
        \end{displaymath}
    hence
        \begin{displaymath}
        \lim_{x {\rightarrow} 0} \frac{f(x)-q(x)}{x^{k}} \,=\, \lim_{x {\rightarrow} 0} \frac{x^{k+1}}{(1-x)x^{k}} \,=\, \lim_{x {\rightarrow} 0} \frac{x}{1-x} \,=\, 0.
        \end{displaymath}
    Thus, Part~(c) of Theorem~\Ref{ThmE60.40} implies that $q(x) \,=\, p_{k}(x)$.
    In addition, by using Part~(d) of the same theorem, one sees, also without any differentiation, that $f^{(k)}(0)/k! \,=\, 1$, so $f^{(k)}(0) \,=\, k!$ when $f(x) \,=\, 1/(1-x)$.


\V

        (4) Let $g:{\RR} \,{\rightarrow}\, {\RR}$ be the function given by the formula ${\displaystyle g(u) \,=\, \frac{1}{1+u^{2}}}$ for all $u$ in ${\RR}$.
    If one replace $x$ by $-u^{2}$ in Equation~\Ref{EqnE.130}, then one gets
        \begin{equation}
        \label{EqnE.140}
        1-u^{2}+u^{4}-u^{6}+\,{\ldots}\,+ (-1)^{k}u^{2k} \,=\, \frac{1+(-1)^{k+2}u^{2k+2}}{1+u^{2}}
        \end{equation}
    By an analysis similar to that done in the previous example, one sees directly (i.e., without computing any derivatives) that
        \begin{displaymath}
        p[g;0]_{2k}(u) \,=\, p[g;0]_{2k+1}(u) \,=\, 1-u^{2}+u^{4}-u^{6}+\,{\ldots}\,+ (-1)^{k}u^{2k},
        \end{displaymath}
    the polynomial on the left side of Equation~\Ref{EqnE.140}.
    From this one easily reads off the derivatives at $0$ of $g$; note that $g^{(m)}(0) \,=\, 0$ if $m$ is odd.

        In contrast, computing these Taylor polynomials {\em via} the `official' definition would require repeatedly differentiating the function $g$.
    The reader is invited to try computing $g^{(m)}(u)$ for, say, $m \,=\, 10$, and comparing the work required for that to what was needed above.


%----------------  End 2J material ---------


\VV

        The Taylor Approximation is of little value unless one has some useful information about the error in the approximation.
    The next result supplies such information.

\V

            \subsection{\small{\bf Theorem} Taylor's Formula with Remainder -- Lagrange Form}\IndB{Taylor's formula with remainder}{Lagrange's formulation}
            \label{ThmE60.70}

        Let $f:I \,{\rightarrow}\, {\RR}$ be a function which, for some nonnegative integer~$k$, is $k+1$-times differentiable on an open interval $I$.
    Let $c$ and $x$ be points of $I$ such that $x \,\,{\neq}\,\, c$, and let $J \,=\, \mbox{Seg}\,[x,c]$.
    Suppose that $m$ and $M$ are, respectively, lower and upper bounds of the set $\{f^{(k+1)}(u): u{\in}J\}$;
    the values $m \,=\, -{\infty}$ and $M \,=\, +{\infty}$ are permitted. Then
        \begin{equation}
        \label{IneqE.150A}
        m\,\,{\leq}\,\,\frac{f(x) - p[f;c]_{k}(x)}{(x-c)^{k+1}/(k+1)!}\,\,{\leq}\,\,M.
        \end{equation}
    Furthermore, if $f^{(k+1)}$ is not constant on $J$ then both of the inequalities in~\Ref{IneqE.150A} are strict.

\V

        \underline{Proof} The case $k \,=\, 0$ corresponds to the hypothesis being that $m\,\,{\leq}\,\,f'(u)\,\,{\leq}\,\,M$ for all $u$ in~$J$,
    and the conclusion being that ${\displaystyle m\,\,{\leq}\,\,\frac{f(x) - f(c)}{x-c}\,\,{\leq}\,\,M}$,
    with equality (on either side) if, and only if, the function $f'$ is constant on~$J$. This is Part~(a) of Theorem~\Ref{ThmE50.20}.

        Now suppose that for some natural number $k$ the given theorem fails to hold, and let $n$ be the least such natural number.
    Then, by what was just proved in the case $k \,=\, 0$, it is clear that the theorem holds in the case of the nonnegative integer $k \,=\, n-1$.
    Let $m \,=\, {\inf}\,\{f^{(n+1)}(u): u{\in}J\}$ and $M \,=\, {\sup}\,\{f^{(n+1)}(u): u{\in}J\}$.% FINISH!!

\V

        The left-hand inequality is automatically satisfied -- and is a strict inequality -- if $m \,=\, -{\infty}$.
    Thus, suppose $m$ is finite. Then one has $m\,\,{\leq}\,\,f^{(k+1)}(u)$ for all $u$ in $J$.
    Now apply Equation~\Ref{EqnE.85} to the constant function $m$, and Equation~\Ref{EqnE.125} to $f^{(k)}$, to get
        \begin{displaymath}
        \left(D^{-k}_{c} m\right)(x) \,=\, m\frac{(x-c)^{k}}{k!}
    \mbox{ and }
        \left(D^{-k}_{c} f^{(k)}\right)(x) \,=\, f(x) - p[f;c]_{k-1}(x)
        \end{displaymath}
    Suppose that $x\,>\,c$, and apply Inequality~\Ref{IneqE.88A} to the inequality $m\,\,{\leq}\,\,f^{(k)}$ to get
        \begin{displaymath}
        \frac{m(x-c)^{k}}{k!}\,\,{\leq}\,\,f(x)-p[f;c]_{k-1}(x).
        \end{displaymath}
    Since $(x-c)^{k}\,>\,0$ in this case, division by $(x-c)^{k}/k!$ preserves the inequality. That is
        \begin{displaymath}
        m\,\,{\leq}\,\,\frac{f(x)-p[f;c]_{k}(x)}{(x-c)^{k}/k!}.
        \end{displaymath}
    Likewise, if $x\,<\,c$ then Inequality~\Ref{IneqE.88B} implies
        \begin{displaymath}
        (-1)^{k}\frac{m(x-c)^{k}}{k!}\,\,{\leq}\,\,(-1)^{k}(f(x)-p[f;c]_{k-1}).
        \end{displaymath}
    In this case one has $(-1)^{k}(x-c)^{k} \,=\, (c-x)^{k}\,>\,0$, so division by $(-1)^{k}(x-c)^{k}/k!$ also preserves the inequality. That is,
        \begin{displaymath}
        m\,\,{\leq}\,\,\frac{(-1)^{k}(f(x)-p[f;c]_{k-1})}{(-1)^{k}(x-c)^{k}/k!} \,=\, \frac{f(x)-p[f;c]_{k-1}}{(x-c)^{k}/k!}.
        \end{displaymath}
    If $f^{(k)}$ is not constant on $J$, then it follows from Part~(b) of Theorem~\Ref{ThmE45.110} that ${\displaystyle m\,<\,\frac{f(x) - p[f;c]_{k-1}(x)}{(x-c)^{k}/k!}}$.

    A similar argument works for the right half of Inequality~\Ref{IneqE.150A}.

\V

            \subsection{\small{\bf Corollary} (Taylor's Formula with Remainder -- Derivative Form)}
            \label{CorE60.80}

         Let $f:I \,{\rightarrow}\, {\RR}$ be a function which, for some positive integer $k$, is $k$-times differentiable on an open interval $I$.
    Let $c$ and $x$ be points of $I$ such that $x \,\,{\neq}\,\, c$, and let $E_{k-1}$ denote the error in the Taylor approximation $f(x) \,{\approx}\, p[f;c]_{k-1}(x)$;
    that is, $E_{k-1} \,=\, f(x)-p[f;c]_{k-1}(x)$.
    Then there exists a number $r$ between $c$ and $x$ such that
        \begin{equation}
        \label{EqnE.160}
        E_{k-1} \,=\, \frac{f^{(k)}(r)}{k!}(x-c)^{k}
        \end{equation}
    This last relation is often written in the form
        \begin{equation}
        \label{EqnE.170}
        f(x) \,=\, f(c) + f'(c)(x-c) + \frac{f''(c)}{2!}(x-c)^{2} + \,{\ldots}\, + \frac{f^{(k-1)}(c)}{(k-1)!}(x-c)^{k-1} + \frac{f^{(k)}(r)}{k!}(x-c)^{k}
        \end{equation}

\V

        \underline{Proof} Apply the Intermediate-Value Theorem for Derivatives to the results of the previous theorem.

\V

        {\bf Remark} This is a sharpening of Theorem~\Ref{ThmE60.35}.


\V
\V

        The Taylor formula allows one to obtain `formulas' for functions such as $\exp$, $\sin$ and $\cos$.

\V

            \subsection{\small{\bf Theorem}}
            \label{ThmE60.85}

\V

\hspace*{\parindent}(a) For each $k \,=\, 0,1,2,\,{\ldots}\,$ let $p_{k}$ denote the Taylor polynomial of order $k$ of the function ${\exp}$ about the center point $c \,=\, 0$ (i.e., the Maclaurin polynomial of order $k$).
    Then for all $x$ in ${\RR}$ one has ${\exp}\,x \,=\, \lim_{k \,{\rightarrow}\, {\infty}} p_{k}(x)$.

\V

        (b) For each $k \,=\, 0,1,2,\,{\ldots}\,$ let $p_{k}$ denote the Taylor polynomial of order $k$ of the function ${\sin}$ about the center point $c \,=\, 0$ (i.e., the Maclaurin polynomial of order $k$).
    Then for all $x$ in ${\RR}$ one has ${\sin}\,x \,=\, \lim_{k \,{\rightarrow}\, {\infty}} p_{k}(x)$.

\V

        (c) For each $k \,=\, 0,1,2,\,{\ldots}\,$ let $p_{k}$ denote the Taylor polynomial of order $k$ of the function ${\cos}$ about the center point $c \,=\, 0$ (i.e., the Maclaurin polynomial of order $k$).
    Then for all $x$ in ${\RR}$ one has ${\cos}\,x \,=\, \lim_{k \,{\rightarrow}\, {\infty}} p_{k}(x)$.

\V

        \underline{Proof} (a) Note that $p_{k}(0) \,=\, 1 \,=\, {\exp}\,(0)$ for all $k$, so the result is true when $x \,=\, 0$.
    Thus suppose that $x \,\,{\neq}\,\, 0$. Then by Taylor's Formula with (Derivative) Remainder one has, for some $r_{k}$ such that $|r_{k}|\,<\,|x|$,
        \begin{displaymath}
        |{\exp}\, x - p_{k}(x)| \,=\, \frac{{\exp}(r_{k})}{(k+1)!}|x|^{k+1}\,\,{\leq}\,\,{\exp}\,(|x|)\frac{|x|^{k+1}}{(k+1)!}.
        \end{displaymath}
    By Example~\Ref{ExampC20.20}~(3) it is clear that the right side of this last inequality approaches~$0$ as $k$ approaches ${\infty}$,
    and thus $\lim_{k \,{\rightarrow}\, {\infty}} p_{k}(x) \,=\, {\exp}\,x$, as claimed.

\V

        (b) and (c): The proofs of these parts are even simpler, and are left as exercises.

\V
\V
}%\EndSkip
%------------ D

\VV

        The next well-known result is one of the standard applications of the Cauchy Mean-Value Theorem.

\V

             \subsection{\small{\bf Theorem} (L'H\^{o}pital's Rule for Functions)}
            \label{ThmE50.60}\IndB{L'H\^{o}pital's Rule}{for functions}

\V

\hspace*{\parindent}(a) Suppose that $f$ and $g$ are functions defined on a half-open interval $(c,b]$; the case $c \,=\, -{\infty}$ is allowed.
    Assume that $f$ and $g$ satisfy the following hypotheses:

        \h (i)\,\,\, $\lim_{x  \,{\rightarrow}\, c+} f(x)$ and $\lim_{x  \,{\rightarrow}\,  c+} g(x)$ both exist,
    and one of the following cases holds:

        \h \h (`$0/0$ case') each of these limits equals~$0$;

        \h \h (`${\infty}/{\infty}$ case') each limit equals one of the infinities $+{\infty}$ or $-{\infty}$.

        \h (ii)\,\, $f'(x)$ and $g'(x)$ exist for all $x$ in $(c,b]$;

        \h (iii) $g'(x) \,\,{\neq}\,\, 0$ if $x{\in}(c,b]$;

        \h (iv)\, ${\displaystyle \lim_{x  \,{\rightarrow}\, c+} \frac{f'(x)}{g'(x)}} \,=\, L$.
    (The quantity $L$ can be either a real number or one of the infinities.)

\noindent Then ${\displaystyle \lim_{x \,{\rightarrow}\, c+} \frac{f(x)}{g(x)} \,=\, L}$ as well.

\noindent Note: If $c \,=\, -{\infty}$ then the notation $\lim_{x \,{\rightarrow}\, c+}$ means the same as the more usual $\lim_{x \,{\rightarrow}\, -{\infty}}$.

\V

        (b) If, instead, $f$ and $g$ are defined on an interval of the form $[a,c)$, then the corresponding results for left-hand limits at $c$ also hold.
    Likewise, the corresponding results for two-sided limits at~$c$ hold in the case for which $f$ and $g$ are defined on the intervals $[a,c)$ and $(c,b]$ with $a\,<\,c\,<\,b$.

\V

        \underline{Proof}

\V


        (a) In light of Definition~\Ref{DefD50.10}, it suffices to prove the following:

\V

    \h \underline{Claim} For each strictly monotonic sequence ${\xi} \,=\, (x_{1},x_{2},\,{\ldots}\,)$ in $(c,b]$ such that $\lim_{k \,{\rightarrow}\, {\infty}} x_{k} \,=\, c$,
    one has $\lim_{k \,{\rightarrow}\, {\infty}} f(x_{k})/g(x_{k}) \,=\, L$.

\V

\noindent To see that `Claim' is true, define sequences ${\alpha} \,=\, (a_{1}, a_{2},\,{\ldots}\,)$ and ${\beta} \,=\, (b_{1},b_{2},\,{\ldots}\,)$ by the rules $a_{k} \,=\, f(x_{k})$ and $b_{k} \,=\, g(x_{k})$ for each $k$ in ${\NN}$.
    Note that from Hypothesis~(iii) above it is clear that the function $g$ is strictly monotonic on $(c,b]$ and thus the sequence ${\beta}$ is strictly monotonic.
    Furthermore, by the Cauchy Mean-Value Theorem, for each $k$ in ${\NN}$ one has
        \begin{displaymath}
        \frac{a_{k} - a_{k+1}}{b_{k}- b_{k+1}} \,=\, \frac{f(x_{k}) - f(x_{k+1})}{g(x_{k}) - g(x_{k+1})} \,=\, \frac{f'(z_{k})}{g'(z_{k})}
        \end{displaymath}
    for some $z_{k}$ between $x_{k}$ and $x_{k+1}$.
    It follows, from the hypothesis that $\lim_{k \,{\rightarrow}\, {\infty}} x_{k} \,=\, c$ and the Squeeze Property for sequences,
that ${\displaystyle \lim_{k \,{\rightarrow}\, {\infty}} z_{k} \,=\, c}$, and thus, by Hypothesis~(iv), one has ${\displaystyle \lim_{k \,{\rightarrow}\, {\infty}} \frac{a_{k} - a_{k+1}}{b_{k} - b_{k+1}} \,=\, L}$.
    The desired result now follows by applying Theorem~\Ref{ThmC60.55A}, the Stoltz-Cesaro theorem.

\V

        (b) The proof here is similar, and is left as an exercise. \Q

\V
\V

             \subsection{\small{\bf Remarks}}
            \label{RemrkE50.70}

\V

\hspace*{\parindent}(1) The spelling `L'H\^{o}pital' used here is common, but not universal.
    Many texts write `L'Hospital' instead; some of these also drop the apostrophe.
    No matter how one decides to spell it, however, the pronunciation is the same: {\em Loh pea tahl}.

\noindent In other words, the `s' in the second spelling is silent. (Of course the `H' and the apostrophe are also silent; it is French, after all.)
    The advantage of using the `L'H\^{o}pital' spelling is that the letter `s' is not visible,
    so there is no temptation for a student to pronounce it `Luh Hahs Pee Tal', as many do.

\V

        (2) It appears that M. L'H\^{o}pital is honored with the name of this result because it appeared in his calculus text (the first such text);
    but the result itself is due to Johann~Bernoulli.

\V

            \subsection{\small{\bf Corollary}}
            \label{CorE50.80}

        Suppose that $f:I \,{\rightarrow}\, {\RR}$ is differentiable at every point of an open interval $I$.
    Then a necessary and sufficient condition for $f'$ to be continuous at a point $c$ of $I$ is that ${\displaystyle \lim_{x \,{\rightarrow}\, c}} f'(x)$ exist.
    (It is {\em not} assumed here that this limit is finite.)

\V

        The simple proof is left as an exercise. \Q

\VV

%--------------------- E
\StartSkip{ %% DON'T NEED EXAMPLES OF L'HOPITAL
            \subsection{\small{\bf Examples}}
            \label{ExampE50.90}
\V

\hspace*{\parindent}(1) Let $f,g:{\RR} \,{\rightarrow}\, {\RR}$ be given by the rules
        \begin{displaymath}
        f(x) \,=\, x^{3}-2x^{2}-x+2 \mbox{ and } g(x) \,=\, x^{2}-4
        \end{displaymath}
    In this example we consider $4$ limits involving the ratio $f(x)/g(x)$.

        \h (a) What can one say about the limit ${\displaystyle \lim_{x \,{\rightarrow}\, 3}\frac{f(x)}{g(x)}}$?

        \underline{Answer} Since $f$ and $g$ are continuous functions, one computes that
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\, 3} f(x) \,=\, f(3) \,=\, 3^{3}-2{\cdot}3^{2}-3+2 \,=\, 8
        \end{displaymath}
    and
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\, 3} g(x) \,=\, g(3) \,=\, 3^{2}-4 \,=\, 5.
        \end{displaymath}
    Since $\lim_{x \,{\rightarrow}\, 3} g(x) \,\,{\neq}\,\, 0$, one can now use Part~(d) of Theorem~\Ref{ThmC90.70} to conclude that
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\, 3}\frac{f(x)}{g(x)} \,=\, \frac{8}{5}.
        \end{displaymath}

        \h (b) What can one say about the limit ${\displaystyle \lim_{x \,{\rightarrow}\, 1}\frac{f(x)}{g(x)}}$?

        \underline{Answer} In this case one computes that
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\, 1} f(x) \,=\, f(1) \,=\, 1-2-1+2 \,=\, 0
        \end{displaymath}
    and
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\, 1} g(x) \,=\, g(1) \,=\, 1-4 \,=\, -3.
        \end{displaymath}
    Since $\lim_{x \,{\rightarrow}\, 3} g(x) \,\,{\neq}\,\, 0$, one can again use Part~(d) of Theorem~\Ref{ThmC90.70} to conclude that
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\, 3}\frac{f(x)}{g(x)} \,=\, \frac{\,0}{-3} \,=\, 0.
        \end{displaymath}

        \h (c) What can one say about the limit ${\displaystyle \lim_{x \,{\rightarrow}\,-2}\frac{f(x)}{g(x)}}$?

        \underline{Answer} In this case one computes that
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\,-2} f(x) \,=\, f(-2) \,=\, (-2)^{3}-2{\cdot}(-2)^{2}-(-2)+2 \,=\, -8-8+2+2 \,=\, -12
        \end{displaymath}
    and
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\,-2} g(x) \,=\, g(-2) \,=\, (-2)^{2}-4 \,=\, 4-4 \,=\, 0.
        \end{displaymath}
    Since $\lim_{x \,{\rightarrow}\, 3} g(x) \,=\, 0$, one {\em cannot} use Part~(d) of Theorem~\Ref{ThmC90.70}, since a key hypothesis of that result is not satisfied here.
    However, $\lim_{x \,{\rightarrow}\, 2} f(x) \,\,{\neq}\,\, 0$, so one of the hypotheses of L'H\^{o}pital's Rule is not satisfied;
    thus one {\em cannot} use L'H\^{o}pital's Rule here either.
    Nevertheless, one can say something; namely, when $x$ is near $-2$ and $x\,<\,-2$ then $f(x)$ is near $-12$ and $g(x)$ is a small positive number.
    With a little more work one sees that the left-hand limit $\lim_{x{\nearrow}-2} f(x)/g(x)$ exists and equals $-{\infty}$;
    likewise, one sees that  $\lim_{x{\searrow}-2} f(x)/g(x) \,=\, +{\infty}$.

\V

        \h (d) What can one say about the limit ${\displaystyle \lim_{x \,{\rightarrow}\,2}\frac{f(x)}{g(x)}}$?

        \underline{Answer} In this case one computes that
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\,2} f(x) \,=\, f(2) \,=\, (2)^{3}-2{\cdot}(2)^{2}-(2)+2 \,=\, 8-8-2+2 \,=\, 0
        \end{displaymath}
    and
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\,2} g(x) \,=\, g(2) \,=\, (2)^{2}-4 \,=\, 4-4 \,=\, 0.
        \end{displaymath}
    The hypotheses for L'H\^{o}pital's Rule are now satisfied. Of course, what that Rule actually tells us is to look at an entirely different limit problem, 
    namely the limit ${\displaystyle \lim_{x \,{\rightarrow}\, c} \frac{f'(x)}{g'(x)}}$,
    and determine what can be said about it. In the case at hand one has
        \begin{displaymath}
        f'(x) \,=\, 3x^{2}-4x-1 \mbox{ and } g'(x) \,=\, 2x.
        \end{displaymath}
    Since $\lim_{x \,{\rightarrow}\, 2} f'(x) \,=\, \lim_{x \,{\rightarrow}\, 2} (3x^{2}-4x-1) \,=\, 3$ and $\lim_{x \,{\rightarrow}\, 2} g'(x) \,=\, \lim_{x \,{\rightarrow}\, 2} 2x \,=\, 4 \,\,{\neq}\,\, 0$, one can use Part~(d) of Theorem~\Ref{ThmC90.70} to conclude that
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\, 2} \frac{f'(x)}{g'(x)} \,=\, \frac{3}{4}, \mbox{ hence } \lim_{x \,{\rightarrow}\, 2} \frac{f(x)}{g(x)} \,=\, \frac{3}{4}.
        \end{displaymath}

\V

        (2) Throughout this example let $I \,=\, (0,+{\infty})$ and let $c \,=\, 4$.

        \h (a) Suppose that $f(x) \,=\, \sqrt{x}-2$ and $g(x) \,=\, x-4$ on $I$.
    Note that $f$ and $g$ are differentiable on $I$ and $f(4) \,=\, g(4) \,=\, 0$,
    so $f$ and $g$ satisfy Hypotheses (i), (ii) and (iii) for L'H\^{o}pital's Rule at $c \,=\, 4$. Also, one computes that
        \begin{displaymath}
        f'(x) \,=\, \frac{1}{2\sqrt{x}} \mbox{ and } g'(x) \,=\, 1 \mbox{ for all $x$ in $I$}.
        \end{displaymath}
    Thus $\lim_{x \,{\rightarrow}\, 4} f'(x) \,=\, 1/4$ and $\lim_{x \,{\rightarrow}\, 4} g'(x) \,=\, 1 \,\,{\neq}\,\, 0$, so one has
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\, 4} \frac{f'(x)}{g'(x)} \,=\, \frac{1/4}{1} \,=\, \frac{1}{4}.
        \end{displaymath}
    Thus, Hypothesis (iv) of L'H\^{o}pital's Rule is also satisfied, so one can conclude that
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\, 4} \frac{\sqrt{x}-2}{x-4} \,=\, \frac{1}{4}.
        \end{displaymath}

        (b) Now let $f(x) \,=\, \sqrt{x}-2-(x-4)/4$ and $g(x) \,=\, (x-4)^{2}$ on $I$.
     One computes that $f(4) \,=\, g(4) \,=\, 0$, and that
        \begin{displaymath}
        f'(x) \,=\, \frac{1}{2\sqrt{x}} - \frac{1}{4} \mbox{ and }
        g'(x) \,=\, 2x-8.
        \end{displaymath}
    Thus $f$ and $g$ satisfy Hypotheses (i), (ii) and (iii) for L'H\^{o}pital's Rule.
    Unfortunately, because $f'(4) \,=\, g'(4) \,=\, 0$ one cannot use the `Quotient Rule for Limits' on the expression $\lim_{x \,{\rightarrow}\, 4} f'(x)/g'(x)$.
    This does {\em not} mean, however, that $\lim_{x \,{\rightarrow}\, 4} f'(x)/g'(x)$ does not exist;
    it simply means that one must do more work to analyse this limit.
    Indeed, note that by what has been just shown, the functions $F(x) \,=\, f'(x)$ and $G(x) \,=\, g'(x)$ do satisfy Hypotheses (i), (ii) and (iii).
    Also, one computes
        \begin{displaymath}
        f''(x) \,=\, F'(x) \,=\, -\frac{1}{4x^{3/2}} \mbox{ and } g''(x) \,=\, G'(x) \,=\, 2.
        \end{displaymath}
    Thus
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\, 4} \frac{F'(x)}{G'(x)} \,=\, 
        \lim_{x \,{\rightarrow}\, 4} \frac{-1/(4x^{3/2})}{2} \,=\, -\frac{1}{64}
        \end{displaymath}
    In particular, $F$ and $G$ also satisfy Hypothesis (iv), so now one can apply L'H\^{o}pital's Rule,
    but to the expression $\lim_{x \,{\rightarrow}\, 4} f'(x)/g'(x)$, to conclude that
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\, 4} \frac{f'(x)}{g'(x)} \,=\, \lim_{x \,{\rightarrow}\, 4} \frac{f''(x)}{g''(x)} \,=\, -\frac{1}{64}.
        \end{displaymath}
    This last result now shows that the original functions $f$ and $g$ also satisfy Hypothesis~(iv),
    so one can (finally!) use L'H\^{o}pital's Rule a second time, but now on the original problem to conclude that
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\, 4} \frac{f(x)}{g(x)} \,=\, \lim_{x \,{\rightarrow}\, 4} \frac{f'(x)}{g'(x)} \,=\, -\frac{1}{64}.
        \end{displaymath}

\V

        (c) Finally, let $f(x) \,=\, \sqrt{x}-2-(x-4)/4+(x-4)^{2}/64$ and $g(x) \,=\, (x-4)^{3}$ for all $x$ in $I$.
    By an analysis similar to that used in Part~(b), but involving {\em three} applications of L'H\^{o}pital's Rule, one can compute $\lim_{x \,{\rightarrow}\, 4} f(x)/g(x)$.
    The complete calculation is often summarized in the form
        \begin{displaymath}
         \lim_{x \,{\rightarrow}\, 4} \frac{f(x)}{g(x)}\,=\, \lim_{x \,{\rightarrow}\, 4} \frac{f'(x)}{g'(x)} \,=\, \lim_{x \,{\rightarrow}\, 4} \frac{f''(x)}{g''(x)} \,=\, \lim_{x \,{\rightarrow}\, 4} \frac{f'''(x)}{g'''(x)} \,=\, \lim_{x \,{\rightarrow}\, 4} \frac{3x^{-5/2}/8}{6} \,=\, \frac{1}{512}.
        \end{displaymath}
    In this calculation, of course, the equations are actually obtained from right to left.
    The details are left to the reader.

\V

            \subsection{\small{\bf Remark}}
            \label{RemrkE50.95}

\V

\hspace*{\parindent}
        (1) The  reader may wonder why, in Example (1) above, only one of the four parts actually involves L'H\^{o}pital's Rule.
    The answer is: in fact, {\em each} part of Example~(1) does involve L'H\^{o}pital's Rule, if only indirectly.
    More precisely, whenever one who knows elementary calculus sees a limit of the form ${\displaystyle \lim_{x \,{\rightarrow}\, c} f(x)/g(x)}$,
    the natural tendency is to assume that one is expected to use L'H\^{o}pital's Rule.
    Unfortunately, this assumption often leads to one not even checking that the hypotheses of L'H\^{o}pital's Rule are satisfied.
    Thus, the point of Parts~(a), (b) and~(c) of Example~(1) is:

        \underline{You must check that the hypotheses are satisfied before you use L'H\^{o}pital's Rule.}

}%% EndSkip
%------------------- E

\V
\V

                \section{{\bf The First Derivative and Linear Approximations}}
                \label{SectE55}\IndB{ZZ Sections}{\Ref{SectE55} First Derivative and Linear Approximations}

        The description of `derivative' given in Definition~\Ref{DefE20.20} is standard.
    The next result provides an alternative formulation.

\V

             \subsection{\small{\bf Theorem}}
            \label{ThmE55.20}

        Let $f:I \,{\rightarrow}\, {\RR}$ be a real-valued function whose domain is an open interval $I$ in ${\RR}$, and let $c$ be a point of $I$.
    Then the following statements are equivalent:

        \h (i)\, The function $f$ is differentiable at $c$. That is, the quantity ${\displaystyle \lim_{x \,{\rightarrow}\, c} \frac{f(x)-f(c)}{x-c}}$ exists and is finite.

        \h (ii) There exists a number $A$ such that for every ${\varepsilon}\,>\,0$ there exists a number ${\delta}\,>\,0$ so that if $|x-c|\,<\,{\delta}$ then
        \begin{displaymath}
        \left|f(x) - f(c) - A(x-c)\right|\,\,{\leq}\,\,{\varepsilon}|x-c| \h ({\ast})
        \end{displaymath}

\V

        {\bf Proof}\, Suppose that Statement~(i) is true, and let $A \,=\, f'(c)$. Then one has $A \,=\, \lim_{x \,{\rightarrow}\, c} {\displaystyle \frac{f(x)-f(c)}{x-c}}$, which means (by definition of `limit') that for every ${\varepsilon}\,>\,0$ there exists ${\delta}\,>\,0$ such that if $0\,<\,|x-c|\,<\,{\delta}$ then
        \begin{displaymath}
        \left|\frac{f(x)-f(c)}{x-c} - A\right|\,<\,{\varepsilon}.
        \end{displaymath}
    Multiply both sides of this inequality by the positive quantity $|x-c|$, and use the fact that if $u$ and $v$ are real numbers,
    then $|u{\cdot}v| \,=\, |u|{\cdot}|v|$, to get
        \begin{displaymath}
        |f(x)-f(c) - A(x-c)|\,<\,{\varepsilon}|x-c| \h ({\ast}{\ast})
        \end{displaymath}
    if $0\,<\,|x-c|\,<\,{\delta}$.
    Inequality~$({\ast}{\ast})$ is strict, and it clearly fails to hold if one allows $x \,=\, c$.
    However, $({\ast}{\ast})$ certainly implies the weaker inequality~$({\ast})$ when $0\,<\,|x-c|\,<\,{\delta}$;
    and Inequality~$({\ast})$ is trivially true when $x \,=\, c$, since in that case it reduces to the statement $0\,\,{\leq}\,\,0$.
    In other words, Statement~(i) implies Statement~(ii).

        Conversely, suppose that Statement~(ii) is true. Let ${\varepsilon}\,>\,0$ be given, and let ${\delta}\,>\,0$ be such that
        \begin{displaymath}
        |f(x)-f(c)-A(x-c)|\,\,{\leq}\,\,\frac{{\varepsilon}}{2}|x-c| \h ({\ast}{\ast}{\ast})
        \end{displaymath}
    for all $x$ such that $0\,\,{\leq}\,\,|x-c|\,<\,{\delta}$.
    Then Inequality~$({\ast}{\ast}{\ast})$ remains true if one restricts $x$ even further to satisfy $0\,<\,|x-c|\,<\,{\delta}$.
    In that situation,  however, one has $0\,<\,{\displaystyle \frac{{\varepsilon}}{2}}|x-c|\,<\,{\varepsilon}|x-c|$, and thus
        \begin{displaymath}
        |f(x)-f(c) - A(x-c)|\,<\,{\varepsilon}|x-c| \mbox{ if $0\,<\,|x-c|\,<\,{\delta}$}.
        \end{displaymath}
    Divide both sides of this last (strict) inequality by the positive quantity $|x-c|$, and carry out the obvious algebraic simplification, to get
        \begin{displaymath}
        \left|\frac{f(x)-f(c)}{x-c} - A\right|\,<\,{\varepsilon} \mbox{ if $0\,<\,|x-c|\,<\,{\delta}$}.
        \end{displaymath}
    It follows from this argument that
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\, c} \frac{f(x)-f(c)}{x-c} \,=\, A.
        \end{displaymath}
    In particular, $f$ is differentiable at $c$, i.e., Statement~(i) is true. Indeed, this argument shows that $f'(c) \,=\, A$. \Q

\V
\V


             \subsection{\small{\bf Remarks}}
            \label{RemrkE55.30}

\V

\hspace*{\parindent}(1) The preceding result can be interpreted in terms of the accuracy near $c$ of `linear approximations' of the function $f$.
    More precisely, suppose that Statement~(ii) of the preceding theorem is satisfied with a certain choice of $A$ (which of course we know equals $f'(c)$).
    Let $g:{\RR} \,{\rightarrow}\, {\RR}$ be another linear function such that $g(c) \,=\, f(c)$; thus $g(x) \,=\, f(c) + m(x-c)$ for some constant $m \,\,{\neq}\,\, A$.
    We wish to compare the accuracy of the two approximations,
        \begin{displaymath}
        f(x) \,{\approx}\, f(c) + A(x-c) \mbox{ and } f(x) \,{\approx}\, f(c) + m(x-c).
        \end{displaymath}
    Note that, by Inequality~\Ref{IneqB.35C}, the `Modified Triangle Inequality', one has
        \begin{displaymath}
        |f(x)-f(c)-m(x-c)| \,=\, |(A-m)(x-c) + (f(x)-f(c)-A(x-c))|\,\,{\geq}\,\,
        \end{displaymath}
        \begin{displaymath}
        \left||A-m||x-c| - |f(x)-f(c)-A(x-c)|\right|\,\,{\geq}\,\,|A-m||x-c| - |f(x)-f(c)-A(x-c)|.
        \end{displaymath}
    Let ${\varepsilon}\,>\,0$ satisfy $0\,<\,{\varepsilon}\,<\,|A-m|/2$, and let ${\delta}\,>\,0$ be small enough that
        \begin{displaymath}
        |f(x) - f(c) - A(x-c)|\,\,{\leq}\,\,{\varepsilon}|x-c| \mbox{ if $0\,<\,|x-c|\,<\,{\delta}$}.
        \end{displaymath}
    For such $x$ one then has
        \begin{displaymath}
        |f(x)-f(c)-m(x-c)|\,\,{\geq}\,\,|A-m||x-c| - |f(x)-f(c)-A(x-c)|\,\,{\geq}\,\,
        \end{displaymath}
        \begin{displaymath}
|A-m||x-c| - \frac{1}{2}|A-m||x-c| \,=\, \frac{1}{2}|A-m||x-c|\,>\,0.
        \end{displaymath}
    This implies that
        \begin{displaymath}
        \left|\frac{f(x)-f(c)-A(x-c)}{f(x)-f(c)-m(x-c)}\right|\,\,{\leq}\,\,2\frac{|f(x)-f(c)-A(x-c)|}{|A-m||x-c|}\,\,{\leq}\,\,\frac{2{\varepsilon}}{|A-m|}
        \end{displaymath}
    Since $A \,=\, f'(c)$, one can interpret this result as follows: as $x$ approaches $c$,
    the error in approximating $f(x)$ by the function $f(c) + f'(c)(x-c)$ becomes `infinitely small'
    in comparison with the error in approximating $f(x)$ by $f(c) + m(x-c)$ when $m \,\,{\neq}\,\, f'(c)$.
    This fact is reflected by the following standard terminology.

\V

        (2) In elementary calculus, the straight line given by the equation $y \,=\, f(c) + f'(c)(x-c)$ is called the {\bf tangent line} to the graph $y \,=\, f(x)$ at the point $(c,f(c))$.
    The preceding analysis then shows that near the point $(c,f(c))$ this tangent line approximates the graph of $f$ infinitely better than any other straight line that passes through that point.

\V

        The preceding results give rise to the following standard terminology.

\V

             \subsection{\small{\bf Definition}}
            \label{DefE55.35}\IndB{approximations}{tangent-line approximation}\IndB{approximations}{best linear approximation}\index{tangent-line approximation}\index{best linear approximation}

\V

        Suppose that $f:I \,{\rightarrow}\, {\RR}$ is a function defined on an open interval $I$ in ${\RR}$, and that $c$ is a point of $I$.
    Suppose that $f$ is differentiable at $c$, and define the linear function $L:{\RR} \,{\rightarrow}\, {\RR}$ by the rule $L(x) \,=\, f(c) + f'(c)(x-c)$ for all $x$ in ${\RR}$.
    Then the approximation $f(x) \,{\approx}\, L(x)$ is called the {\bf tangent-line approximation of $f$ at $c$}.
    The function $L$ itself is called the {\bf best linear approximation of $f$ at $c$}.


\V
\V


             \subsection{\small{\bf Remarks}}
            \label{RemrkE55.37}

\V


\hspace*{\parindent}(1) The content of Theorem~\Ref{ThmE55.20} can be summarized to say that  one can define the derivative of a real-valued function of a single real variable either in terms of limits (Statement~(i)) or in terms of tangent lines (Statement~(ii)).
    The `limit' viewpoint is usually preferred in single-variable calculus because the proofs of basic facts, such as the Product Rule and the Chain Rule,
    tend to be more straight forward using the `limit' approach.

        The situation is more complicated in multivariable calculus.
    Indeed, in that context the two viewpoints correspond to different concepts:
    the obvious generalization of the `limit' approach leads to the concept of `Partial Derivatives';
    while the obvious generalization of the `tangent line' approach leads to a stronger concept, the `Total Derivative', sometimes called the `Frechet derivative'.
    It turns out, for example, that for functions of two or more variables the analog of Theorem~\Ref{ThmE20.30},
    the `Differentiability-Implies-Continuity' Theorem,
    remains valid for `total derivatives', but does not hold for partial derivatives.

\V

        (2) In the preceding we follow the standard usage in high-school algebra and refer to a function of the form $L(x) \,=\, ax+b$,
    with $a$ and $b$ constant, as a `linear function of $x$'. This usage is quite standard in single-variable analysis, and needs no apologies.
    However, in the subject of Linear Algebra the word `linear' has a somewhat more restricted meaning;
for example, for a function $f:{\RR} \,{\rightarrow}\, {\RR}$ to be called `linear' in Linear Algebra,
    it would be required (among other things) that $f(0) \,=\, 0$. Thus, some careful authors
    use the word `affine' to describe a function of the form $ax+b$, and restrict `linear' to the case $b \,=\, 0$.
    Such an author would refer to {\bf \IndA{affine approximations}} instead of linear approximations.
    Since this issue is of little consequence in single-variable analysis, we do not use the `affine' terminology any further in {\ThisText}.

\VV

        The tangent-line approximation is among the most elementary of the many schemes used in applied mathematics to simplify problems.
    The basic principle of such approximations is easy to grasp: Suppose that you need to perform a certain type of operation on a complicated function $f$. 
    Replace $f$ by a simpler function $g$ which provides a good approximation of $f$, and for which the desired operation is easy to carry out;
    then carry out the operation on $g$. The expectation is that the result of doing the operation on $g$
    should provide a decent approximation of the result of actually doing the operation on $f$.

        One of the most familiar of these `approximation schemes' is {\bf \IndA{Newton's method}} for solving equations of the form $f(x) \,=\, 0$,
    where $f$ is a real-valued function which is differentiable at each point of an open interval $I$.
    (This method is also referred to as the {\bf \IndDD{Newton-Raphson method}{Newton's method}}.)
    The idea is simple: choose a point $x_{0}$ in $I$, preferably one which is close to a root $r$ of the equation $f(x) \,=\, 0$.
    Let $g_{0}:{\RR} \,{\rightarrow}\, {\RR}$ be the corresponding tangent-line approximation of $f$ at $x_{0}$,
    so that $g_{0}(x) \,=\, f(x_{0}) + f'(x_{0})(x-x_{0})$. If $x_{0}$ really is close to $r$,
    then one could reasonably expect the line $y \,=\, f(x_{0}) + f'(x_{0})(x-x_{0})$
    to remain close to the graph $y \,=\, f(x)$, at least throughout the segment $\mbox{Seg}\,[x_{0},r]$.
    And if that is the case, then it is reasonable to presume that solving the equation $f(x) \,=\, 0$
    is almost the same as solving the {\em linear} equation $g_{0}(x) \,=\, 0$. The latter equation is easy to solve:
        \begin{displaymath}
        f(x_{0}) + f'(x_{0})(x-x_{0}) \,=\, 0 \mbox{ implies }
        x \,=\, x_{0} - \frac{f(x_{0})}{f'(x_{0})}
        \end{displaymath}
    (Of course in this expression one tacitly assumes that $f'(x_{0}) \,\,{\neq}\,\, 0$.)
    Moreover, if the {\em equation} $g_{0}(x) \,=\, 0$ is almost the same as the equation $f(x) \,=\, 0$,
    then it seems plausible that the {\em solution} $x_{1} \,=\, x_{0}-f(x_{0})/f'(x_{0})$ of the approximate equation $g(x) \,=\, 0$
    is even closer to the desired root $r$ than $x_{0}$ is. And if that is the case,
    then applying the same idea to the tangent-line approximation of $f$ at $x_{1}$ should produce an even better approximation of the desired root $r$.
    The complete procedure thus produces the familiar infinite sequence $x_{0}$, $x_{1}$,\,{\ldots}\,$x_{k}$,\,{\ldots}\, given recursively by the rule
        \begin{displaymath}
        x_{k+1} \,=\, x_{k} - \frac{f(x_{k})}{f'(x_{k})} \mbox{ for $k \,=\, 1,2,\,{\ldots}\,$}.
        \end{displaymath}
    It is assumed here that $f'(x_{k}) \,\,{\neq}\,\, 0$ and that $x_{k}$ being in the domain of $f$ implies that $x_{k+1}$ is also.
    Note that in the special case $f(x) \,=\, x^{2}-C$, where $C$ is a positive constant,
    Newton's Method with $x_{0}\,>\,0$ is the same as Heron's Method for computing square roots.

        Another important linear approximation scheme is given by linear interpolation (see Example~\Ref{ExampA30.25}).
    For this method, one needs {\em two} points on the graph of the given function $f$.
    More precisely, suppose that $(a,f(a))$ and $(b,f(b))$ are points such that $a \,\,{\neq}\,\, b$.
    Then the linear function $g:{\RR} \,{\rightarrow}\, {\RR}$ which interpolates these points is given by the formula
        \begin{displaymath}
        g(x) \,=\, f(a) + \left(\frac{f(b)-f(a)}{b-a}\right)(x-a) \mbox{ for all $x$ in ${\RR}$}.
        \end{displaymath}
    Note that the graph $y \,=\, g(x)$ of the function $g$ is what one calls in elementary calculus
    the `secant line' to the graph $y \,=\, f(x)$ through the points $(a,f(a))$ and $(b,f(b))$.

        As with the tangent line approximation, one can use the `secant line approximation'
    $y \,=\, g(x)$ to approximate solutions of the equation $f(x) \,=\, 0$. More precisely,
    suppose that $(x_{0},f(x_{0}))$ and $(x_{1},f(x_{1}))$ are points of the graph of $f$ with $x_{0} \,\,{\neq}\,\, x_{1}$,
    and let $g_{0}$ be the corresponding linear interpolation function. Now consider the equation $g_{0}(x) \,=\, 0$; one gets the solution
        \begin{displaymath}
        x \,=\, x_{0} - f(x_{0})\left(\frac{x_{1}-x_{0}}{f(x_{1})-f(x_{0})}\right).
        \end{displaymath}
    As with Newton's Method, this process can be repeated to produce of an infinite sequence of approximations of the solution of $f(x) \,=\, 0$.
    More precisely, if $x_{0}$, $x_{1}$,\,{\ldots}\, $x_{k}$ have been defined, then set
        \begin{displaymath}
        x_{k+1} \,=\, x_{k-1} - f(x_{k-1})\left(\frac{x_{k}-x_{k-1}}{f(x_{k}) - f(x_{k-1})}\right) \mbox{ for each $k \,=\, 1,2,\,{\ldots}\,$}
        \end{displaymath}
    In numerical analysis this procedure is called the {\bf \IndA{Secant Method}}.

\VV

        In numerical analysis it is customary to estimate the errors in the tangent line and secant line approximations in terms of the second derivative of the original function $f$.
    The following estimates, which involve only the first derivative of $f$, are cruder, but are sufficiently good for our current purposes.
    To simplify the discussion, we assume that the function $f$ to be approximated has continuous first derivative.

\V

             \subsection{\small{\bf Theorem}}
            \label{ThmE55.40}

\V

        Suppose that $f:I \,{\rightarrow}\, {\RR}$ is a $C^{1}$ function on an open interval $I$ in ${\RR}$.
    Let $a$ and $b$ be numbers in $I$ such that $a\,<\,b$, and let $M$ be an upper bound for $|f'|$ on the closed bounded interval $[a,b]$.
    (The existence of the finite upper bound $M$ follows from the Extreme-Value Theorem applied to the continuous function $f'$ on $[a,b]$.)

\V

        (a) Suppose that $c{\in}[a,b]$, and let $g:{\RR} \,{\rightarrow}\, {\RR}$ be the tangent line approximation of $f$ at $c$;
    that is, $g(x) \,=\, f(c) + f'(c)(x-c)$ for all $x$.
    Then
        \begin{equation}
        \label{EqnE.250A}
        |f(x)-g(x)|\,\,{\leq}\,\,2M|x-c|\,\,{\leq}\,\,2M|b-a| \mbox{ for all $x$ in $[a,b]$}.
        \end{equation}

\V

        (b) Let $g:{\RR} \,{\rightarrow}\, {\RR}$ be the linear interpolation of $f$ on the interval $[a,b]$;
    that is,
        \begin{displaymath}
        g(x) \,=\, f(a) + \left(\frac{f(b)-f(a)}{b-a}\right)(x-a).
        \end{displaymath}
    Then
        \begin{equation}
        \label{EqnE.250B}
        |f(x)-g(x)|\,\,{\leq}\,\,M|b-a| \mbox{ for all $x$ in $[a,b]$}.
        \end{equation}

\V

        The key to the proof is the following result:

\V

             \subsection{\small{\bf Lemma}}
            \label{LemmaE55.45}

\V

        Suppose that $f$, $I$, $a$, $b$ and $M$ are as in the statement of the preceding theorem.
    Let $g:{\RR} \,{\rightarrow}\, {\RR}$ be a linear function, so that $g'(x) \,=\, m$ for some constant $m$ such that $|m|\,\,{\leq}\,\,M$.
    Assume further that there is a number $c$ in $[a,b]$ such that $f(c) \,=\, g(c)$;
    speaking geometrically, assume that the graph of $f$ and the graph of $g$ intersect somewhere in $[a,b]$.
    Then for all $x$ in $[a,b]$ one has
        \begin{displaymath}
        |f(x)-g(x)|\,\,{\leq}\,\,2M\max\{|b-c|, |c-a|\}.
        \end{displaymath}

        {\bf Proof of Lemma}\, Note that if $x{\in}[a,b]$ then, since $f(c) \,=\, g(c)$, there exists $u$ in $\mbox{Seg}\,[x,c]$ such that
        \begin{displaymath}
        |f(x)-g(x)| \,=\, |(f(x)-f(c)) - (f(c) - g(x))| \,=\, |(f(x)-f(c)) - (g(c) - g(x))| \,=\, 
        \end{displaymath}
        \begin{displaymath}
        |f'(u)(x-c) - m(x-c)| \,=\, |f'(u)-m|{\cdot}|x-c|\,\,{\leq}\,\,2M|x-c|.
        \end{displaymath}
    Since $c{\in}[a,b]$, one must have either $a\,\,{\leq}\,\,x\,\,{\leq}\,\,c$ or $c\,\,{\leq}\,\,x\,\,{\leq}\,\,b$.
    In the former case one gets $|x-c|\,\,{\leq}\,\,|c-a|$, while in the lattr one gets $|x-c|\,\,{\leq}\,\,|b-c|$.
    Thus in both cases one has $|x-c|\,\,{\leq}\,\,\max\{|b-c|, |c-a|\}$, and the lemma follows.


\V

        {\bf Proof of Theorem}

        (a) Since $m \,=\, g'(a) \,=\, f'(a)$ by construction of the tangdent line at $(a,f(a))$, it follows that $m$ is a value of $f'$ on $[a,b]$ and thus $|m|\,\,{\leq}\,\,M$.
    In addition, one has $g(a) \,=\, f(a)$.
    The desired result now follows from the lemma with $c \,=\, a$.

\V

        (b) Let $m$ be the (constant) value of the interpolating linear function $g$. Then by the definition of `interpolatoion', combined with the standard Mean-Value Theorem one has
        \begin{displaymath}
        m \,=\, \frac{g(b)-g(a)}{b-a} \,=\, \frac{f(b)-f(a)}{b-a} \,=\, f'({\xi}) \mbox{ for some ${\xi}$ in $(a,b)$}.
        \end{displaymath}
    In partcular, $m$ is a value of $f'$ and thus $|m|\,\,{\leq}\,\,M$. Let ${\mu} \,=\, (a+b)/2$ be the midpoint of the interval $[a,b]$,
    and note that $M$ is an upper bound for $|f'|$ on each of the subintervals $[a,{\mu}]$ and $[{\mu},b]$.
    Suppose that $x{\in}[a,b]$. Then $x$ is in (at least one) of the subintervals $[a,{\mu}]$ or $[{\mu},b]$.
    In the former case the lemma, with $c \,=\, a$, implies that $|f(x)-g(x)|\,\,{\leq}\,\,2M|{\mu}-a| \,=\, M|b-a|$.
    Likewise, in the latter case, the same lemma, but now with $c \,=\, b$, implies $|f(x)-g(x)|\,\,{\leq}\,\,M|b-a|$.
    The desired result now follows. \Q

\V

        {\bf Remark} A generalization of Part~(b) is given in the exercises.

\V
\V

             \subsection{\small{\bf Remark}}
            \label{RemrkE55.50}

\V

        The careful reader may be grumbling that the main hypothesis in the preceding theorem, namely that $f$ be of class $C^{1}$ on $I$,
    is stronger than necessary.
    Indeed, this hypothesis could be replaced by the following weaker one:


    \h `{\em The derivative $f'$ is defined on the interval $I$, and for each $a$ and $b$ in $I$ with $a\,<\,b$, $f'$ is bounded on the subinterval $[a,b]$.}'

\noindent In light of Example~\Ref{ExampE35.130}, this alternate hypothesis is certainly weaker than the $C^{1}$ hypothesis actually used,
    and the proof using the weaker hypothesis is essentially the same as the one given above.
    So why not use the weaker hypothesis?

        \underline{Answer} The issue of balancing generality with ease-of-use arises frequently in mathematics (as in other subjects).
    In the current situation, expressing the result in terms of $f$ being $C^{1}$ makes the statement marginally easier to remember, while being sufficiently general for our purposes.


%--------------------- F
\StartSkip{
                \section{{\bf Antiderivatives}} 
                \label{SectE45}\IndB{ZZ Sections}{\Ref{SectE45} Antiderivatives}

        In elementary calculus many of the most significant applications involve finding {\em anti}derivatives of functions.
    These applications include computing area, volume, arclength, work, center of gravity and so on.

\V

             \subsection{\small{\bf Examples}}
            \label{ExampE45.20}

\V

\hspace*{\parindent}(1) Consider the equation $F'(x) \,=\, 2x^{2} + 4x -7$ for $x$ in ${\RR}$.
    This is an equation for the (`unknown') function $F$ which relates the derivative $F'$ to the known function $f(x) \,=\, 2x^{2}+4x-7$ on the interval $I \,=\, {\RR}$.

\V

        (2) In Euclidean geometry one defines the number ${\pi}$ to be the ratio of the circumference $C$ of the circle to its diameter $d$: ${\pi} \,=\, C/d$.
    This is usually expressed in terns of the radius $r \,=\, d/2$ in the form $C \,=\, 2{\pi}r$.
    One can then show geometrically that, as a function of the radius, the area $A$ satisfies the relation ${\displaystyle \frac{dA}{dr} \,=\, C \,=\, 2{\pi}r}$.
    Thus, the problem of determining the area of a circle as a function of its radius reduces to the calculus problem of solving the equation $A'(r) \,=\, 2{\pi}r$ for the function~$A$.

\VV

    There is a standard terminology for this situation.

\V

             \subsection{\small{\bf Definition} (Antiderivatives)}
            \label{DefE45.30}\IndB{antiderivatives}{definition}

        Suppose that $f:I \,{\rightarrow}\, {\RR}$ is function defined on an open interval $I$ in ${\RR}$.

\V

        (1) A function $F:I \,{\rightarrow}\, {\RR}$ is said to be {\bf an antiderivative of $f$ on $I$} provided $F'(x) \,=\, f(x)$ for all $x$ in $I$.

\V

        (2) More generally, let $k$ be a positive integer. A function $G:I \,{\rightarrow}\, {\RR}$ is said to be a {\bf \IndBB{antiderivatives}{$k$-th order antiderivative} of $f$ on $I$} provided $G^{(k)}(x) \,=\, f(x)$ for all $x$ in $I$.

\V

        (3) The process of calculating an antiderivative of a given function is called {\bf antidifferentiation}.


\V
\V

             \subsection{\small{\bf Examples}}
            \label{ExampE45.40}

\V

\hspace*{\parindent}(1) Example~\Ref{ExampE45.20}~(1) above can be phrased as saying that the function $F$ is an antiderivative of the function $f(x) \,=\, 2x^{2} + 4x -7$.
    It is easy to find explicit examples of such antiderivatives. For instance,
        \begin{displaymath}
        F(x) \,=\, \frac{2x^{3}}{3} + 2x^{2} - 7x
        \end{displaymath}
    clearly satisfies the equation $F'(x) \,=\, f(x)$ for all $x$ in ${\RR}$; to see this, simply differentiate the function $F$ using the standard rules, and compare the result with the formula for $f$.
    But the function
        \begin{displaymath}
        F(x) \,=\, \frac{2x^{3}}{3} + 2x^{2} - 7x + 5,
        \end{displaymath}
    also satisfies the same equation, since the derivative of the constant function $5$ is $0$.
    More generally. one can replace the term $5$ by any constant function $C$, and still get an antiderivative of $f$.

\V

        (2) Example~\Ref{ExampE45.20}~(2) can likewise be phrased as saying that the area function $A$ is an antiderivative of the circumference function, at least when both quantities are expressed as functions of the radius.
    Clearly $A \,=\, {\pi}r^{2}$ satisfies the condition $dA/dr \,=\, 2{\pi}r$; but so does $A \,=\, 2{\pi}r + B$, where $B$ can be any constant.


\V

%---------------- G
\StartSkip{
       (3) Let ${\cal H}:{\RR} \,{\rightarrow}\, {\RR}$ be the function given by
        \begin{displaymath}
        {\cal H}(x) \,=\, \left\{
        \begin{array}{ll}
        0 & \mbox{if $x\,<\,0$}   \\
        1/2 & \mbox{if $x \,=\, 0$} \\
        1 & \mbox{if $x\,>\,0$}
        \end{array}
            \right.
        \end{displaymath}
    This function is called the {\bf \IndAA{Heaviside function}}, after the English engineer Oliver Heaviside,
    who introduced it in the late nineteenth century. (It is also called the {\bf \IndBB{Heaviside function}{unit step function}}.)
   The function $f$ clearly does not possess the `Intermediate-Value Property' on ${\RR}$.
    Thus it follows from from the Intermediate-Value Theorem for Derivatives that $f$ cannot have an antiderivative on ${\RR}$.
    (For those who like using fancy terminology: the Heaviside function is the concatenation $0\mbox{\&}_{(0,1/2)}1$, where here~$0$ denotes the constant function of value~$0$, and~$1$ denotes the constant function of value~$1$.)

        \underline{Remarks} (1) Curiously enough, Heaviside introduced this function as an antiderivative on ${\RR}$ of something called the `Delta function'.
    It turns out that this `Delta function' is a kind of generalized function that in mathematics is now called a `distribution'.
    The theory of such `distributions' is outside the scope of {\ThisText}.

        (2) Assigning the value $1/2$ to ${\cal H}(0)$ is common in the literature, but not universal.
    Some authors set ${\cal H}(0) \,=\, 1$; some don't assign any value to ${\cal H}(0)$.
}%\EndSkip
%------------------- G

\V

        (3) Let $f(x) \,=\, |x|$ for all $x$ in ${\RR}$. Then $f(x) \,=\, x$ if $x\,\,{\geq}\,\,0$, and $f(x) \,=\, -x$ if $x\,<\,0$.
    On the open interval $(0,+{\infty})$ the function $f$ has many antiderivatives;
    namely, any function on $(0,+{\infty})$ of the form $G(x) \,=\, {\displaystyle \frac{x^{2}}{2}+C_{1}}$, where $C_{1}$ is constant.
    Likewise $f$ has infinitely many antiderivatives on the interval $(-{\infty},0)$,
    namely functions of the form $H(x) \,=\, {\displaystyle -\frac{x^{2}}{2}+C_{2}}$.
    To get an antiderivative defined on all of ${\RR}$, choose the constants $C_{1}$ and $C_{2}$ so that $\lim_{x{\nearrow}0} H(x) \,=\, \lim_{x{\searrow}0} G(x)$.
    This simply requires $C_{1} \,=\, C_{2}$, so let us make the simplest choice, namely $C_{1} \,=\, C_{2} \,=\, 0$.
    Then define $F:{\RR} \,{\rightarrow}\, {\RR}$ by the rule
        \begin{displaymath}
        F(x) \,=\, \left\{
        \begin{array}{ccrl}
        H(x) & \,=\, & -x^{2}/2 & \mbox{if $x\,<\,0$} \\
          0  & \,=\, &          & \mbox{if $x\,=\,0$} \\
        G(x) & \,=\, &  x^{2}/2 & \mbox{if $x\,>\,0$}
        \end{array}
                        \right.
        \end{displaymath}
    This is the same function that appears in Example~\Ref{ExampE20.85} above, where it is shown that $F'$ is the absolute-value function.

\VV

        As is indicated in Example~(1) above, the use of the indefinite article `an' in the definition of `an antiderivative' is needed:
    If $F$ is an antiderivative of $f$ on an interval $I$, then for every constant function $C$ the function $F+C$ is also an antiderivative of $F$ on $I$.
    The next result shows that this is the only ambiguity possible for first-order antiderivatives.

\V

             \subsection{\small{\bf Theorem}}
            \label{ThmE45.50}

\V

        Suppose that $F_{1}$ and $F_{2}$ are antiderivatives of $f$ on an open interval $I$.
    Then there exists a constant function $C$ such that $F_{2}(x) \,=\, F_{1}(x) + C$ for all $x$ in $I$.

    Equivalently: The function $F_{2}-F_{1}$ is constant on $I$.

\V

        \underline{Proof} Note that, by Theorem~\Ref{ThmE30.20}, the function $F_{2}-F_{1}$ is differentiable on $I$, and
        \begin{displaymath}
        (F_{1}-F_{2})'(x) \,=\, F_{1}'(x)-F_{2}'(x) \,=\, f(x)-f(x) \,=\, 0
        \end{displaymath}
    for all $x$ in $I$.
    It then follows from Corollary~\Ref{CorE40.50} that $F_{2}-F_{1}$ is constant on $I$, as required. \Q

\V

        \underline{Notes} (1) Because of the preceding result, if $F$ is a particular antiderivative of a given function $f$ on an open interval $I$,
    then it is common to refer to the expression $F+C$ in which $C$ is an `arbitrary constant', 
    as {\bf the \IndBB{antiderivatives}{general} antiderivative of $f$ on $I$}.

\V

        (2) The corresponding ambiguity for higher-order antiderivatives is more complicated, and is considered later; see Theorem~\Ref{ThmE45.110}.

\V
\V

             \subsection{\small{\bf Corollary}}
            \label{CorE45.60}

\V

        \hspace*{\parindent}(a) Suppose that $f:I \,{\rightarrow}\, {\RR}$ is a function which has an antiderivative on an open interval $I$.
    Let $c$ be any point in $I$, and let $A$ be any real number.
    Then there exists a unique antiderivative $F$ of $f$ on $I$ such that $F(c) \,=\, A$.

        More precisely, if $G:I \,{\rightarrow}\, {\RR}$ is any antiderivative of $f$ on $I$, then the unique $F$ with this property is given by
        \begin{displaymath}
        F(x) \,=\, A + G(x) - G(c) \h ({\ast})
        \end{displaymath}
    In particular, if $A \,=\, 0$ then the formula reduces to
        \begin{displaymath}
        F(x) \,=\, G(x)-G(c).
        \end{displaymath}

\V

        (b) Suppose that $f:I \,{\rightarrow}\, {\RR}$ is a function defined on an open interval $I$ and that $a$ and $b$ are elements of $I$ with $a\,<\,b$.
    Suppose that there are numbers $x_{1}$, $x_{2}$,\,{\ldots}\,$x_{k}$ with $a\,<\,x_{1}\,<\,x_{2}\,<\,\,{\ldots}\,\,<\,x_{k}\,<\,b$ such that $f$ has an antiderivative on each of the subintervals $[a,x_{1}]$, $[x_{1},x_{2}]$,\,{\ldots}\,$[x_{k-1},x_{k}]$, $[x_{k},b]$.
    Then for each $A$ in ${\RR}$ there exists a unique antiderivative $F:[a,b] \,{\rightarrow}\, {\RR}$ of $f$ on $[a,b]$ such that $F(a) \,=\, A$.
\V

        \underline{Proof}

        (a) \underline{Existence} If $G$ is any antiderivative of $F$ on $I$, then clearly the function $F$ given by Equation~$({\ast})$ is also an antiderivative of $f$, since it differs from $G$ by the constant $A-G(c)$.
    Furthermore, one computes that $F(c) \,=\, A+G(c)-G(c) \,=\, A$, as required.

        \underline{Uniqueness} Suppose that $F_{1}$ and $F_{2}$ are both antiderivatives of $f$ on $I$ such that $F_{1}(c) \,=\, A$ and $F_{2}(c) \,=\, A$.
    By the preceding theorem there exists a constant function $C$ such that $F_{2}(x) - F_{1}(x) \,=\, C$ for all $x$ in $I$.
    In particular, this condition must hold when $x \,=\, c$; that is,
        \begin{displaymath}
        0 \,=\, A-A \,=\, F_{1}(c)-F_{1}(c) \,=\, C,
        \end{displaymath}
    so $C \,=\, 0$ and $F_{2} \,=\, F_{1}$ on $I$, as claimed.

\V

        (b) For notational convenience, set $x_{0} \,=\, a$ and $x_{k+1} \,=\, b$.
    By Part~(a), $f$ has a unique antiderivative $F_{1}:[x_{0},x_{1}] \,{\rightarrow}\, {\RR}$ on $[x_{0},x_{1}]$ such that $F_{1}(x_{0}) \,=\, A$.
     Then by Part~(a) again $f$ has a unique antiderivative $F_{2}:[x_{1},x_{2}] \,{\rightarrow}\, {\RR}$ on $[x_{1},x_{2}]$ such that $F_{2}(x_{1}) \,=\, F_{1}(x_{1})$.
    Continuing on this way, one obtains functions $F_{1}$, $F_{2}$,\,{\ldots}\,$F_{k+1}$ such that

        \h (i)\, for each $j \,=\, 1,2,\,{\ldots}\,k+1$, $F_{j}$ is an antiderivative of $f$ on the subinterval $[x_{j-1},x_{j}]$.

        \h (ii) $F_{1}(x_{0}) \,=\, A$; and for each $j \,=\, 1,2,\,{\ldots}\,k$, $F_{j+1}(x_{j}) \,=\, F_{j}(x_{j})$.

\noindent Now define $F:[a,b] \,{\rightarrow}\, {\RR}$ by the rule
        \begin{displaymath}
        F(x) \,=\, F_{j}(x) \mbox{ if $x{\in}[x_{j-1},x_{j}]$}.
        \end{displaymath}
    It is easy to show by Property~(ii) above that $F(x)$ is well-defined at all $x$ in $[a,b]$, even at $x \,=\, x_{j}$.
    It is also easy to show by using one-sided limits at the points $x_{j}$ that $F'(x) \,=\, f(x)$ for all $x$ in $[a,b]$; the details are left to the reader.
    The desired result now follows. \Q

\VV

             \subsection{\small{\bf Examples}}
            \label{ExampE45.65}
\V

\hspace*{\parindent}(1) Suppose that $f:{\RR} \,{\rightarrow}\, {\RR}$ is a monomial, in the sense of high-school algebra;
    that is, there is a nonzero constant $a$ and a nonnegative integer $k$ such that $f(x) \,=\, a\,x^{k}$ for all $x$ in~${\RR}$.
    (Note that if $k$ is even, then $f(-x) \,=\, f(x)$ for all $x$, while if $k$ is odd then $f(-x) \,=\, -f(x)$ for all~$x$.)
    The function $f$ has a unique antiderivative $F$ on ${\RR}$ which is also a monomial, namely the antiderivatives $F$ such that $F(0) \,=\, 0$.
    Indeed, this antiderivative is given by $F(x) \,=\, b\,x^{k+1}$, where $b \,=\, a/(k+1)$.
    Note that if $f$ is a monomial of even degree, then $F$ is a monomial of odd  degree. Similarly, if $f$ is of odd degree, then $F$ is of even degree.
   In the former case one has $f(-x) \,=\, f(x)$ and $F(-x) \,=\, -F(x)$, while in the latter case one has $f(-x) \,=\, -f(x)$ and $F(-x) \,=\, F(x)$.

\V

        (2) More generally, suppose that $I$ is an open interval of the form $(-b,b)$, where $b\,>\,0$; the case $b \,=\, +{\infty}$ is allowed.
    Recall, from high-school algebra, that a function $g:I \,{\rightarrow}\, {\RR}$ is said to be an
    {\bf even function on $\Bfm{I}$}\IndBD{functions}{even, odd functions} provided $g(-x) \,=\, g(-x)$ for all $x$ in~$I$.
    Likewise, a function $h:I \,{\rightarrow}\, {\RR}$ is said to be an {\bf odd function on $\Bfm{I}$} provided $h(-x) \,=\, -h(x)$ for all $x$ in~$I$.

        Suppose that $f:I \,{\rightarrow}\, {\RR}$ is a real-valued function with domain~$I$, and that $f$ has an antiderivative on~$I$.
    Let $F:I \,{\rightarrow}\, {\RR}$ be the unique antiderivative of $f$ on~$I$ such that $F(0) \,=\, 0$.

\VA

        \h (i)\, If $f$ is an even function on the interval~$I$, then $F$ is an odd function on~$I$.

        \h (ii) If $f$ is an odd function on~$I$, then $F$ is an even function on~$I$.

\VA

\noindent Indeed, consider the functiom $G:I \,{\rightarrow}\, {\RR}$ given by the rule $G(x) \,=\, F(-x)$ for all $x$ in~$I$.
    By the Chain Rule one sees that $G'(x) \,=\, -F'(-x) \,=\, -f(-x)$, so that $(-G)'(x) \,=\, f(-x)$.

        In Case~(i) one has $f(-x) \,=\, f(x)$, so that $(-G)'(x) \,=\, f(x)$. Since clearly $-G(0) \,=\, 0$,
    it follows that in this case one has $-G(x) \,=\, F(x)$; that is, $-F(-x) \,=\, F(x)$, which implies that $F$ is an odd function, as claimed.
    Likewise, in Case~(ii) one gets $(-G)'(x) \,=\, -f(x)$, which implies that $F(-x) \,=\, F(x)$, so that $F$ is an even function, as claimed.

\VV

             \subsection{\small{\bf Remarks}}
            \label{RemrkE45.70}
\V

\hspace*{\parindent}(1) The use of the word `antidifferentiation', to indicate the process opposite to the process of `differentiation', seems reasonable;
    likewise for naming the result of that process an `antiderivative'. Indeed, the `antidifferentiation/antiderivative' terminology
    seems to be universal in the modern calculus texts.

    However, for most of the centuries since calculus was first developed the words used instead of `antidifferentiation' and `antiderivative' were 
    `integration' and `integral'; some authors used the word `primitive' instead `integral', and sometimes the word `indefinite' is used before `integral'.

        Why is it important for you, the modern reader, to know this? Because the `integration/integral' terminology for these concepts is still widely used;
    thus if you encounter it, you need to know what it means. Furthermore, although the use of the `antiderivative' {\em terminology} ,
    the {\em notation} used in connection with this concept remains stuck in the eighteenth century.
    More precisely, the way one writes the statement `$F+C$ is the general antiderivative of $f$' in mathematical symbols is this:
        \begin{displaymath}
        F(x)+C \,=\, \int\, f(x)\,dx
        \end{displaymath}
    In the expression ${\displaystyle \int\, f(x)\,dx}$ the symbol ${\displaystyle \int}$ is called the {\bf integral sign},
    the function $f(x)$ is called the {\bf integrand}, and the `arbitrary constant' $C$ is called the {\bf constant of integration}.
    Even in those elementary-calculus textbooks which consistently use the `antiderivative' terminology,
     the section in which one learns how to compute antiderivatives is normally titled something like `Techniques of Integration',
    not `Techniques of Antidifferentiation'.

\V

        (2) The question of whether a given function $f:I \,{\rightarrow}\, {\RR}$ actually has an antiderivative on an open interval $I$ is not trivial to answer.
    It is answered in the affirmative in this section for continuous functions.

\VV

{\footnotesize \underline{\Note}\IndB{\notes}{on the fundamental goal of calculus} (on the fundamental goal of calculus)
    REDO THIS DISCUSSION. GET REFERENCE TO THE NEWTON ANAGRAM INTO THE MAIN BODY OF THE TEXT.

    There is good reason to assert that the fundamental goal of elementary single-variable calculus is this:

\V

    {\em Given a function $f$, to find its derivative;
    and given a function $g$, to find its (general) antiderivative.}

\V

\noindent In fact, this assertion simply reflects the reality of the typical elementary calculus course:
    the actual {\em calculus} that the student is expected to use in such a course consists almost entirely of differentiation and antidifferentiation.
    The remainder of such a course consists of applications of differentiation and antidifferentiation to a variety of situations:
    max/min, related rates, geometry etc. The definition of the (definite) integral, as a limit of so-called `Riemann sums',
    appears only as a means of motivating useful examples of antiderivatives to compute.
    In most cases, the same results can be motivated, often more easily, 

        The preceding assertion can also be `proved' by a `resort to authority'.
    Indeed, recall the anagram which appears in Chapter Quote~(1) at the start of this chapter:

\begin{quotation}
{\footnotesize
        {\em 6accd{\ae}13eff7i3l9n4o4qrr4s8t12vx}
}%EndFootNoteSize
\end{quotation}

\noindent Isaac Newton, one of the cofounders of calculus in the seventeenth century, used the letters in that anagram to disguise the following statement:

\begin{quotation}
{\footnotesize
      {\em Data {\ae}quatione quotcunque fluentes quantitates involvente, fluxiones invenire: et vice vers\^{a}}.
}%EndFootNoteSize
\end{quotation}

    That is,
\begin{quotation}
{\footnotesize 
    \em Given an equation involving any number of fluent quantities, to find the fluxions: and vice versa.
}%EndFootNoteSize
\end{quotation}

        In Newton's terminology, a `fluent' is a changing quantity, and the corresponding `fluxion' is the rate of change (with respect to time) of that quantity.
    Thus, according to Newton the key is to take derivatives and find antiderivatives, as asserted.
    (The source for this is Volume~(II) of the classic work {\em The Correspondence of Isaac Newton} (in seven volumes), by H.~B. Turnbull.)
}%EndFootNoteSize

\begin{quotation}
{\footnotesize \underline{Historical Remark}: 

}%EndFootnotesize
\end{quotation}

\VV

        The following question is natural:

\VA

        \h `Under what circumstances does a given function have an antiderivative on a given interval?'.

\VA

\noindent It is easy to provide simple examples of functions $f:I \,{\rightarrow}\, {\RR}$
    which fail to have an antiderivative on an open interval~$I$. For example, if $f:{\RR} \,{\rightarrow}\, {\RR}$ is the Dirichlet function,
    then there is no function $F:{\RR} \,{\rightarrow}\, {\RR}$ such that $F'(x) \,=\, f(x)$ for all~$x$.
    This is obvious because the Dirichlet function clearly fails to possess the Intermediate-Value Property on any subinterval of~${\RR}$.

        In contrast, one encounters many examples of functions, even quite complicated functions,
    for which one can find explicit formulas for the antiderivatives.
    Similarly, the Absolute-Value function (see Example~\Ref{ExampE45.40}~(4)) is quite simple, and it also does have an antiderivative on its domain;
    but the description of this antiderivative requires using different formulas on different parts of the domain.

        In contrast, it often happens in calculus that a student is asked to find an antiderivative of a given function,
    but is unable to do so, usually because the required algebra is somewhat complicated.
    This does not `prove' that no such antiderivative exists, only that the student may need more study.
    Even worse, there exist fairly simple examples


    In other words, a complete answer to this question is likely to be complicated.

\V

        The next result generalizes the method used in the `Absolute Value' example above.

\V

             \subsection{\small{\bf Theorem}}
            \label{ThmE45.125A}

\V

        Let $[a,b]$ be a closed bounded interval in~${\RR}$, and let ${\cal D} \,=\, \{(x_{0},y_{0}), (x_{1},y_{1}),\,{\ldots}\,(x_{k},y_{k})\}$
    be a set of `data points' in ${\RR}^{2}$ such that $a \,=\, x_{0}\,<\,x_{1}\,<\,\,{\ldots}\,\,<\,x_{k} \,=\, b$,
    and let $g:{\RR} \,{\rightarrow}\, {\RR}$ be the corresponding continuous piecewise-linear interpolating function through these points;
    see Example~\Ref{ExampA30.29}~(3) and Example~\Ref{ExampD20.53}~(5). Then for each real number $c$ in the interval $[a,b]$
    the function $g$ has a unique antiderivative $G$ on ${\RR}$ such that $G(c) \,=\, 0$.

\V

        \underline{Proof} It suffices to show that $g$ has an antiderivative $G$ which takes on the value $0$ at $c \,=\, a \,=\, x_{0}$;
    the case for general choice of $c$ follows easily.

        Note that for each index $j \,=\, 1,2,\,{\ldots}\,k$, if $x$ satisfies the condition $x_{j-1}\,\,{\leq}\,\,x\,\,{\leq}\,\,x_{j}$, then by definition
        \begin{displaymath}
        g(x) \,=\, y_{j-1} + a_{j}\,(x-x_{j-1}) \mbox{ where } a_{j} \,=\, \frac{y_{j} - y_{j-1}}{x_{j} - x_{j-1}}.
        \end{displaymath}
    It is clear that, on the inverval $[x_{j-1},x_{j}]$,
    every function of the form $G_{j}(x) \,=\, a_{j}\,(x-x_{j-1})^{2}/2 + y_{j}\,(x-x_{j-1}) + c_{j}$
    is an antiderivative of $g$ on the interval~$[x_{j-1},x_{j}]$ such that $G_{j}(x_{j-1}) \,=\, c_{j}$.
    The remainder of the construction of the desired function~$G$ is to choose the constants
    $c_{1}$, $c_{2}$,\,{\ldots}\,$c_{k}$ so that the antiderivatives $G_{1}$, $G_{2}$,\,{\ldots}\, `fit' together properly at the points $x_{j}$.

        \underline{The case $j \,=\, 1$} Since, as is observed above, one has $G_{1}(x_{0}) \,=\, c_{1}$, one must choose $c_{1} \,=\, 0$.

        \underline{The Case $j \,=\, 2$} Choose $c_{2}$ so that $G_{1}(x_{1}) \,=\, G_{2}(x_{1})$.
    Since $G_{2}(x_{1}) \,=\, c_{2}$, this requires that one should choose $c_{2} \,=\, G_{1}(x_{1})$.

        The case of general $j$ is carried out similarly, so that $c_{j} \,=\, G_{j-1}(x_{j})$ for each~$j$.

        Finally, define $G:[a,b] \,{\rightarrow}\, {\RR}$ by the rule that if $x_{j-1}\,\,{\leq}\,\,x\,\,{\leq}\,\,x_{j}$, then $G(x) \,=\, G_{j}(x)$.
    One can apply Theorem~\Ref{ThmE20.25A} to conclude that $G'(x) \,=\, g(x)$ for all $x$ in~$[a,b]$. \Q

\V

        \subsection{\small{\bf Remark}}
                         \label{RemrkE45.125AB}

\V

        Modern courses in real analysis pride themselves for being rigorous. In particular,
    this means they exclude from their arguments any nonanalytic features, such as geometric or physical reasoning,
    that one would find in typical courses in elementary calculus. Nevertheless, it is sometimes good pedagogy to mention such nonanalytic features in passing.
    For example, if one draws the graph $y \,=\, g(x)$ of a continuous piecewise linear $g$ over $[a,b]$ as above,
    one sees, from the formula for the area of a trapezoid from high-school geometry,
    that for each $x$ in $[a,b]$ the quantity $G(x) - G(a)$ is the signed area between the graph of $g$ and the horizontal axis from $a$ to~$x$.

% EXERCISE

\VV

        The next theorem is one of the most important in calculus. 

\VV

             \subsection{\small{\bf Theorem} (Cauchy's Antiderivative Theorem)}
                         \label{ThmE45.125B}\IndBD{antiderivatives}{Cauchy's antiderivative theorem}

\V

        Let $f:I \,{\rightarrow}\, {\RR}$ be a function which is continuous on an open interval $I$ in ${\RR}$.
    Then the function $f$ has an antiderivative on $I$. More precisely, if $c$ is any point of $I$,
    then $f$ has a unique antiderivative on $I$ whose value at $c$ is~$0$.

\V

        {\bf Proof}\,Let $a$ and $b$ be points of $I$ such that $a\,<\,c\,<\,b$.
    Since $f$ is continuous on $I$, for each positive integer $k$ there is a continuous function $g_{k}:{\RR} \,{\rightarrow}\, {\RR}$ such that

\VA

        \h Condition (i)\, $|g_{k}(x) - f(x)|\,<\,1/k$ for all $x$ in the interval $[a,b]$.

        \h Condition (ii) $g_{k}$ has an antiderivative on ${\RR}$;



\VA

\hspace*{\parindent}Indeed, Theorem~\Ref{ThmD25.60} implies that there is a continuous piecewise-linear function on ${\RR}$ which satisfies Condition~(i),
    while Theorem~\Ref{ThmE45.125A} implies that every such function satisfies Condition~(ii).
    Note also that Condition~(i) implies that for each $x$ in $[a,b]$ one has ${\displaystyle \lim_{k \,{\rightarrow}\, {\infty}} g_{k}(x) \,=\, f(x)}$.

        Let $G_{k}:[a,b] \,{\rightarrow}\, {\RR}$ denote the unique antiderivative of $g_{k}$ on $[a,b]$ such that $G_{k}(c) \,=\, 0$.

\V

        \underline{Claim 1} For each $x$ in $[a,b]$ the sequence ${\Gamma}_{x} \,=\, \{G_{1}(x), G_{2}(x),\,{\ldots}\,\}$ is a Cauchy sequence.

        \underline{Proof of Claim 1} Let $k$ and $m$ be positive integers. Note that if $x{\in}[a,b]$ then, by the `segment' form of the Mean-Value Theorem applied to the function $G_{k+m} - G_{k}$, there exists a number $\hat{x}$ in ${\mbox{Seg}\,[x,c]}$ such that the following holds:
        \begin{displaymath}
        G_{k+m}(x) - G_{k}(x) \,=\, (G_{k+m} - G_{k})(x) - (G_{k+m} - G_{k})(c)
     \,=\, 
        (G_{k+m} - G_{k})'(\hat{x})(x-c) \,=\,
        \end{displaymath}
        \begin{displaymath}
         (G'_{k+m}(\hat{x}) - G_{k}'(\hat{x}))(x-c)
     \,=\, 
       (g_{k+m}(\hat{x}) - g_{k}(\hat{x}))(x-c).
        \end{displaymath}
    It follows, after doing a judicious `add-and-substract' trick and using the Triangle Inequality, that
        \begin{displaymath}
        |G_{k+m}(x) - G_{k}(x)|\,\,{\leq}\,\,\left(|g_{k+m}(x) - f(x)| + |f(x) - g_{k}(x)|\right)|x-c|\,\,{\leq}\,\,\left(\frac{1}{k+m} + \frac{1}{k}\right)|b-a|\,\,{\leq}\,\,\frac{|b-a|}{2k}.
        \end{displaymath}
    In particular, if ${\varepsilon}\,>\,0$ is given, then one has
        \begin{equation}
        \label{IneqE.83A}
        |G_{k+m}(x) - G_{k}(x)|\,\,{\leq}\,\,\frac{|b-a|}{2k}\,<\,{\varepsilon}
        \end{equation}
    for all $k$ in ${\NN}$ such that $k\,>\,|b-a|/(2{\varepsilon})$ and all $m$ in ${\NN}$. In particular, the sequence ${\Gamma}(x)$ is Cauchy, as claimed.

        The preceding result suggests a natural candidate for the desired antiderivative of the given function~$f$; namely,
    define $F:[a,b] \,{\rightarrow}\, {\RR}$ by the rule $F(x) \,=\, \lim_{k \,{\rightarrow}\, {\infty}} G_{k}(x)$ for all $x$ in the interval $[a,b]$.
    (The fact that for each $x$ in $[a,b]$ this limit exists and is finite follows from Claim~1.)

\V

    \underline{Claim 2} The function $F$ just described is the desired antiderivative of $f$.
    More precisely, for each $x_{0}$ in the open interval $(a,b)$ and for each ${\varepsilon}\,>\,0$,
    there exists ${\delta}\,>\,0$ such that if $h$ satisfies $0\,<\,|h|\,<\,{\delta}$ then
        \begin{displaymath}
        \left|\frac{F(x_{0} + h) - F(x_{0})}{h} - f(x_{0})\right|\,\,{\leq}\,\,{\varepsilon} \h ({\ast})
        \end{displaymath}

\V

        \underline{Proof of Claim 2} Let $x_{0}$ be any point of $(a,b)$, and let $h$ be any nonzero real number small enough that $x_{0}+h$ is also in $(a,b)$; more precisely, $|h|\,<\,{\min}\,\{|x_{0}-a|,|b-x_{0}|\}$.
    Then for each $k$ in ${\NN}$ one has, for some $\hat{x}$ in $\mbox{Seg}\,[x_{0}, x_{0}+h]$,
        \begin{displaymath}
        G_{k}(x_{0}+h) - G_{k}(x_{0}) - g_{k}(x_{0})h \,=\, G_{k}'(\hat{x})h - g_{k}(x_{0})h \,=\, (g_{k}(\hat{x}) - g_{k}(x_{0}))h.
        \end{displaymath}
    It then follows from the Extended Triangle Inequality that
        \begin{displaymath}
        \left|G_{k}(x_{0}+h) - G_{k}(x_{0}) - g_{k}(x_{0})h\right| 
    \,\,{\leq}\,\,\left(\left|g_{k}(\hat{x}) - f(\hat{x})\right| +|f(\hat{x}) - f(x_{0})| + \left|f(x_{0}) - g_{k}(x_{0})\right|\right){\cdot}|h| \h ({\ast}{\ast})
        \end{displaymath}
    Now let ${\varepsilon}\,>\,0$ be given. Using the uniform continuity of $f$ on the interval $[a,b]$, choose ${\delta}\,>\,0$ small enough that if $x$ and $y$ are any points of $[a,b]$ such that 
    $|y-x|\,<\,{\delta}$, then one has $|f(y)-f(x)|\,<\,{\varepsilon}/3$.
    It then follows from Condition~(ii) above that if $0\,<\,|h|\,<\,{\delta}$ then~$({\ast}{\ast})$ implies
        \begin{displaymath}
        \left|G_{k}(x_{0}+h) - G_{k}(x_{0}) - g_{k}(x_{0})h\right| \,<\,\left(\frac{{\varepsilon}}{3} + \frac{{\varepsilon}}{3} + \frac{{\varepsilon}}{3}\right)|h| \,=\, {\varepsilon}|h|
        \end{displaymath}
    Divide by $|h|$ and do some simple algebraic simplification to get
        \begin{displaymath}
        \left|\frac{G_{k}(x_{0}+h) - G_{k}(x_{0})}{h} - g_{k}(x_{0})\right|\,<\,{\varepsilon}
        \end{displaymath}
    This last inequality is true for {\em all} sufficiently large $k$. In particular,
    by letting $k$ approach ${\infty}$ one gets the desired inequality~$({\ast})$ for all $h$ such that $0\,<\,|h|\,<\,{\delta}$.

    Since this is true for all ${\varepsilon}\,>\,0$, it follows that $F'(x_{0})$ exists and equals $f(x_{0})$.
    Since $x_{0}$ can be any point of $(a,b)$, it follows that $F$ is an antiderivative of $f$ on $(a,b)$.
    The fact that $F(c) \,=\, 0$ follows from the fact that, by definition, $G_{k}(c) \,=\, 0$ for each~$k$.

\V

        \underline{Summary} For each $a$ and $b$ in $I$ such that $a\,<\,c\,<\,b$,
    there exists a unique antiderivative $F$ of $f$ on $(a,b)$ such that $F(c) \,=\, 0$.

        The desired result, namely that $f$ has a unique antiderivative, whose value at $c$ is $0$, on the {\em full} open interval $I$,
    now follows by letting $a$ approach ${\inf}\,I$ and $b$ approach ${\sup}\,I$; the details are left to the reader. \Q

\VV

%%%
\begin{quotation}
{\footnotesize \underline{\Note}\IndB{\notes}{on the the proof of the Cauchy Antiderivative Theorem} (on the the proof of the Cauchy Antiderivative Theorem)
    The approach to Theorem~\Ref{ThmE45.125B} given above follows that of the French mathematician H.~Lebesgue; see [LEBESGUE~1905]. 
    It is very different from Cauchy's original proof which one finds in texts on elementary calculus. That proof is given in Chapter~\Ref{ChaptH} below.
}%EndFootNoteSize
\end{quotation}
%##

\VV

             \subsection{\small{\bf Remark}}
            \label{RemrkE45.126A}

\V

        The proof of Cauchy's Antiderivative Theorem used above follows that of H.~Lebesgue (Bull. Sci. Math. 29~ (1905), 272-275).
    It differs from the standard one found in calculus texts, which is essentially Cauchy's original proof,
    in that it attacks the issue of the existence of antiderivatives directly in terms of antiderivatives, not in terms of definite integrals.
    Cauchy's standard proof is given in the next chapter.

    \VV


        The proof given above does more than demonstrate the abstract existence of an antiderivative on~$I$ of the given function~$f$.
    It also provides a simple method for approximating such an antiderivative to any desired degree of accuracy, at least if $f$ is reasonably `nice'.
    The next result provides an illustration of this feature.


\V

             \subsection{\small{\bf Corollary}}
            \label{CorE45.126B}

\V

        Suppose that $f:I \,{\rightarrow}\, {\RR}$ is a $C^{1}$ function on an open interval $I$ in ${\RR}$.
    Let $c$ be a point in $I$, and let $F:I \,{\rightarrow}\, {\RR}$ be the unique antiderivative of $f$ on $I$ such that $F(c) \,=\, 0$.
    Let $a$ and $b$ be elements of $I$ such that $a\,<\,c\,<\,b$, and let $M$ be an upper bound of $|f'|$ on the closed bounded interval $[a,b]$.
    For each $n$ in ${\NN}$ let $g_{n}$ denote the piecewise-linear function described in the proof above,
    and let $G_{n}$ be the antiderivative of $g_{n}$ such that $G_{n}(c) \,=\, 0$. Then for every $n$ in ${\NN}$ one has
        \begin{displaymath}
        |F(x)-G_{n}(x)|\,\,{\leq}\,\,\frac{2\,M\,(b-a)^{2}}{n} \mbox{ for all $x$ in $[a,b]$}.
        \end{displaymath}

\V

        {\bf Proof}\, From Inequality~\Ref{IneqE.83A} one has
        \begin{displaymath}
        \left|\left(G_{n+k}-G_{n}\right)(x)\right|\,\,{\leq}\,\,\frac{2M(b-a)^{2}}{n}
        \end{displaymath}
    for each $x$ in $[a,b]$ and each $k$ in ${\NN}$.
    The desired result now follows by letting $k$ approach ${\infty}$ and recalling that $F(x) \,=\, \lim_{j \,{\rightarrow}\, {\infty}} G_{j}(x)$.

\VV
}%\EndSkip
%---------------------------- F

\VV

                \section{{\bf Cauchy's Theorem on the Existence of Antiderivatives}} 
                \label{SectE45D}\IndB{ZZ Sections}{\Ref{SectE45D} Cauchy's Theorem on the Existence of Antiderivatives} %% NEW


\VV


        The next theorem is one of the most important in analysis.

\VV

             \subsection{\small{\bf Theorem} (Cauchy's Antiderivative Theorem)}
                         \label{ThmE45.125B}\IndBD{antiderivatives}{Cauchy's antiderivative theorem}

\V

        Let $f:I \,{\rightarrow}\, {\RR}$ be a function which is continuous on an interval $I$ in ${\RR}$. Then the function $f$ has an antiderivative on $I$.
    More precisely, if $c$ is any point of $I$, then $f$ has a unique antiderivative $F$ on $I$ whose value at $c$ is~$0$.
    (As usual, if the interval $I$ includes an endpoint $p$, then $F'(p)$ refers to the appropriate one-sided derivative.)

\V

        {\bf Remark}\,The proof given here differs quite a bit from that of Cauchy. It is due to H.~Lebesgue; see [LEBESGUE~1905]. Cauchy's original proof is outlined in Chapter~\Ref{ChaptH}.

\V


%%%
\begin{quotation}
{\footnotesize \underline{\Note}\IndB{\notes}{on Lebesgues' proof of Cauchy's Antiderivative Theorem} (on Lebesgues' proof of Cauchy's Antiderivative Theorem)
        The purpose of this {\Note} is outline the basic structure of the detailed proof given below. As usual, one can procede directly to the proof without reading this {\Note}.

    The concept of `continuity' is not obviously related to that of `antiderivative', so the truth of Cauchy's Antiderivative Theorem comes somewhat as a surprise. Lebesgue's proof makes this relation believable.
    Indeed, consider any pair of numbers in $I$ such that $a\,<\,c\,<\,b$, so that, by Theorem~\Ref{ThmD25.60}, there exists a sequence of
    continuous piecewise linear functions $g_{k}:{\RR} \,{\rightarrow}\, {\RR}$ such that $|f(x)-g_{k}(x)|\,<\,1/k$ for each $x{\in}[a,b]$;
    in particular, $\lim_{k \,{\rightarrow}\, {\infty}} g_{k}(x) \,=\, f(x)$ for each $x{\in}[a,b]$. By Theorem~\Ref{ThmE45.125A}, each $g_{k}$ has a unique antiderivative $G_{k}$ over ${\RR}$ with value $0$ at~$c$, 
    so that $G_{k}'(x) \,=\, g_{k}(x)$ for each $x$ in~$[a,b]$. It is easy to show that for each $x$ in $[a,b]$ the sequence $(G_{1}(x),\,{\ldots}\,G_{k},\,{\ldots}\,)$ is Cauchy, and thus is convergent.
    Let $F:[a,b] \,{\rightarrow}\, {\RR}$ be defined by the rule $F(x) \,=\, \lim_{k \,{\rightarrow}\, {\infty}} G_{k}(x)$. Lebesgue shows that $F$ is the desired antiderivative of $f$ on~$[a,b]$.

    In summary: $f$ has an antiderivative over $[a,b]$ because $f$ can be `nicely' approximated over $[a,b]$ by functions which themselves have antiderivatives over $[a,b]$.
    The details of the proof below use the precise meaning of `nicely'. The final step, extending this solution to the full interval~$I$, is then straight-forward.
}%EndFootNoteSize
\end{quotation}
%##


\V

        {\bf Proof}\, Let $a$ and $b$ be points of $I$ such that $a\,\,{\leq}\,\,c\,\,{\leq}\,\,b$.
    Since $f$ is continuous on $I$, one sees that for each positive integer $k$ there is a continuous function $g_{k}:{\RR} \,{\rightarrow}\, {\RR}$ such that

\VA

        \h Condition (i)\, $|g_{k}(x) - f(x)|\,<\,1/k$ for all $x$ in the interval $[a,b]$;

        \h Condition (ii) $g_{k}$ has a unique antiderivative $G_{k}$ on ${\RR}$ such that $G_{k}(c) \,=\, 0$.

\VA%\\\\\

\hspace*{\parindent}Indeed, Theorem~\Ref{ThmD25.60} implies that there is a continuous piecewise-linear function on ${\RR}$ which satisfies Condition~(i),
    while Theorem~\Ref{ThmE45.125A} implies that every such function satisfies Condition~(ii). Note also that Condition~(i)
    implies that for each $x$ in $[a,b]$ one has ${\displaystyle \lim_{k \,{\rightarrow}\, {\infty}} g_{k}(x) \,=\, f(x)}$.

\V

        \underline{Claim 1} For each $x$ in $[a,b]$ the sequence ${\Gamma}_{x} \,=\, \{G_{1}(x), G_{2}(x),\,{\ldots}\,\}$ is a Cauchy sequence.

        \underline{Proof of Claim 1} Let $k$ and $m$ be positive integers. Note that if $x{\in}[a,b]$ then, by Corollary~\Ref{CorE50.25},
    the Segment Form of the Mean-Value Theorem applied to the function $G_{k+m} - G_{k}$, there exists a number $\hat{x}$ in ${\mbox{Seg}\,[x,c]}$ such that the following holds:
        \begin{displaymath}
        G_{k+m}(x) - G_{k}(x) \,=\, (G_{k+m} - G_{k})(x) - (G_{k+m} - G_{k})(c)
     \,=\, 
        (G_{k+m} - G_{k})'(\hat{x})(x-c) \,=\,
        \end{displaymath}
        \begin{displaymath}
         (G'_{k+m}(\hat{x}) - G_{k}'(\hat{x}))(x-c)
     \,=\, 
       (g_{k+m}(\hat{x}) - g_{k}(\hat{x}))(x-c).
        \end{displaymath}
    It follows, after doing a judicious `add-and-substract' trick and using the Triangle Inequality, that
        \begin{displaymath}
        |G_{k+m}(x) - G_{k}(x)|\,\,{\leq}\,\,\left(|g_{k+m}(x) - f(x)| + |f(x) - g_{k}(x)|\right)|x-c|\,\,{\leq}\,\,\left(\frac{1}{k+m} + \frac{1}{k}\right)|b-a|\,\,{\leq}\,\,\frac{|b-a|}{2k}.
        \end{displaymath}
    In particular, if ${\varepsilon}\,>\,0$ is given, then one has
        \begin{equation}
        \label{IneqE.83A}
        |G_{k+m}(x) - G_{k}(x)|\,\,{\leq}\,\,\frac{|b-a|}{2k}\,<\,{\varepsilon}
        \end{equation}
    for all $k$ in ${\NN}$ such that $k\,>\,|b-a|/(2{\varepsilon})$ and all $m$ in ${\NN}$. In particular, the sequence ${\Gamma}(x)$ is Cauchy, as claimed.

        The preceding result suggests a natural candidate for the desired antiderivative of the given function~$f$; namely,
    define $F:[a,b] \,{\rightarrow}\, {\RR}$ by the rule $F(x) \,=\, \lim_{k \,{\rightarrow}\, {\infty}} G_{k}(x)$ for all $x$ in the interval $[a,b]$.
    (The fact that for each $x$ in $[a,b]$ this limit exists and is finite follows from Claim~1.)

\V

    \underline{Claim 2} The function $F$ just described is the desired antiderivative of $f$.
    More precisely, for each $x_{0}$ in the open interval $(a,b)$ and for each ${\varepsilon}\,>\,0$,
    there exists ${\delta}\,>\,0$ such that if $h$ satisfies $0\,<\,|h|\,<\,{\delta}$ then
        \begin{displaymath}
        \left|\frac{F(x_{0} + h) - F(x_{0})}{h} - f(x_{0})\right|\,\,{\leq}\,\,{\varepsilon} \h ({\ast})
        \end{displaymath}

\V

        \underline{Proof of Claim 2} Let $x_{0}$ be any point of $(a,b)$, and let $h$ be any nonzero real number small enough that $x_{0}+h$ is also in $(a,b)$; more precisely, $|h|\,<\,{\min}\,\{|x_{0}-a|,|b-x_{0}|\}$.
    Then for each $k$ in ${\NN}$ one has, for some $\hat{x}$ in $\mbox{Seg}\,[x_{0}, x_{0}+h]$,
        \begin{displaymath}
        G_{k}(x_{0}+h) - G_{k}(x_{0}) - g_{k}(x_{0})h \,=\, G_{k}'(\hat{x})h - g_{k}(x_{0})h \,=\, (g_{k}(\hat{x}) - g_{k}(x_{0}))h.
        \end{displaymath}
    It then follows from the Extended Triangle Inequality that
        \begin{displaymath}
        \left|G_{k}(x_{0}+h) - G_{k}(x_{0}) - g_{k}(x_{0})h\right| 
    \,\,{\leq}\,\,\left(\left|g_{k}(\hat{x}) - f(\hat{x})\right| +|f(\hat{x}) - f(x_{0})| + \left|f(x_{0}) - g_{k}(x_{0})\right|\right){\cdot}|h| \h ({\ast}{\ast})
        \end{displaymath}
    Now let ${\varepsilon}\,>\,0$ be given. Using the uniform continuity of $f$ on the interval $[a,b]$, choose ${\delta}\,>\,0$ small enough that if $x$ and $y$ are any points of $[a,b]$ such that 
    $|y-x|\,<\,{\delta}$, then one has $|f(y)-f(x)|\,<\,{\varepsilon}/3$.
    It then follows from Condition~(ii) above that if $0\,<\,|h|\,<\,{\delta}$ then~$({\ast}{\ast})$ implies
        \begin{displaymath}
        \left|G_{k}(x_{0}+h) - G_{k}(x_{0}) - g_{k}(x_{0})h\right| \,<\,\left(\frac{{\varepsilon}}{3} + \frac{{\varepsilon}}{3} + \frac{{\varepsilon}}{3}\right)|h| \,=\, {\varepsilon}|h|
        \end{displaymath}
    Divide by $|h|$ and do some simple algebraic simplification to get
        \begin{displaymath}
        \left|\frac{G_{k}(x_{0}+h) - G_{k}(x_{0})}{h} - g_{k}(x_{0})\right|\,<\,{\varepsilon}
        \end{displaymath}
    This last inequality is true for {\em all} sufficiently large $k$. In particular,
    by letting $k$ approach ${\infty}$ one gets the desired inequality~$({\ast})$ for all $h$ such that $0\,<\,|h|\,<\,{\delta}$.

    Since this is true for all ${\varepsilon}\,>\,0$, it follows that $F'(x_{0})$ exists and equals $f(x_{0})$.
    Since $x_{0}$ can be any point of $(a,b)$, it follows that $F$ is an antiderivative of $f$ on $(a,b)$.
    The fact that $F(c) \,=\, 0$ follows from the fact that, by definition, $G_{k}(c) \,=\, 0$ for each~$k$.

\V

        \underline{Summary} For each $a$ and $b$ in $I$ such that $a\,<\,c\,<\,b$,
    there exists a unique antiderivative $F$ of $f$ on $(a,b)$ such that $F(c) \,=\, 0$.

        The desired result, namely that $f$ has a unique antiderivative, whose value at $c$ is $0$, on the {\em full} open interval $I$,
    now follows by letting $a$ approach ${\inf}\,I$ and $b$ approach ${\sup}\,I$; the details are left to the reader. \Q

\VV

                \section{{\bf Numerical Methods for Antiderivatives}} 
                \label{SectE45C}\IndB{ZZ Sections}{\Ref{SectE45C} Numerical Methods for Antiderivatives}


\VV

        If $f:I \,{\rightarrow}\, {\RR}$ is a continuous function on the open interval $I$,
    then the existence of an antiderivative $F:I \,{\rightarrow}\, {\RR}$ of $f$ on $I$ is guaranteed by the preceding results.
    The numbers of greatest interest for such an antiderivative are differences of the form $F(b) - F(a)$, with $a$ and $b$ in $I$ and $a\,<\,b$.
    In elementary calculus one normally computes such differences using a method that is independent of the specific choice of $a$ and~$b$:
    from the formula for $f(x)$ on $I$, obtain an explicit formula for $F(x)$; then substitute $x \,=\, a$ and $x \,=\, b$ into the latter formula, and subtract.

        Unfortunately, in many cases determining an explicit formula for $F$ from the formula for $f$ can be difficult or even impossible.
    In such cases one may need to use methods which depend on the specific choice of $a$ and~$b$ and which provide only estimates of $F(b)-F(a)$.
    Most of these methods fall under the heading of so-called `numerical methods'; there is a vast literature of such methods.
    We consider several of the most familar here.

\V

        Perhaps the most straight-forward way to estimate the difference $F(b) - F(a)$, where $F' \,=\, f$, is to use the Mean-Value Inequality, 
    Corollary~\Ref{CorE40.69A}. In the present context this Inequality takes the more precise form
        \begin{displaymath}
        m\,(b-a)\,\,{\leq}\,\,F(b)-F(a)\,\,{\leq}\,\,M\,(b-a),
        \end{displaymath}
    where $m$ is the minimum value of $f$ on $[a,b]$ and $M$ is the corrsponding maximum value; these extreme values exist because $f$ is continuous on~$[a,b]$.
    Of course the quantities $m$ and $M$ need not be close to each other, so this estimate may be of little value.
    To improve it, divide the interval $[a,b]$ into short subtintervals and apply the analogous inequalities on each of them.
    More precisely, let ${\cal P} \,=\, \{a \,=\, x_{0}\,<\,x_{1}\,<\,\,{\ldots}\,\,<\,x_{n-1}\,<\,x_{n}\,=\,b\}$
    be a partition of $[a,b]$ into $n$ subintervals $[x_{j-1},x_{j}]$, $1\,\,{\leq}\,\,j\,\,{\leq}\,\,n$.
    For each such index $j$ let $m_{j}$ be the minimum value of $f$ on the subinterval $[x_{j-1},x_{j}]$, and let $M_{j}$ be the corresponding maximum.
    Then from the equation
        \begin{displaymath}
        F(b) - F(a) \,=\, (F(x_{n})-F(x_{n-1})) + \,{\ldots}\, + (F(x_{1})-F(0)), \mbox{ that is, }
        F(b) - F(a) \,=\, \sum_{j=1}^{n} {\Delta}F_{j}(x),
        \end{displaymath}
    where ${\Delta}F_{j}(x) \,=\, F(x_{j})-F(x_{j-1})$, one obtains the estimate
        \begin{displaymath}
        m_{1}\,{\Delta}x_{1} + m_{2}\,{\Delta}x_{2} + \,{\ldots}\,+ m_{n}\,{\Delta}x_{n}
        \,\,{\leq}\,\,F(b) - F(a)\,\,{\leq}\,\,
        M_{1}\,{\Delta}x_{1} + M_{2}\,{\Delta}x_{2} + \,{\ldots}\,+ M_{n}\,{\Delta}x_{n};
        \end{displaymath}
    as usual, ${\Delta}x_{j} \,=\, x_{j}-x_{j-1}$. For brevity, denote the quantities
    $m_{1}\,{\Delta}x_{1} + m_{2}\,{\Delta}x_{2} + \,{\ldots}\,+ m_{n}\,{\Delta}x_{n}$ and
    $M_{1}\,{\Delta}x_{1} + M_{2}\,{\Delta}x_{2} + \,{\ldots}\,+ M_{n}\,{\Delta}x_{n}$ by $L(f;{\cal P})$ and $U(f;{\cal P})$, respectively.
    In particular, the numbers $L(f;{\cal P})$ and $U(f;{\cal P})$ determine a full range of reasonable estimates $F(b)-F(a) \,{\approx}\, A$:
    namely, let $A$ be any number such that $L(f;{\cal P})\,\,{\leq}\,\,A\,\,{\leq}\,\,U(f;{\cal P})$.
    Here are a few of the commonly used ways to choose the estimating number~$A$:

\V

        \h (1) Let ${\zeta} \,=\, (z_{1}, z_{2},\,{\ldots}\,z_{n})$ be an ordered list of `sample points'
    such that for each index $j$ one has $z_{j}{\in}[x_{j-1},x_{j}]$. It is clear that $m\,\,{\leq}\,\,f(z_{j})\,\,{\leq}\,\,M_{j}$ for each~$j$, so that
    $F(b) - F(a) \,{\approx}\, A_{{\cal P};{\zeta}}$, where $A_{{\cal P}; {\zeta}} \,=\, f(z_{1})\,{\Delta}x_{1} + \,{\ldots}\, + f(z_{n})\,{\Delta}\,x_{n}$.
    Here are several popular `Rules' for choosing the sample points $z_{j}$:

\VA

        \h \h (Left-hand Rule) $z_{j} \,=\, x_{j-1}$; that is,
        \begin{displaymath}
        A_{{\cal P}; {\lambda}} \,=\, \sum_{j=1}^{n} f(x_{j-1})\,{\Delta}x_{j}.
        \end{displaymath}

        \h \h (Right-hand Rule) $z_{j} \,=\, x_{j}$; that is,
        \begin{displaymath}
        A_{{\cal P}; {\rho}} \,=\, \sum_{j=1}^{n} f(x_{j})\,{\Delta}x_{j}.
        \end{displaymath}

        \h \h (Midpoint Rule) $z_{j} \,=\, (x_{j-1}+x_{j})/2$; that is,
        \begin{displaymath}
        A_{{\cal P}; {\mu}} \,=\, \sum_{j=1}^{n} f\left(\frac{x_{j-1}+x_{j}}{2}\right)\,{\Delta}x_{j}.
        \end{displaymath}

        \h \h (Minimum-Value Rule) $z_{j}$ is a point in $[x_{j-1},x_{j}]$ at which $f(z_{j}) \,=\, m_{j}$; that is,
        \begin{displaymath}
        A_{{\cal P}; \mbox{min}} \,=\, L(f;{\cal P})
        \end{displaymath}

        \h \h (Maximum-Value Rule) $z_{j}$ is a point in $[x_{j-1},x_{j}]$ at which $f(z_{j}) \,=\, M_{j}$; that is,
        \begin{displaymath}
        A_{{\cal P}; \mbox{max}} \,=\, U(f;{\cal P})
        \end{displaymath}

        \h \h (Mean-Value-Theorem Rule) $z_{j}$ is a point in the interval $[x_{j-1},x_{j}]$ at which
    $f(z_{j}) \,=\, {\displaystyle \left(\frac{F(x_{j})-F(x_{j-1})}{x_{j}-x_{j-1}}\right)}$; that is,
        \begin{displaymath}
        A_{{\cal P}; \mbox{MVT}} \,=\, \sum_{j=1}^{n} f(z_{j})\,{\Delta}x_{j} \,=\, 
        \sum_{j=1}^{n} (F(x_{j})-F(x_{j-1})) \,=\, F(b) - F(a).
        \end{displaymath}

        {\bf Remark} The final rule provides an approximation, $A_{{\cal P}; \mbox{MVT}} \,{\approx}\, F(b)-F(a)$, which in fact is exactly correct.
    Unfortunately, it is almost always impossible to determine the corresponding values of the points $z_{j}$
    without already knowing the exact value of $F(b)-F(a)$ in advance, making this rule of little practical value.
    Similarly, the difficulty of finding extrema for the function $f$ on the $n$ subintervals makes the Minimum/Maximum-Value Rules of little value.

% EXERCISE GAUSSIAN INTEGRATION

\V

        \h (2) Since each of the estimating values $A_{{\cal P}; {\lambda}}$, $A_{{\cal P}; {\rho}}$ and $A_{{\cal P}; {\mu}}$ satisfies the condition
    $L(f;{\cal P})\,\,{\leq}\,\,A\,\,{\leq}\,\,U(f;{\cal P})$, it follows that any linear combination
    $p\,A_{{\cal P}; {\lambda}} + q\,A_{{\cal P}; {\rho}} + r\,A_{{\cal P}; {\mu}}$,
    with $0\,\,{\leq}\,\,p, q, r\,\,{\leq}\,\,1$ and $p+q+r \,=\, 1$, also satisfies this condition. The two most familiar examples are these:

\VA

        \h \h (Trapezoid Rule: $p \,=\, q \,=\, 1/2, r \,=\, 0$)
    ${\displaystyle A_{{\cal P}; {\tau}} \,=\, \frac{A_{{\lambda}} + A_{{\rho}}}{2}}$.

        \h \h (Simpson's Rule: $p \,=\, q \,=\, 1/6, r \,=\, 1/3$)
    ${\displaystyle A_{{\cal P}; {\sigma}} \,=\, \frac{A_{{\cal P}; {\tau}} + 2\,A_{{\cal P}; {\mu}}}{3}}$.

\V

        {\bf Remarks} (1) For simplicity, many calculus texts formulate these rules only in the special case the partition ${\cal P}$ has constant spacing;
    that is, all $n$ of the subintervals $[x_{j-1},x_{j}]$ are of equal length; equivalently, ${\Delta}x_{j} \,=\, (b-a)/n$ for each~$j$.

\V

        (2) Lebesgue's proof of Cauchy's Antiderivative Theorem, given above, involves repeated use of the Trapezoid Rule.

\VV

        The next result shows that these estimates can be guaranteed to be as accurate as one wishes by choosing the partition ${\cal P}$ appropriately.
    The key to this result is the observation that from the inequalities 
        \begin{displaymath}
        L(f;{\cal P})\,\,{\leq}\,\,F(b)-F(a)\,\,{\leq}\,\,U(f;{\cal P})
    \mbox{ and }
        L(f;{\cal P})\,\,{\leq}\,\,A_{{\cal P}; {\zeta}}\,\,{\leq}\,\,U(f;{\cal P})
        \end{displaymath}
    obtained above, it follows that
        \begin{displaymath}
        0\,\,{\leq}\,\,|(F(b)-F(a)) - A_{{\cal P}; {\zeta}}|\,\,{\leq}\,\,U(f;{\cal P})-L(f;{\cal P}) \h ({\ast})
        \end{displaymath}

\VV

             \subsection{\small{\bf Theorem}}
             \label{TheoremE45.130}

\V

        Suppose that $f:I \,{\rightarrow}\, {\RR}$ is continuous on an open interval~$I$,
    and that $F:I \,{\rightarrow}\, {\RR}$ is an antiderivative of $f$ on~$I$. Let $a$ and $b$ be points of $I$ such that $a\,<\,b$. Then:

\V

        (a) For every ${\varepsilon}\,>\,0$ there exists ${\delta}\,>\,0$ such that if
    ${\cal P} \,=\, \{a \,=\, x_{0}\,<\,x_{1}\,<\,\,{\ldots}\,\,<\,x_{n-1}\,<\,x_{n} \,=\, b\}$ is any partition of $[a,b]$ whose mesh $||{\cal P}||$
    satisfies $||{\cal P}||\,<\,{\delta}$, then for every list ${\zeta} \,=\, (z_{1}, z_{2},\,{\ldots}\,z_{n})$ of sample points associated with ${\cal P}$
    one has
        \begin{displaymath}
        \left|(F(b)-F(a)) - A_{{\cal P};{\zeta}}\right|\,<\,{\varepsilon}.
        \end{displaymath}
    (As usual, $||{\cal P}||$ is defined to be the the largest of the numbers ${\Delta}x_{j}$, $1\,\,{\leq}\,\,j\,\,{\leq}\,\,n$.)

\V

        (b) The quantity $F(b) - F(a)$ is the unique number $B$ such that $L(f;{\cal P})\,\,{\leq}\,\,B\,\,{\leq}\,\,U(f;{\cal P})$
    for every partition ${\cal P}$ of the interval~$ \,{\subseteq}\, a,b]$.

\V

        {\bf Proof}\, (a)\,By Theorem~\Ref{ThmD25.70}, the function $f$ is uniformly continuous on the closed bounded interval~$[a,b]$.
    Thus, given ${\varepsilon}\,>\,0$ let ${\delta}\,>\,0$ be small enough that if $c$ and $d$
    are any points in $[a,b]$ such that $|d-c|\,<\,{\delta}$, then $|f(d)-f(c)|\,<\,{\varepsilon}/(b-a)$.
    Let ${\cal P} \,=\, \{a \,=\, x_{0}\,<\,x_{1}\,<\,\,{\ldots}\,\,<\,x_{n-1}\,<\,x_{n} \,=\, b\}$ be a partition as above such that $||{\cal P}||\,<\,{\delta}$.
    Then for each index $j \,=\, 1,2,\,{\ldots}\,n$ one has $|d-c|\,\,{\leq}\,\,(x_{j}-x_{j-1})\,<\,{\delta}$ for each $c$ and $d$ in the subinterval~$[x_{j-1},x_{j}]$.
    In particular, choose $c$ so that $f(c) \,=\, m_{j}$ and choose $d$ so that $f(d) \,=\, M_{j}$. Then $0\,\,{\leq}\,\,M_{j} - m_{j}\,<\,{\varepsilon}/(b-a)$.
    Multiply by the positive quantity ${\Delta}x_{j}$ and sum over $j$ to get
        \begin{displaymath}
        0\,\,{\leq}\,\,\sum_{j=1}^{n} (M_{j}-m_{j})\,{\Delta}x_{j}\,<\,\frac{{\varepsilon}}{b-a}\,\sum_{j=1}^{n} {\Delta}x_{j} \,=\, {\varepsilon}. 
        \end{displaymath}
    It follows from Inequality~$({\ast})$ above that $|(F(b)-F(a)) - A_{{\cal P}; {\zeta}}|\,<\,{\varepsilon}$, as required.

\V

        (b) This follows easily from the preceding. \Q

\VV

            \subsection{\small{\bf Remark}}
            \label{RemrkE45.140}

\V

        The preceding discussion provides a natural motivation for one of the most characteristic notations found in classical analysis.
    Indeed, recall the equation $F(b)-F(a) \,=\, \sum_{j=1}^{n} {\Delta}F_{j}(x)$ mentioned above,
    which expresses the `whole difference' $F(b)-F(a)$ as the sum of `partial differences' ${\Delta}F_{j}(x)$ as $j$ runs from $j \,=\, 1$ to $j \,=\, n$. 
    Leibniz' treatment of calculus often replaces such discrete sums of ordinary quantities by `continuous' sums of infinitely small quantities.
    Thus, he expresses $F(b)-F(a)$ as the `continuous sum' of infinitely small differences $dF(x)$ as $x$ runs from $x \,=\, a$ to~$x \,=\, b$.
    Instead of our use of the upper case Greek letter $\Sigma$ as a reminder of the first letter of the word `sum',
    he uses the symbol ${\displaystyle \int}$, an elongated version of the letter `S', for the same purpose when summing infinitely small quantities.
    Thus, when combined with the formula $dF(x) \,=\, F'(x)\,dx$ obtained above, one gets, using the hypothesis $F'(x) \,=\, f(x)$, that
        \begin{displaymath}
        F(b) - F(a) \,=\, \int\,dF(x) \,=\, \int\,f(x)\,dx.
        \end{displaymath}
    It is assumed here that one knows that $x$ runs from $a$ to $b$ either from the context or by stating it in words;
    the standard calculus notation ${\displaystyle \int_{a}^{b} f(x)\,dx}$ arose only at the beginning of the nineteenth century.

        The process in calculus of obtaining `whole' quantities by summing infinitely small `partial quantities', as above,
    is called `integration', from the Latin for `whole' or `complete'. Likewise, the symbol ${\displaystyle \int}$ is called the `integral sign', and the quantity
    ${\displaystyle \int f(x)\,dx}$ is the `integral of the function~$f$'. From the time of Leibniz until the nineteenth century,
    the concept of `integrals' was identified with what we now call `antiderivatives'.\IndB{integrals}{Leibniz' antiderivative viewpoint}
    Even now, most calculus texts refer to the methods of computing antiderivatives as `Techniques of Integration'.
    The reason for the modern practice of separating the concept of `integral' from that of `antiderivative' grew from profound studies of Fourier,
    published in~$1822$, on the theory of heat flow. This transition is discussed at the begining of the next chapter.


%--------------- M EXERCISE??
\StartSkip{
             \subsection{\small{\bf Corollary}}
            \label{CorE45.126B}

\V

        Suppose that $f:I \,{\rightarrow}\, {\RR}$ is a $C^{1}$ function on an open interval $I$ in ${\RR}$.
    Let $c$ be a point in $I$, and let $F:I \,{\rightarrow}\, {\RR}$ be the unique antiderivative of $f$ on $I$ such that $F(c) \,=\, 0$.
    Let $a$ and $b$ be elements of $I$ such that $a\,<\,c\,<\,b$, and let $M$ be an upper bound of $|f'|$ on the closed bounded interval $[a,b]$.
    For each $n$ in ${\NN}$ let $g_{n}$ denote the piecewise-linear function 


described in the proof above,
    and let $G_{n}$ be the antiderivative of $g_{n}$ such that $G_{n}(c) \,=\, 0$. Then for every $n$ in ${\NN}$ one has
        \begin{displaymath}
        |F(x)-G_{n}(x)|\,\,{\leq}\,\,\frac{2\,M\,(b-a)^{2}}{n} \mbox{ for all $x$ in $[a,b]$}.
        \end{displaymath}

\V

        {\bf Proof}\, From Inequality~\Ref{IneqE.83A} one has
        \begin{displaymath}
        \left|\left(G_{n+k}-G_{n}\right)(x)\right|\,\,{\leq}\,\,\frac{2M(b-a)^{2}}{n}
        \end{displaymath}
    for each $x$ in $[a,b]$ and each $k$ in ${\NN}$.
    The desired result now follows by letting $k$ approach ${\infty}$ and recalling that $F(x) \,=\, \lim_{j \,{\rightarrow}\, {\infty}} G_{j}(x)$.

}%\EndSkip
%---------------- M


                \section{{\bf The Standard Transcendental Functions}}
                \label{SectE45A}\IndB{ZZ Sections}{\Ref{SectE45A} The Standard Transcendental Functions}


\V

        All the explicit functions considered so far in this chapter can be constructed using only the basic operations of algebra a finite number of times.
    One knows from elementary calculus, of course, that there other functions which also play vital roles in calculus;
    namely the standard exponential, logarithmic, trigonometric and inverse trigonometric functions.
    These special functions are sometimes called the {\bf Standard Transcendental Functions}
    (because their construction `transcends' the use of finite algebra).

        The goal of the present section is to develop these functions rigorously by using the properties of antiderivatives obtained in the preceding section.
    This treatment is quite instructive, but somewhat tedious. Some instructors may elect to simply assume as `known'
    the standard facts about these functions which one learns in elementary calculus and thus save time by skipping portions of this section.

\V

        The key facts needed to construct the standard transcendental functions are given in the next result.

\V

             \subsection{\small{\bf Theorem}}
            \label{ThmE45.127A}

\V

\hspace*{\parindent}(a) There is a unique function $F:(0,+{\infty}) \,{\rightarrow}\, {\RR}$
    such that ${\displaystyle F'(x) \,=\, \frac{1}{x}}$ for all $x\,>\,0$, and $F(1) \,=\, 0$.

\V

        (b) There is a unique function $G:(-1,1) \,{\rightarrow}\, {\RR}$ such that
    ${\displaystyle G'(x) \,=\, \frac{1}{\sqrt{1-x^{2}}}}$ for all $x$ in $(-1,1)$ and $G(0) \,=\, 0$.

\V

        \underline{Proof} Both statements follow from the Cauchy Antiderivative Theorem because the functions $f:(0,+{\infty}) \,{\rightarrow}\, {\RR}$ and
     $g:(-1,1) \,{\rightarrow}\, {\RR}$, given by $f(x) \,=\, 1/x$ and $g(x) \,=\, \/\sqrt{1-x^{2}}$, respectively, are continuous on their given domains.

\VV


             \subsection{\small{\bf Definition}}
             \label{DefE45.127B}

\V

\hspace*{\parindent}(1) The function $F$ described in Part~(a) of the preceding theorem is called the {\bf natural logarithm function\IndC{functions}{special functions}{logarithm function, natural}}; it is denoted ${\ln}$, so that $F(x) \,=\, {\ln}\,x$ for all $x\,>\,0$.

\V

        (2) The function $G$ described in Part~(b) of the preceding theorem is called the
    {\bf  (principle) arcsine function}\IndC{functions}{special functions}{arcsine function, principle}, and is denoted ${\Arcsin}$.
(The reason for using the adjective `principle' and upper-case letter `A' will be explained later.)
%}

\VV

        {\bf Properties of the Natural Logarithm Function and Related Functions}

\VV

        The next result shows that the function $\ln$ defined above has the usual properties familiar from elementary calculus.

\V

             \subsection{\small{\bf Theorem}}
            \label{ThmE45.127C}

\V

        The natural logarithm function ${\ln}$ has the following properties:

\V

        (a) ${\ln}'(x) \,=\, {\displaystyle \frac{1}{x}}$ for all $x\,>\,0$, and ${\ln}\,(1) \,=\, 0$.

\V

        (b) ${\ln}\,(x{\cdot}y) \,=\, {\ln}\,(x) \,+\, {\ln}\,(y)$ for all $x,y\,>\,0$.

\V

        (c)  ${\ln}\,(x^{-1}) \,=\, -{\ln}\,(x)$ for all $x\,>\,0$.

\V

        (d) ${\ln}\,(x^{k}) \,=\, k{\ln}\,x$ for all $x\,>\,0$ and all $k$ in ${\ZZ}$.

\V

        (e) $\lim_{x \,{\rightarrow}\, +{\infty}} {\ln}\, x \,=\, +{\infty}$,
    and $\lim_{x \,{\searrow}\, 0} {\ln}\,x \,=\, -{\infty}$.

\V

        (f) The function ${\ln}$ is a bijection of the interval $(0,+{\infty})$ onto ${\RR}$.

\V

        (g) $\lim_{x \,{\rightarrow}\, 0+} x^{k}{\ln}\,x \,=\, 0$ for every $k$ in ${\NN}$.

\V


        {\bf Partial Proof} 

\V

        (a) These are just the defining properties of the function $\ln$; they are included in the list for ease of reference.

\V

        (b) For fixed $y\,>\,0$ define functions $g$ and $h$ on $(0,+{\infty})$ by the rules
        \begin{displaymath}
        g(x) \,=\, {\ln}\,(x{\cdot}y), \h h(x) \,=\, {\ln}\,(x) \,+\, {\ln}\,(y).
        \end{displaymath}
    By the Chain Rule one has $g'(x) \,=\, {\displaystyle \frac{y}{xy} \,=\, \frac{1}{x}}$,
    while from the Addition Rule for Derivatives (and the fact that $y$ is `constant') one has $h'(x) \,=\, {\displaystyle \frac{1}{x}}$.
    That is, $g' \,=\, h'$ for each point of the domain $(0,+{\infty})$.
    Thus, by Corollary~\Ref{CorE40.50}, one has $h \,=\, g+C$ for some constant function $C$.
    However, note that $g(1) \,=\, {\ln}\,(1{\cdot}y) \,=\, {\ln}\,(y)$, and $h(1) \,=\, {\ln}\,(1) + {\ln}\,(y) \,=\, {\ln}\,(y)$.
    It follows that one must have $C \,=\, 0$ and thus $g \,=\, h$.
    In particular, for each $x,y\,>\,0$ one has ${\ln}\,(x{\cdot}y) \,=\, {\ln}\,(x) + {\ln}\,(y)$, as claimed.

\V

        (c) Note that, by Part~(b), one has ${\ln}\,(x) + {\ln}\,(x^{-1}) \,=\, {\ln}\,(x{\cdot}x^{-1}) \,=\, {\ln}\,(1) \,=\, 0$.
    The desired result follows easily.

\V

        (d), (e) and (f): The simple proofs of these parts of the theorem are left as exercises.

\V

        (g) Notice that one can express the quantity $x^{k}{\ln}\,x$ in the form ${\displaystyle \frac{f(x)}{g(x)}}$, where $\lim_{x \,{\rightarrow}\, 0+} f(x) \,=\, -{\infty}$ and $\lim_{x \,{\rightarrow}\, 0+} g(x) \,=\, +{\infty}$.
    Indeed, let $f(x) \,=\, {\ln}\,(x)$ and $g(x) \,=\, x^{-k}$.
    Note also that $f'(x) \,=\, 1/x$ and $g'(x) \,=\, -kx^{-k-1}$.
    Thus
        \begin{displaymath}
        \frac{f'(x)}{g'(x)} \,=\, \frac{1/x}{(-k)x^{-k-1}} \,=\, -\frac{x^{k}}{k}.
        \end{displaymath}
    Since $\lim_{x \,{\rightarrow}\, 0+} x^{k} \,=\, 0$, L'H\^{o}pital's Rule can be used to conclude that $\lim_{x \,{\rightarrow}\, 0+} x^{k}{\ln}\,x \,=\, 0$. \Q

\VV

        It follows from Part~(f) of the preceding theorem that the function ${\ln}$ has an inverse ${\ln}^{-1}:{\RR} \,{\rightarrow}\, (0,+{\infty})$ which maps ${\RR}$ onto the set of positive real numbers.

\V

             \subsection{\small{\bf Definition}}
            \label{DefE45.127D}

        The function ${\ln}^{-1}:{\RR} \,{\rightarrow}\, (0,+{\infty})$ described above is called the (standard) {\bf \IndAA{exponential function}},
    and is denoted by~$\exp$.

\V


        The next result summarizes the main properties of the function $\exp$.

\V

             \subsection{\small{\bf Theorem}}
            \label{ThmE45.127E}

\V

\hspace*{\parindent}(a) The function ${\exp}$ is differentiable at each point of ${\RR}$, and ${\exp}'(x) \,=\, {\exp}\,(x)$ for all $x$ in ${\RR}$; also, ${\exp}\,(0) \,=\, 1$.

\V

        (b) ${\exp}\,(x+y) \,=\, ({\exp}\,(x)){\cdot}({\exp}\,(y))$ for all $x$ and $y$ in ${\RR}$.

        \h Special case: ${\exp}\,(0) \,=\, 1$.

\V

        (c) If $x{\in}{\RR}$ and $m{\in}{\ZZ}$ then ${\exp}\,(mx) \,=\, \left({\exp}\,x\right)^{m}$.

        \h Special case: ${\exp}\,(-x) \,=\, 1/{\exp}\,(x)$ for all $x$ in ${\RR}$.

\V

        (d) More generally, if $x{\in}{\RR}$ and $r{\in}{\QQ}$, then ${\exp}\,(rx) \,=\, \left({\exp}\,x\right)^{r}$.
    (`Rational exponents' are defined in Example~\Ref{ExampD30.110}.)

\V

        (e) Define $e$ to be the number ${\exp}\,(1)$. Then ${\ln}\,(e) \,=\, 1$.
    In addition, ${\exp}\,(r) \,=\, e^{r}$ for all rational numbers $r$.

\V

        (f) $\lim_{x \,{\rightarrow}\, +{\infty}}x^{k}e^{-x} \,=\, 0$ for each $k$ in ${\NN}$.


\V

        (g) Let $I$ be an open interval in ${\RR}$, and suppose that $f:I \,{\rightarrow}\, {\RR}$ is a function such that $f'(x) \,=\, f(x)$ for all $x$ in $I$.
    Then there exists a constant $A$ such that $f(x) \,=\, A{\exp}\,(x)$ for all $x$ in $I$.

\V

        The proofs of Parts (a) through (f) follow easily from corresponding properties of the logarithm function.
    The proof of~(g) can be obtained by differentiating the quotient $f/{\exp}$. The details are left as exercises. \Q

\VV

        In Definition~\Ref{DefB10.30} we introduced the standard `power' notion $c^{p}$, where $c$ is a real number and $p$ is a natural number,
    as a shorthand for the repeated multiplication $c{\cdot}c{\cdot}\,{\ldots}\,{\cdot}c$, with the factor $c$ appearing $p$ times.
    Later we extended this `exponent' notation to include expressions of the form $c^{p}$ in which $p$ can be any integer, at least if $c \,\,{\neq}\,\, 0$.
    We then further extended the notation $c^{p}$ to make sense when $p$ is any rational number, although now $c$ needs to be a {\em positive} real number.
    In light of Part~(e) of the preceding theorem, it is now possible to extend the exponent notation
    one more time to allow for exponents which can be any {\em real} number.

\V

             \subsection{\small{\bf Definition}}
            \label{DefE45.127F}

\V

\hspace*{\parindent}(1) The number $e \,=\, {\exp}\,(1)$ described in Part~(e) of Theorem~\Ref{ThmE45.127E} is called the {\bf \IndA{base of the natural logarithms}}.

\V

        (2) If $x$ is a real number then the {\bf $x$ power of the number $e$} is the number ${\exp}\,x$.
    It is usually denoted by the expression $e^{x}$, which is pronounced `$e$ to the power~$x$'.

\V

        (3) More generally, if $b\,>\,0$ and $x$ is any real number, then the {\bf $x$ power of $b$} is the number ${\exp}\,(x{\cdot}{\ln}\,(b))$;
    that is, $e^{x{\cdot}{\ln}\,b}$. It is usually denoted by the expression $b^{x}$, pronounced `$b$ to the power~$x$'.

\V

        The next result summarizes some of the main facts associated with the `exponent notation'.

\V

             \subsection{\small{\bf Theorem}}
            \label{ThmE45.127G}

\V

        Let $b$ be a positive real number.

\V

        (a) If $x$ is a real number then $b^{x}\,>\,0$ and $b^{-x} \,=\, 1/b^{x}$; in particular, $b^{0} \,=\, 1$.

\V

        (b) If $x$ and $y$ are real numbers then $b^{x+y} \,=\, b^{x}{\cdot}b^{y}$ and $\left(b^{x}\right)^{y} \,=\, b^{(xy)}$.

\V

        (c) If $x$ a rational number, or an integer, or a natural number, then the value of the number $b^{x}$ given in the preceding definition agrees with the value assigned to such expressions in our earlier definitions.

\V

        (d) Let $f:{\RR} \,{\rightarrow}\, {\RR}$ be defined by the formula $f(x) \,=\, b^{x}$ for each $x$ in ${\RR}$.
    Then $f$ is differentiable on ${\RR}$, and one has $f'(x) \,=\, \left({\ln}\,(b)\right){\cdot}b^{x}$.

\V

        The simple proof is left as an exercise. \Q

\VV

        In Section~\Ref{SectE35} we construct certain $C^{k}$ `bump functions'; see Definition~\Ref{DefE35.110}.
    Because the underlying functions for this construction are polynomials, these bump functions are not of class $C^{{\infty}}$; they are {\em strictly}~$C^{k}$.
    With the introduction of the transcendental function $\exp$, however, one can produce $C^{{\infty}}$ bump functions.
    The key is the following result.

\V

             \subsection{\small{\bf Lemma}}
            \label{LemmaE45.127H}
\V

\hspace*{\parindent}(a) For every $k$ in ${\NN}$ one has
        \begin{equation}
        \label{EqnE.83B}
        \lim_{x \,{\rightarrow}\, 0+} \frac{e^{-1/x}}{x^{k}} \,=\, 0
        \end{equation}

\V

        (b) For every $k$ in ${\NN}$ one has
        \begin{equation}
        \label{EqnE.83C}
        \lim_{x \,{\rightarrow}\, 0} \frac{e^{-1/x^{2}}}{x^{k}} \,=\, 0
        \end{equation}

\V

        {\bf Proof}\,

\V

        (a) Let $u \,=\, 1/x$. Then one has
        \begin{displaymath}
        \frac{e^{-1/x}}{x^{k}} \,=\, u^{k}e^{-u}.
        \end{displaymath}
    Moreover, one sees that $u \,{\rightarrow}\, +{\infty}$ as $x \,{\rightarrow}\,  +0$.
    The desired result now follows from Part~(g) of Theorem~\Ref{ThmE45.127E}.

\V

        (b) This follows easily from Part~(a); the details are left as an exercise. \Q


\V

             \subsection{\small{\bf Example}}
            \label{ExampE45.127J}

\V

        Let $f_{0}:{\RR} \,{\rightarrow}\, {\RR}$ be given by the formula
        \begin{displaymath}
        f_{0}(x) \,=\, \left\{
        \begin{array}{cl}
             0   & \mbox{if $x\,\,{\leq}\,\,0$} \\
        e^{-1/x} & \mbox{if $x\,>\,0$}
        \end{array}
                                \right.
        \end{displaymath}
    Then $f_{0}$ is of class $C^{{\infty}}$ at each point $x$ of ${\RR}$, even at $x \,=\, 0$;
    indeed, at $x \,=\, 0$ one has $f_{0}^{(k)}(0) \,=\, 0$ for all $k$ in ${\NN}$.

        To see why this is the case, let $g:(0,+{\infty}) \,{\rightarrow}\, {\RR}$ be given by the formula $g(x) \,=\, e^{-1/x}$.
    It is easy to see (by Mathematical Induction) that $g$ is of class $C^{{\infty}}$ at each point of the interval $(0,+{\infty})$.
    In fact, each of the derivatives $g^{(k)}(x)$ is the sum of finitely many terms of the form ${\displaystyle \frac{c_{j}}{x^{j}}e^{-1/x}}$ for certain constants $c_{j}$.
    In particular, it follows from repeated use of Part~(a) of the preceding lemma that the one-sided derivatives of $f_{0}$ at $x \,=\, 0$ from the right all exist and equal~$0$.
    And since the same is trivially true for the one-sided derivatives at $0$ from the left,
    the desired result follows from the Concatenation Theorem for Derivatives.

\V

        Following the pattern used in Example~\Ref{ExampE30.85B} and Definition~\Ref{DefE35.110}, one can use the function $f_{0}$ described above to construct $C^{{\infty}}$ `bump functions'.
    We summarize the construction as follows.

\V
             \subsection{\small{\bf Definition}}
            \label{DefE45.127K}\IndB{bump functions}{$C^{{\infty}}$}

\V

\hspace*{\parindent}(1) Define $B^{[{\infty}]}_{[0,1]}:{\RR} \,{\rightarrow}\, {\RR}$ by the rule
        \begin{displaymath}
        B^{[{\infty}]}_{[0,1]}(x)  \,=\, \left\{
        \begin{array}{cl}
        0 & \mbox{if $x\,\,{\leq}\,\,0$ or $x\,\,{\geq}\,\,1$} \\
        f_{0}(x)f_{0}(1-x) & \mbox{if $0\,<\,x\,<\,1$}.
        \end{array}
                           \right.
        \end{displaymath}
    Let $M$ denote the maximum value of this function on the interval $[0,1]$, and set $\hat{B}^{[{\infty}]}_{[0,1]}(x) \,=\, B^{[{\infty}]}_{[0,1]}(x)/M$ for each $x$ in ${\RR}$.
    The function $\hat{B}^{[{\infty}]}_{[0,1]}$ is called the {\bf normalized $C^{{\infty}}$ bump function on $[0,1]$}.

\V

        (2) More generally, if $[a,b]$ is any closed interval in ${\RR}$, then the function $B^{[{\infty}]}_{[a,b]}:{\RR} \,{\rightarrow}\, {\RR}$ given by the rule
        \begin{displaymath}
        B^{[{\infty}]}_{[a,b]}(x) \,=\, B^{[{\infty}]}_{[0,1]}\left(\frac{x-a}{b-a}\right) \mbox{ for all $x$ in ${\RR}$}
        \end{displaymath}
    is called the {\bf normalized $C^{{\infty}}$ bump function on $[a,b]$}.

\V

        \underline{Remark}  These $C^{{\infty}}$ bump functions may appear to be mere curiosities,
    but they actually play an important role in advanced analysis as `test functions' for distributions.
    The topic of `distributions' is outside the scope of {\ThisText}.



%SUGGESTED EXERCISE: e = \lim_{k \,{\rightarrow}\, {\infty}} (1+1/k)^{k}

\VV

        {\bf Properties of the Arcsine Function and Related Functions}

\VV

        As in the case of the function $\ln$, the function `Arcsine' has been introduced into our theory in a purely analytical manner,
    as an antiderivative of an algebraically defined function.
    And as in the case of the logarithm, this function has an inverse function which is of great importance in analysis.
    The basis for all this is the following result.

\V

             \subsection{\small{\bf Theorem}}
            \label{ThmE45.128A}

\V

        \hspace*{\parindent}(a) The function ${\Arcsin}:(-1,1) \,{\rightarrow}\, {\RR}$ is an odd function, and is strictly increasing, on $(-1,1)$.
    Furthermore, its image is a bounded open interval of the form $(-c,c)$, where $c$ is given by $c \,=\, \lim_{x{\nearrow}1} {\Arcsin}\,x$.

\V

        (b) The limit $c$ described in Part~(a) is finite.
    More precisely, it is a real number such that $0\,<\,c\,\,{\leq}\,\,2$.

\V

        (c) Let $g:(-c,c) \,{\rightarrow}\, (-1,1)$ be the inverse function ${\Arcsin}^{-1}$, where the quantity $c$ is described in Part~(a).
    Then $g$ satisfies the following conditions:
        \begin{equation}
        \label{EqnE.128a}
        (i) \h g''(x) \,=\, -g(x) \mbox{ for all $x$ in $(-c,c)$}; \h (ii) \h g(0) \,=\, 0 \mbox{ and } g'(0) \,=\, 1.
        \end{equation}
    In addition, $g$ is also an odd function on~$(-c,c)$.

 

\V

        \underline{Proof} (a) To save writing, let us set $G(x) \,=\, {\Arcsin}\,(x)$ for all $x$ in the interval~$(-1,1)$.

        The fact that $G$ is an odd function on $(-1,1)$ follows easily from the results obtained in Example~\Ref{ExampE45.65} above,
    combined with the obvious fact that the function $f:(-1,1) \,{\rightarrow}\, {\RR}$
    given by the rule $f(x) \,=\, 1/\sqrt{1-x^{2}}$ is even on the interval~$(-1,1)$.

    Next, note that $G'(x)\,>\,0$ for all $x$ in the interval~$(-1,1)$, so by Theorem~\Ref{ThmE40.30}, $G$ is strictly increasing on $(-1,1)$.
    It now follows from Theorem~\Ref{ThmD25.55C} that $G$ is a bijection of $(-1,1)$ onto some open interval $I$ in ${\RR}$.
    Because $G$ is an odd function, it follows that $I$ is an open interval of the form $(-c,c)$,
    where $c \,=\, \lim_{x {\nearrow}1} G(x)$ is either a positive real number or $+{\infty}$, as claimed.

\V

        (b) To estimate the size of the quantity~$c$, observe that if $0\,\,{\leq}\,\,x\,<\,1$, then
        \begin{displaymath}
        \frac{1}{\sqrt{1-x^{2}}} \,=\, \frac{1}{\sqrt{(1+x)\,(1-x)}}\,\,{\leq}\,\,\frac{1}{\sqrt{1-x}} \h ({\ast})
        \end{displaymath}
    It is easy to see that if $H(x) \,=\, -2\,\sqrt{1-x}$, then $H'(x) \,=\, 1/\sqrt{1-x}$ for $0\,\,{\leq}\,\,x\,<\,1$.
    Combine this with Inequality~$({\ast})$ above and Part~(a) of Corollary~\Ref{CorE40.65},
    with the roles of $f$ and $g$ in that corollary being played here by $G$ and~$H$, respectively, to get
        \begin{displaymath}
        G(x) \,=\, G(x) - G(0)\,\,{\leq}\,\,H(x) - H(0) \,=\, 2 - 2\,\sqrt{1-x}\,<\,2 \mbox{ for all $x$ such that $0\,\,{\leq}\,\,x\,<\,1$}.
        \end{displaymath}
    The desired inequality $0\,<\,c\,\,{\leq}\,\,2$ now follows by letting $x$ approach~$1$ from below.

\V

        (c) The fact that $g$ is differentiable on $(-c,c)$ follows from Theorem~\Ref{ThmE40.110}.
    One then computes $g'(y)$ using Equation~\Ref{EqnE.70}: if $g(y) \,=\, x$, so that $y \,=\, {\Arcsin}x$, then
        \begin{displaymath}
        g'(y) \,=\, \frac{1}{{\arcsin}'(x)} \,=\, \frac{1}{(1/\sqrt{1-x^{2}})}
     \,=\, \sqrt{1-x^{2}} \,=\, \sqrt{1-g^{2}(y)} \mbox{ for each $y$ in $(-c, c)$}.
        \end{displaymath}
    From this one gets
        \begin{displaymath}
        g''(y) \,=\, \frac{-g(y)g'(y)}{\sqrt{1-g^{2}(y)}} \,=\, \frac{-g(y)\sqrt{1-g^{2}(y)}}{\sqrt{1-g^{2}(y)}} \,=\, -g(y) \mbox{ for each $y$ in $(-c,c)$},
        \end{displaymath}
    as required. The fact that $g(0) \,=\, 0$ and $g'(0) \,=\, 1$ follows easily. \Q

\VV

             \subsection{\small{\bf Remark}}
            \label{RemrkE45.128AA}

\V

        In elementary calculus we define the trigonometric functions and their inverses geometrically, in terms of angles (using radian measure).
    With that geometric interpretation, the Arcsine function has domain $(-{\pi}/2, {\pi}/2)$,
    which allows us to identify the number $c$ described above with the geometrically defined number~${\pi} \,=\, 3.14159\,{\ldots}\,$.
    However, the reason for introducing this function first is because it can be done rigorously, using antiderivatives, without any reliance on geometry.
    The same holds for the number~${\pi}$: it can now be defined -- without using geometry -- in terms of the number~$c$.

\V

             \subsection{\small{\bf Definition} (The Number ${\pi}$)}
            \label{DefE45.128AAA}\index{pi (${\pi}$)}

        The number ${\pi}$ is given by the formula
        \begin{equation}
        \label{EqnE.84}
        {\pi} \,=\, 2\lim_{x {\nearrow}1} {\arcsin}\,x.
        \end{equation}

\V

        {\bf Remark} With this new notation one can replace the number $c$ in our previous discussion with the expression ${\pi}/2$.
    Thus, one can write
        \begin{displaymath}
        {\Arcsin}:(-1,1) \,{\rightarrow}\, \left(\frac{-{\pi}}{2},\frac{{\pi}}{2}\right),
        \end{displaymath}
    with the


\V

\VV

        It would natural to define the function $g \,=\, {\Arcsin}^{-1}$ described above to be the sine function, but there is a problem:
    the standard sine function one uses in calculus is defined on {\em all} of ${\RR}$,
    while $g$ is defined only on the bounded interval $(-{\pi}/2,{\pi}/2)$.
    This can be fixed by recalling that the usual sine function satisfies certain identities which allow one to extend it from the interval $(-{\pi}/2,{\pi}/2)$ to all other numbers.
    For example, in calculus one uses the fact that ${\sin}\,( \,{\pm}\, {\pi}/2) \,=\,  \,{\pm}\, 1$.
    Likewise, one has the identity ${\sin}\,(x-k{\pi}) \,=\, (-1)^{k}{\sin}\,x$ for each $k$ in ${\ZZ}$.
    These facts lead one to the following definition; basically, it describes the only function which satisfies these required identities and which agrees with $g$ on the interval $(-{\pi}/2,{\pi}/2)$.
    Likewise, it defines the cosine function to be the derivative of the sine function.

\V

             \subsection{\small{\bf Definition}}
            \label{DefE45.128B}

\V

\hspace*{\parindent}(1) The function $g:(-{\pi}/2, {\pi}/2) \,{\rightarrow}\, {\RR}$
    described above is called the {\bf restricted sine function}\IndC{functions}{special functions}{sine function, restricted},
    and is denoted by ${\Sin}:(-{\pi}/2,{\pi}/2) \,{\rightarrow}\, {\RR}$; note the upper-case letter~`S'.

\V

        (2) The {\bf standard sine function}\IndC{functions}{special functions}{sine function, standard}, denoted ${\sin}\,:{\RR} \,{\rightarrow}\, {\RR}$, is given by the following rules:

       \h (i)\,\, ${\sin}\,x \,=\, g(x)$ for $-{\pi}/2\,<\,x\,<\,{\pi}/2$;

       \h (ii)\, if $-{\pi}/2 + k{\pi}\,<\,x\,<\,{\pi}/2 + k{\pi}$, for some $k$ in ${\ZZ}$,
    then ${\sin}\,x \,=\, (-1)^{k}g(x-k{\pi})$.

       \h (iii) ${\sin}\,\left({\pi}/2 - k{\pi}\right) \,=\, (-1)^{k}$ for each $k$ in ${\ZZ}$.

\V

        (3) The {\bf cosine function \IndC{functions}{special functions}{cosine function}},
    denoted ${\cos}:{\RR} \,{\rightarrow}\, {\RR}$, is defined to be ${\sin}'$.

\V

             \subsection{\small{\bf Theorem}}
            \label{ThmE45.128C}

\V

        The functions ${\sin}\,$ and ${\cos}\,$ defined above satisfy the following conditions:

\V

        (a) The function ${\sin}$ is $C^{{\infty}}$ on ${\RR}$, and ${\sin}'' \,=\, -{\sin}\,$. Moreover, ${\sin}\,(0) \,=\, 0$ and ${\sin}'(0) \,=\, 1$.

\V

        (b) The function ${\cos}$ is $C^{{\infty}}$ on ${\RR}$ and satisfies the equations ${\cos}'\,x \,=\, -{\sin}\,x$ and ${\cos}''\,x \,=\, -{\cos}\,x$ for all $x$ in ${\RR}$.
    Moreover, ${\cos}\,(0) \,=\, 1$ and ${\cos}'(0) \,=\, 0$.

\V

        (c) One has ${\sin}\,^{2}(x) + {\cos}\,^{2}(x) \,=\, 1$ for all $x$ in ${\RR}$.

\V

        (d) The solutions of the equation ${\sin}\,x \,=\, 0$ are precisely the numbers of the form $k{\pi}$ with $k$ in~${\ZZ}$.
    Likewise the solutions of the equation ${\cos}\,x \,=\, 0$ are precisely the numbers $(2k-1){\pi}/2$ for $k$ in~${\ZZ}$.

\V

        (e) Suppose $f:(a,b) \,{\rightarrow}\, {\RR}$ is a function, with domain an open interval $(a,b)$, such that $f'' \,=\, -f$ on $(a,b)$.
    Then there exist unique constants $A$ and $B$ such that $f(x) \,=\, A{\cos}\,x + B{\sin}\,x$ for all $x$ in $(a,b)$.

\V


        (f) The sine and cosine functions satisfy the following `Addition Formulas':
        \begin{displaymath}
        {\sin}\,(x_{1} + x_{2}) \,=\, {\sin}\,x_{1}\,{\cos}\,x_{2} + {\cos}\,x_{1}\,{\sin}\,x_{2}
        \end{displaymath}
    and
        \begin{displaymath}
        {\cos}\,(x_{1} + x_{2}) \,=\, {\cos}\,x_{1}\,{\cos}\,x_{2} - {\sin}\,x_{1}\,{\sin}\,x_{2}
        \end{displaymath}
    for all $x_{1}$ and $x_{2}$ in ${\RR}$.

\V

        (g) The sine and cosine functions are periodic with period $2{\pi}$.
    That is,
        \begin{displaymath}
        {\sin}\,(x+2{\pi}) \,=\, {\sin}\,x \mbox{ and } {\cos}\,(x+2{\pi}) \,=\, {\cos}\,x \mbox{ for all $x$ in ${\RR}$}.
        \end{displaymath}

        The proofs of these well-known properties are left as exercises. \Q

\V

        The remaining basic trigonometric functions, namely ${\tan}$, ${\cot}$, ${\sec}$ and ${\csc}$
    can be defined in terms of ${\sin}\,$ and ${\cos}\,$ as in elementary calculus, and their standard properties can then be derived in the usual way from the properties of sine and cosine obtained above.
    Similarly, the various inverse trig functions can be defined in terms of the standard trig functions in the standard manner.
    The details for all this are left to the exercises. Henceforth we shall feel free to use all these standard facts.

\VV

%---------------- H
\StartSkip{

                \section{{\bf Antiderivatives of Higher Order}}
                \label{SectE45B}\IndB{ZZ Sections}{\Ref{SectE45B} Antiderivatives of Higher Order}


\VV

        It follows from Part~(b) of Theorem~\Ref{ThmE45.125B} that if a continuous function $f$ is the derivative of a function $F$ on an open interval $I$,
    then for each $k$ in ${\NN}$ $f$ is the $k$-th derivative of some function $F_{k}$ on $I$.
    This phenomenon suggests the following terminology and notation.

\V

             \subsection{\small{\bf Definition}}
            \label{DefE45.80}\IndB{antiderivatives}{$c$-antiderivatives}
\V

        Suppose that $f:I \,{\rightarrow}\, {\RR}$ is a function defined on an open interval $I$ and that $f$ has an antiderivative on $I$; let $c$ be a point of $I$.
    The unique antiderivative $F$ of $f$ on $I$ such that $F(c) \,=\, 0$ (see Corollary~\Ref{CorE45.60}) is called
    {\bf the first-order $c$-antiderivative of $f$}, and it is denoted $D^{-1}_{c} f$.
    The first-order $c$-antiderivative of the function $D^{-1}_{c} f$, if it exists, is then called {\bf the second-order $c$-antiderivative of $f$},
    and is denoted $D^{-2}_{c} f$.
    The $c$-antiderivatives of $f$ of order $k$, with $k$ in ${\NN}$, are defined similarly, with the obvious notation $D^{-k}_{c} f$.
    The recursive relation is $D^{-(k+1)}_{c} f \,=\, D^{-1}_{c}\left(D^{-k}_{c}f\right)$.


\V

             \subsection{\small{\bf Remarks}}
            \label{RemrkE45.85}

\V

\hspace*{\parindent}(1) It is easy to see that if $k{\in}{\NN}$, a necessary and sufficient condition for $g:I \,{\rightarrow}\, {\RR}$ to equal $D^{-k}_{c} f$
    is that $g^{(k)}(x) \,=\, f(x)$ for all $x$ in $I$ and $g^{(j)}(c) \,=\, 0$ for all $j \,=\, 0,1,2,\,{\ldots}\,k-1$.

\V

        (2) The existence of antiderivatives of higher order in this context follows, at least when $f$ has an antiderivative on $I$,
    from Part~(b) of Theorem~\Ref{ThmE45.125B}, Cauchy's Antiderivative Theorem, combined with the fact that differentiable functions are continuous.


\VV

             \subsection{\small{\bf Examples}}
            \label{exampE45.90}

\V

\hspace*{\parindent}(1) Suppose that $B$ is a constant function on an open interval $I$.
    Then for each $c$ in $I$ one has $\left(D^{-1}_{c} B\right)(x) \,=\, B(x-c)$ for all $x$ in $I$.
    Indeed, the derivative of $F(x) \,=\, B(x-c)$ is the constant $B$, and $F(c) \,=\, B(c-c) \,=\, 0$.

        More generally, it is easy to show (using Mathematical Induction) that for each $k$ in ${\NN}$ one has
        \begin{equation}
        \label{EqnE.85}
        \left(D^{-k}_{c} B\right)(x) \,=\, B\frac{(x-c)^{k}}{k!} \mbox{ if $x{\in}I$}.
        \end{equation}

\V

        (2) Let $f:(-{\infty},0) \,{\rightarrow}\, {\RR}$ be the function given by the rule $f(x) \,=\,{\displaystyle  \frac{1}{x^{3}}}$ \mbox{ if $x\,<\,0$}.
    Let $c \,=\, -2$.
    It is easy to see that $\left(D^{-1}_{-2} f\right)(x) \,=\, {\displaystyle \frac{1}{8}-\frac{1}{2x^{2}}}$ for all $x\,<\,0$.
    To prove this, note that, by the Power Rule for Derivatives, the function ${\displaystyle F(x) \,=\, \frac{1}{8} - \frac{1}{2x^{2}}}$ has as first derivative the given $f$ for $x\,<\,0$; and clearly $F(-2) \,=\, 0$.

        It is also easy to check by the Power Rule that $\left(D^{-2}_{-2} f\right)(x) \,=\, {\displaystyle \frac{1}{2} + \frac{x}{8} + \frac{1}{2x}}$ for all $x\,<\,0$.
    Likewise,
        \begin{displaymath}
        (D^{-3}_{-2} f)(x) \,=\, \frac{x}{2} + \frac{x^{2}}{16} + \frac{1}{2}{\ln}\,(-x) + \frac{3}{4} -\frac{1}{2}{\ln}\,2.
        \end{displaymath}

\V
\V

        The next result summarizes some important properties of the operators of the form $D^{-k}_{c}$.

\V

            \subsection{\small{\bf Theorem}}
            \label{ThmE45.100}

\V

        (a) Suppose that $f$ and $g$ have antiderivatives on the open interval $I$.
    Let $c$ be a point of $I$, and let ${\alpha}$ and ${\beta}$ be constants.
    Then $h \,=\, {\alpha}f+{\beta}g$ also has an antiderivative on $I$, and
        \begin{displaymath}
        D^{-1}_{c} h \,=\, {\alpha}D^{-1}_{c} f + {\beta}D^{-1}_{c} g.
        \end{displaymath}
    More generally, it follows that $f$, $g$ and $h$ have antiderivatives of all orders on $I$ and that
        \begin{displaymath}
        D^{-k}_{c} h \,=\, {\alpha}D^{-k}_{c} f + {\beta}D^{-k}_{c} g \mbox{ for all $k$ in ${\NN}$}.
        \end{displaymath}

\V

        (b) Suppose that $f:I \,{\rightarrow}\, {\RR}$ is differentiable on the open interval $I$, and let $c$ be a point of $I$.
    Then for all $x$ in $I$ one has
        \begin{equation}
        \label{EqnE.86A}
        D^{-1}_{c} f'(x) \,=\, f(x)-f(c).
        \end{equation}

    More generally, suppose that $f:I \,{\rightarrow}\, {\RR}$ is $k$-times differentiable on the open interval $I$ for some $k$ in ${\NN}$.
    Let $c$ be a point of $I$.
    Then for all $x$ in $I$ one has
        \begin{equation}
        \label{EqnE.86B}
        \left(D^{-k}_{c} f^{(k)}\right)(x) \,=\, f(x) - f(c) - f'(c)(x-c) -
    \frac{f''(c)(x-c)^{2}}{2} - \,{\ldots}\,
    - \frac{f^{(k-1)}(c)(x-c)^{k-1}}{(k-1)!}
        \end{equation}

\V

        \underline{Proof}

\V

        (a) Let $F \,=\, D^{-1}_{c} f$, $G \,=\, D^{-1}_{c} g$, and $H \,=\, {\alpha}F+{\beta}G$.
    Because $F' \,=\, f$ and $G' \,=\, g$ (by definition of `antiderivative'), it follows from basic differentiation rules that $H$ is differentiable, and
        \begin{displaymath}
        H' \,=\, ({\alpha}F+{\beta}G)' \,=\, {\alpha}F'+{\beta}G' \,=\, {\alpha}f+{\beta}g \,=\, h.
        \end{displaymath}
    That is, $H$ is an antiderivative of $h$.

        Also, by definition of $D^{-1}_{c}$ one also has $F(c) \,=\, G(c) \,=\, 0$. hence
        \begin{displaymath}
        H(c) \,=\, {\alpha}F(c)+{\beta}G(c) \,=\, 0.
        \end{displaymath}
    Thus,  $H \,=\, D^{-1}_{c} h$, as claimed. The extension to antiderivatives of order $k$ follows by using Mathematical Induction.

\V

        (b) Equation~\Ref{EqnE.86A}, which is the case $k \,=\, 1$ of Equation~\Ref{EqnE.86B} follows directly from Corollary~\Ref{CorE45.60}.

       Suppose next that $k \,=\, 2$.
    Recall that
        \begin{displaymath}
        D^{-2}_{c} f'' \,=\, D^{-1}_{c}\left(D^{-1}_{c} f''\right) \,=\, 
    D^{-1}_{c} \left(f' - f'(c)\right) \,=\, D^{-1}_{c} f' - D^{-1}_{c} f'(c),
        \end{displaymath}
    where the last equation follows from Part~(a).
    By Corollary~\Ref{CorE45.60} one has $D^{-1}_{c} f' \,=\, f-f(c)$.
    Also, by Equation~\Ref{EqnE.85}, with $B \,=\, f'(c)$, one has $D^{-1}_{c} f'(c) \,=\, f'(c)(x-c)$.
    Combining these yields
        \begin{displaymath}
        \left(D^{-2}_{c} f''\right)(x) \,=\, f(x) - f(c) - f'(c)(x-c),
        \end{displaymath}
    which is the content of Equation~\Ref{EqnE.86B} when $k \,=\, 2$.

        The general case follows by using Part~(a) and Equation~\Ref{EqnE.85}, as above, $k-2$ more times. \Q

\V
\V

        Corollary~\Ref{CorE40.65} can be reformulated in terms of antiderivatives.
    To simplify the discussion here, we state this reformulation only for the case $J \,=\, I$ in that corollary;
    it is a simple exercise to state and prove the corresponding version for more general $J$.
    We also provide here the analogous results for higher-order antiderivatives.


            \subsection{\small{\bf Theorem}}
            \label{ThmE45.110}


        Suppose that $f,g:(a,b) \,{\rightarrow}\, {\RR}$ are functions, defined on an open interval $I \,=\, (a,b)$, such that
        \begin{displaymath}
        f(x)\,\,{\leq}\,\,g(x) \mbox{ for all $x$ in $I$}.
        \end{displaymath}
    Let $c$ be a point of $I$.

\V

        (a) Assume that the first antiderivatives $D^{-1}_{c} f$ and $D^{-1}_{c} g$ are defined on $I$.
    Then
        \begin{equation}
        \label{IneqE.87A}
        \left(D^{-1}_{c} f\right)(x)\,\,{\leq}\,\,\left(D^{-1}_{c} g\right)(x)
    \mbox{ for all $x$ in $I$ such that $x\,\,{\geq}\,\,c$},
        \end{equation}
    while
        \begin{equation}
        \label{IneqE.87B}
        -\left(D^{-1}_{c} f\right)(x)\,\,{\leq}\,\,-\left(D^{-1}_{c} g\right)(x)
    \mbox{ for all $x$ in $I$ such that $x\,\,{\leq}\,\,c$}.
        \end{equation}

        More generally, if for some $k$ in ${\NN}$ the higher-order antiderivatives  $D^{-k}_{c} f$ and $D^{-k}_{c} g$ are defined on $I$, then
            \begin{equation}
        \label{IneqE.88A}
        \left(D^{-k}_{c} f\right)(x)\,\,{\leq}\,\,\left(D^{-k}_{c} g\right)(x)
    \mbox{ for all $x$ in $I$ such that $x\,\,{\geq}\,\,c$},
        \end{equation}
    while
        \begin{equation}
        \label{IneqE.88B}
        (-1)^{k}\left(D^{-k}_{c} f\right)(x)\,\,{\leq}\,\,(-1)^{k}\left(D^{-k}_{c} g\right)(x)
    \mbox{ for all $x$ in $I$ such that $x\,\,{\leq}\,\,c$}
        \end{equation}

\V

        (b) Suppose that for some $x$ in $I$, with $x \,\,{\neq}\,\, c$, one gets equality in any of the inequalities obtained in Part~(a).
    Then $f(x) \,=\, g(x)$ for all $x$ in $\mbox{Seg}\,[x,c]$.

\V

        \underline{Proof} 

        (a) The proof is by induction on $k$. Let $A$ be the set of all $k$ in ${\NN}$ for which the statement is correct.

        \underline{Initial Step} \underline{Case 1} Suppose that $x$ in $I$ satisfies $x\,\,{\geq}\,\,c$. Apply Part~(a) of Corollary~\Ref{CorE40.65}, with $x_{1} \,=\, c$ and $x_{2} \,=\, x$, to obtain
        \begin{displaymath}
        \left(D^{-1}_{c} f\right)(x) - \left(D^{-1}_{c} f\right)(c)\,\,{\leq}\,\,
    \left(D^{-1}_{c} g\right)(x) - \left(D^{-1}_{c} g\right)(c);
        \end{displaymath}
    but since $\left(D^{-1}_{c} f\right)(c) \,=\, \left(D^{-1}_{c} g\right)(c) \,=\, 0$,
    one gets Inequality~\Ref{IneqE.87A}.

        \underline{Case 2} Suppose that $x$ in $I$ satisfies $x\,\,{\leq}\,\,c$.
    Apply Part~(a) of the same corollary again, but this time with $x_{1} \,=\, x$ and $x_{2} \,=\, c$, to get
        \begin{displaymath}
        \left(D^{-1}_{c} f\right)(c) - \left(D^{-1}_{c} f\right)(x)\,\,{\leq}\,\,
    \left(D^{-1}_{c} g\right)(c) - \left(D^{-1}_{c} g\right)(x);
        \end{displaymath}
    that is,
        \begin{displaymath}
        - \left(D^{-1}_{c} f\right)(x)\,\,{\leq}\,\,
        - \left(D^{-1}_{c} g\right)(x),
        \end{displaymath}
    which is Inequality~\Ref{IneqE.87B}.

        It follows that $1{\in}A$.

\V

        \underline{Inductive Step} Suppose that $k{\in}A$.

        \underline{Case 1} Suppose $x\,\,{\geq}\,\,c$. By the induction hypothesis (i.e., $k{\in}A$), one has
        \begin{displaymath}
        \left(D^{-k}_{c} f\right)(x)\,\,{\leq}\,\,\left(D^{-k}_{c} g\right)(x)
        \end{displaymath}
    for all such $x$.
    Apply Part~(a) of Corollary~\Ref{CorE40.65} to the functions $D^{-k}_{c} f$ and $D^{-k}_{c} g$, with $x_{1} \,=\, x$ and $x_{2} \,=\, c$, to get
        \begin{displaymath}
        \left(D^{-(k+1)}_{c} f\right)(x) - \left(D^{-(k+1)}_{c} f\right)(c) 
    \,\,{\leq}\,\,
        \left(D^{-(k+1)}_{c} g\right)(x) - \left(D^{-(k+1)}_{c} g\right)(c) 
        \end{displaymath}
    That is,
        \begin{displaymath}
        \left(D^{-(k+1)}_{c} f\right)(x) 
    \,\,{\leq}\,\,
        \left(D^{-(k+1)}_{c} g\right)(x). 
        \end{displaymath}

        \underline{Case 2} Suppose $x\,\,{\leq}\,\,c$. By the induction hypothesis, one has
        \begin{displaymath}
        (-1)^{k}\left(D^{-k}_{c} f\right)(x)\,\,{\leq}\,\,(-1)^{k}\left(D^{-k}_{c} g\right)(x)
        \end{displaymath}
    for all such $x$.
    Apply Part~(a) of Corollary~\Ref{CorE40.65} again, but this time to the functions $(-1)^{k}D^{-k}_{c} f$ and $(-1)^{k}D^{-k}_{c} g$ with $x_{1} \,=\, c$ and $x_{2} \,=\, x$, to get
        \begin{displaymath}
        (-1)^{k}\left(\left(D^{-(k+1)}_{c} f\right)(c) - \left(D^{-(k+1)}_{c} f\right)(x)\right) 
    \,\,{\leq}\,\,
        (-1)^{k}\left(\left(D^{-(k+1)}_{c} g\right)(c) - \left(D^{-(k+1)}_{c} g\right)(x)\right) 
        \end{displaymath}
    That is,
        \begin{displaymath}
        (-1)^{k+1}\left(D^{-(k+1)}_{c} f\right)(x)
        \,\,{\leq}\,\,
        (-1)^{k+1}\left(D^{-(k+1)}_{c} g\right)(x)
        \end{displaymath}
    (Note that in both cases the fact that $\left(D^{-(k+1)}_{c} f\right)(c) \,=\, \left(D^{-(k+1)}_{c} g\right)(c) \,=\, 0$ was used.)

        It is now clear that $(k+1)$ is also in $A$, so that the Principle of Mathematical Induction can be used to conclude that $A \,=\, {\NN}$ and thus the stated theorem is correct.

\V

        (b) The proof in this part uses Part~(b) of Corollary~\Ref{CorE40.65} and mathematical induction in much the same way as in Part~(a).
    The details are left to the reader. \Q

\VV

}%\EndSkip
%--------------------------- H

%----------------------------------------------- J
\StartSkip{
                \section{{\bf Taylor's Formula with Remainder}}
                \label{SectE60}\IndB{ZZ Sections}{\Ref{SectE60} Taylor's Formula with Remainder}


\VV

        One of the earliest applications of derivatives to analysis is in the use of derivatives to approximate difficult-to-compute functions by polynomials.
    The next result motivates the construction of these polynomials.

\V


            \subsection{\small{\bf Examples}}
            \label{ExampE60.10}

\V

        Let $I$ be an open interval in~${\RR}$, and that $c$ is a point of the interval~$I$.
    Suppose that $f:I \,{\rightarrow}\, {\RR}$ is a smooth (i.e., $C^{{\infty}}$) function on~$I$.

        Recall that in ordinary language to say that two real numbers $A$ and $B$ are {\bf approximately equal}, written $A \,{\approx}\, B$,
    means that the nonnegative quantity $|B-A|$ is small. For example, in calculus one often interprets an equation such as $\lim_{x \,{\rightarrow}\, c} f(x) \,=\, f(c)$ to mean,
    speaking intuitively, that when $|x-c|$ is small, then $|f(x)-f(c)|$ is also small; that is, $f(x) \,{\approx}\, f(c)$ when $x \,{\approx}\, c$.
    Of course, vague statements like these are not precise enough for a rigorous treatment of analysis, but they do guide one's intuition.
    Since the purpose of this set of examples is to lead one to a fairly complicated definition below, the use of `approximate equality' is appropriate.

\V

        (1) From the basic properties of `continuity on an interval', one has
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\, c} f(x) \,=\, f(c).
        \end{displaymath}
    Hence, as noted above, one can write
        \begin{displaymath}
        f(x) \,{\approx}\, f(c) \mbox{ when $x \,{\approx}\, c$}.
        \end{displaymath}
    The error in this last approximation, i.e., the amount by which $f(c)$ fails to equal~$f(x)$, is the quantity $E_{0}(x) \,=\, f(x)-f(c)$.
    This last equation can be written $f(x) \,=\, f(c) + E_{0}(x)$. In the next example we improve this initial approximation of $f(x)$
    by using the derivative to estimate the error~$E_{0}(x)$.

\V

        (2)  One can use the first derivative of~$f$ to estimate the error $E_{0}(x)$ just obtained, at least when $x \,{\approx}\, c$.
    Indeed, from the definition of `derivative', together with the obvious fact that $E_{0}(c) \,=\, 0$, one has $E_{0}(x) \,=\, E_{0}(x) - E_{0}(c)$, so that
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\, c} \frac{E_{0}(x)}{x-c} \,=\, \lim_{x \,{\rightarrow}\, c} \frac{f(x)-f(c)}{x-c} \,=\, f'(c).
        \end{displaymath}
    This leads to a simple estimate for $E_{0}(x)$:
        \begin{displaymath}
        \frac{E_{0}(x)}{x-c} \,=\, \frac{f(x)-f(c)}{x-c}  \,{\approx}\, f'(c) \mbox{ and thus }
        E_{0}(x) \,{\approx}\, f'(c)\,(x-c)  \mbox{ when $x \,{\approx}\, c$}.
        \end{displaymath}
    Replace $E_{0}(x)$ in the equation $f(x) \,=\, f(c) + E_{0}(x)$ by the estimate just obtained
    to obtain a new (and, presumably, improved) approximation of~$f(x)$:
        \begin{displaymath}
        f(x) \,{\approx}\, f(c) + f'(c)\,(x-c)
        \end{displaymath}
    The error in this new approximation is $E_{1}(x) \,=\, f(x) - (f(c) + f'(c)\,(x-c))$, so that $f(x) \,=\, f(c) + f'(c)\,(x-c) + E_{1}(x)$.
    Note that $E_{1}(c) \,=\, E_{1}'(c) \,=\, 0$.

\V

        (3) In a like manner, one can use the {\em second} derivative of $f$ to estimate the error $E_{1}$ just obtained.
    Indeed,
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\, c} \frac{E_{1}(x)}{(x-c)^{2}}
     \,=\, 
        \lim_{x \,{\rightarrow}\, c} \frac{f(x) - (f(c) + f'(c)\,(x-c))}{(x-c)^{2}}
     \,=\, 
        \frac{f''(c)}{2},
        \end{displaymath}
    where the final equation comes from using L'H\^{o}pital's Rule twice. In turn, the last equation can be written as an approximation:
        \begin{displaymath}
        f(x) \,{\approx}\, f(c) + f'(c)\,(x-c) + f''(c)\,\frac{(x-c)^{2}}{2} \mbox{ when $x \,{\approx}\, c$}.
        \end{displaymath}
    The error in this approximation is $E_{2}(x) \,=\, {\displaystyle f(x) - \left(f(c) + f'(c)\,(x-c) + f''(c)\,\frac{(x-c)^{2}}{2}\right)}$.
    Note that $E_{2}(c) \,=\, E_{2}'(c) \,=\, E_{2}''(c) \,=\, 0$.

\V

        (4) In a similar way one can estimate the error $E_{2}(x)$ just obtained when $x \,{\approx}\, c$.
    Indeed, one has, by repeated use of L'H\^{o}pital's Rule,
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\, c} \frac{E_{2}(x)}{(x-c)^{3}}
     \,=\, 
        \lim_{x \,{\rightarrow}\, c} f(x) - \left(f(c) + f'(c)\,(x-c) + {\displaystyle f''(c)\,\frac{(x-c)^{2}}{2}}\right)
     \,=\, 
        \frac{f'''(c)}{3{\cdot}2}.
        \end{displaymath}
    Thus one gets $E_{2}(x) \,{\approx}\, {\displaystyle f'''(c)\,\frac{(x-c)^{3}}{2{\cdot}3}}$.

\V

        (5) Continuing this way, one sees a pattern setting up. More precisely, for each $k$ in~${\NN}$ one is led to the approximation
        \begin{displaymath}
        f(x) \,{\approx}\, f(c) + f'(c)\,(x-c) + f''(c)\,\frac{(x-c)^{2}}{2} + f'''(c)\,\frac{(x-c)^{3}}{3{\cdot}2} + \,{\ldots}\, + f^{(k)}(c)\,\frac{(x-c)^{k}}{k!}
        \end{displaymath}
    The error $E_{k}(x)$ for this approximation is given by
        \begin{displaymath}
        E_{k}(x) \,=\, f(x) - \left(f'(c)\,(x-c) + f''(c)\,\frac{(x-c)^{2}}{2} + f'''(c)\,\frac{(x-c)^{3}}{3{\cdot}2} + \,{\ldots}\, + f^{(k)}\,\frac{(x-c)^{k}}{k\,!}\right).
        \end{displaymath}
    By more applications of L'H\^{o}pital's Rule, one also gets
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\, c} \frac{E_{k}(x)}{(x-c)^{k+1}}
     \,=\, 
        \frac{f^{(k+1)}(c)}{(k+1)!}, \mbox{ hence } E_{k}(x) \,{\approx}\, f^{(k+1)}(c)\frac{(x-c)^{k+1}}{(k+1)!} \mbox{ when $x \,{\approx}\, c$}.
        \end{displaymath}

\VV

        The preceding examples suggest that the function $f$
    can be approximated near $c$ by certain polynomials whose coefficients involve the derivatives of $f$ at~$c$.
    The next definition provides the standard names of these polynomials. Note that the hypothesis used above,
    that $f$ be $C^{{\infty}}$, which was included to simplify the treatment, is weakened below.

\V

            \subsection{\small{\bf Definition} (Taylor Polynomials; the Taylor Approximation)}
            \label{DefE60.20}

        Let $f:I \,{\rightarrow}\, {\RR}$ be a $k$-times differentiable function on an open interval $I$, where $k{\in}{\NN}$.

\V

        (1) If $c$ is a point of $I$ then the {\bf Taylor Polynomial of order $k$ for $f$ about the center point $\Bfm{c}$}\IndB{functions}{Taylor polynomials}
    is the polynomial $p_{k}:{\RR} \,{\rightarrow}\, {\RR}$ given by the formula
        \begin{equation}
        \label{EqnE.120A}
        p_{k}(x) \,=\, f(c) + f'(c)(x-c) + \frac{f''(c)(x-c)^{2}}{2} + \frac{f^{(3)}(c)(x-c)^{3}}{3!} + \,{\ldots}\,+ \frac{f^{(k)}(c)(x-c)^{k}}{k!}
        \end{equation}

\V

        (2) The statement `$f(x) \,{\approx}\, p_{k}(x) \mbox{ for $x$ in $I$ near $c$}$'
    is called the {\bf Taylor Approximation of $f$ of order $k$ for $f$ near $c$}.
    The difference $E_{k}(x) \,=\, f(x) - p_{k}(x)$ is called the {\bf error} of this approximation.

\V

        (3) In the important special case in which the center point $c$ is the origin $0$, many authors use the name `Maclaurin' in place of the name `Taylor',
    and the explicit reference to $c$ is dropped. Thus the {\bf Maclaurin polynomial of order $k$ for $f$}\IndB{functions}{Maclaurin polynomials}
    is the polynomial one obtains from Equation~\Ref{EqnE.120A} by replacing $c$ with $0$ and $(x-c)$ with $x$; in other words, the polynomial
        \begin{equation}
        \label{EqnE.120B}
        p_{k}(x) \,=\, f(0) + f'(0)x + \frac{f''(0)x^{2}}{2} + \frac{f^{(3)}(0)x^{3}}{3!} + \,{\ldots}\,+ \frac{f^{(k)}(0)x^{k}}{k!}
        \end{equation}
    In {\ThisText} the `Maclaurin' terminology will normally not be used, since `Taylor polynomial about the center point $c \,=\, 0$'
    is more directly descriptive.

        \underline{Note} Brook Taylor and Colin Maclaurin were British mathematicians who were active in the early eighteenth century.

\VV

            \subsection{\small{\bf Remarks}}
            \label{RemrkE60.25}

\V


\hspace*{\parindent}(1) At times one may wish to compare the Taylor polynomials for a given function $f$ about two different choices of centers, $c_{1}$ and $c_{2}$;
    or one may wish to compare the Taylor polynomials about $c$ for two different functions $f$ and $g$.
    If no simpler notational alternatives are available, one can always resort to a more descriptive notation such as
    $p[f;c]_{k}(x)$ as a symbol for the expression on the right side of Equation~\Ref{EqnE.120A}.

\V

        (2) It is an easy consequence of the definition of the Taylor polynomials that one has $f^{(j)}(c) \,=\, p_{k}^{(j)}(c)$ for all $j \,=\, 0,1,\,{\ldots}\,k$.
    In addition, since $p_{k}$ is a polynomial of degree $k$ or less, it is clear that $p_{k}^{(k+1)}(x) \,=\, 0$ for all $x$

\V

        (3) Many authors refer to the polynomial $p_{k}$ described above as the Taylor polynomial of {\em degree} $k$ for $f$, instead of {\em order $k$}.
    It is a subtle difference; but the `order' terminology is preferable, since if $f^{(k)}(c)$ happens to equal $0$, then the degree of the polynomial $p_{k}$ is actually less than $k$.
    In other words, the `order' terminology refers to the index $k$ in the notation $p_{k}(x)$, not to the degree of the polynomial $p_{k}$.

\VV

            \subsection{\small{\bf Simple Examples}}
            \label{ExampE60.30}

\V

\hspace*{\parindent}(1) Let $f:{\RR} \,{\rightarrow}\, {\RR}$ be given by the rule $f(x) \,=\, x^{4}$ for all $x$.
    One computes
        \begin{displaymath}
        f(x) \,=\, x^{4}; \h f'(x) \,=\, 4x^{3}; \h f''(x) \,=\, 12x^{2}; \h f^{(3)}(x) \,=\, 24x; \h f^{(4)}(x) \,=\, 24; \mbox{ and}
        \end{displaymath}
        \begin{displaymath}
         f^{(k)}(x) \,=\, 0
\mbox{ for all $k$ in ${\NN}$ with $k\,\,{\geq}\,\,5$}
        \end{displaymath}

        Here are the Taylor polynomials for $f$ about two different choices of center point $c$.

        \underline{The Case $c \,=\, 0$}: One has $f(0) \,=\, f'(0) \,=\, f''(0) \,=\, f^{(3)}(0) \,=\, 0$, $f^{(4)}(0) \,=\, 24$, $f^{(k)}(0) \,=\, 0$ for $k\,\,{\geq}\,\,5$.
    Thus the corresponding Taylor polynomials are
        \begin{displaymath}
        p_{1}(x) \,=\, 0; \h p_{1}(x) \,=\, 0; \h p_{2}(x) \,=\, 0; \h p_{3}(x) \,=\, 0 \h p_{k}(x) \,=\, x^{4} \mbox{ for all $k\,\,{\geq}\,\,4$}.
        \end{displaymath}

        \underline{The Case $c \,=\, -1$}: In this case one has $f(-1) \,=\, 1$, $f'(-1) \,=\, -4$, $f''(-1) \,=\, 12$,
    $f^{(3)}(-1) \,=\, -24$, $f^{(4)}(-1) \,=\, 24$, and $f^{(k)}(-1) \,=\, 0$ for all $k\,\,{\geq}\,\,5$.
    Here are the corresponding Taylor polynomials:
        \begin{displaymath}
        p_{0}(x) \,=\, 1; \h p_{1}(x) \,=\, 1-4(x+1); \h p_{2}(x) \,=\, 1-4(x+1) + 6(x+1)^{2};
        \end{displaymath}
        \begin{displaymath}
        p_{3}(x) \,=\, 1-4(x+1) + 6(x+1)^{2} - 4(x+1)^{3}; \h p_{k}(x) \,=\, 1-4(x+1) + 6(x+1)^{2} - 4(x+1)^{3} + (x+1)^{4} \mbox{ for all $k\,\,{\geq}\,\,4$}.
        \end{displaymath}

\V

        (2) Let $f \,=\, {\exp}$ and $c \,=\, 0$. Since $f'(x) \,=\, f(x)$ for all $x$, one has $f^{(k)}(x) \,=\, f(x)$ for all $x$.
    Thus $f^{(k)}(0) \,=\, f(0) \,=\, 1$ for all $k \,=\, 0,1,2,\,{\ldots}\,$.
    Thus, the corresponding Taylor polynomials are
        \begin{displaymath}
        p_{k}(x) \,=\, 1 + x + \frac{x^{2}}{2} + \frac{x^{3}}{3!} + \,{\ldots}\, + \frac{x^{k}}{k!} \mbox{ for each $k \,=\, 0,1,2,\,{\ldots}\,$}.
        \end{displaymath}

\V

        (3) Let $f \,=\, {\sin}\,$ and $c \,=\, 0$.
    Then one computes that $f' \,=\, {\cos}\,$, $f'' \,=\, -{\sin}\,$, $f''' \,=\, -{\cos}\,$, $f^{(4)} \,=\, {\sin}\,$, and so on.
    More generally,
        \begin{displaymath}
        f^{(2k-2)} \,=\, (-1)^{k-1}{\sin}\,\, \h f^{(2k-1)} \,=\, (-1)^{k-1}{\cos}\, \mbox{ for each $k \,=\, 1,2,\,{\ldots}\,$}.
        \end{displaymath}
    Then from the usual properties of sine and cosine, one gets
        \begin{displaymath}
        f(0) \,=\, 0, \h f'(0) \,=\, 1, \h f''(0) \,=\, 0, \h f^{(3)}(0) \,=\, -1, \h f^{(4)}(0) \,=\, 0,
        \end{displaymath}
    and so on. The general rule is
        \begin{displaymath}
        f^{(2k-2)}(0) \,=\, 0, f^{(2k-1)}(0) \,=\, (-1)^{k-1} \mbox{ for $k \,=\, 1,2,\,{\ldots}\,$}.
        \end{displaymath}
    The corresponding Taylor polynomials are
        \begin{displaymath}
        p_{0}(x) \,=\, 0; \h p_{1}(x) \,=\, p_{2}(x) \,=\, x; \h p_{3}(x) \,=\, p_{4}(x) \,=\, x - \frac{x^{3}}{3!}; \,{\ldots}\,
        \end{displaymath}
    The general rule is
        \begin{displaymath}
        p_{2k-1}(x) \,=\, p_{2k}(x) \,=\, x - \frac{x^{3}}{3!} + \frac{x^{5}}{5!} - \frac{x^{7}}{7!} + \,{\ldots}\,+ (-1)^{k-1}\frac{x^{2k-1}}{(2k-1)!}
        \end{displaymath}

\VV

        The next theorem summarizes the basic conclusions of Example~\Ref{ExampE60.10} using the `Taylor polynomial' terminology.
    The proof is omitted here since it reduces to repeating the calculations carried out in that example.

\V

            \subsection{\small{\bf Theorem}}
            \label{ThmE60.35}
\V


        Let $f:I \,{\rightarrow}\, {\RR}$ be a $C^{k}$ function on an open interval $I$, where $k{\in}{\NN}$.
    Suppose that $c$ is a point of $I$, and for each $j \,=\, 0,1,\,{\ldots}\,k$ let $p_{j}$
    denote the Taylor polynomial of order $j$ for $f$ about the center point~$c$.

\V


        (a) One has
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\, c} \frac{f(x)-p_{k}(x)}{(x-c)^{k}} \,=\, 0.
        \end{displaymath}

\V

        (b) One also has
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\, c} \frac{f(x)-p_{k}(x)}{(x-c)^{k+1}} \,=\, \frac{f^{(k+1)}(c)}{(k+1)!}.
        \end{displaymath}

\V

        {\bf Remark} A more precise statement of the error in the Taylor approximation is given later (`Taylor's Formula with Remainder').

%---------------- Start 2J Material --------

\VV


        The Taylor polynomials of a function $f$ are officially defined in terms of the derivatives of $f$; see Equation~\Ref{EqnE.120A} above.
    Thus, it may appear that the only way to compute these polynomials is to carry out the indicated repeated differentiations on the function~$f$.
    However, there are alternate ways of characterizing the Taylor polynomials which sometimes are easier to use than the `official' definition;
    indeed, some of these alternate ways do not involve the calculation of any derivatives at all.
    The next result lists several such characterizations.

\V

            \subsection{\small{\bf Theorem}}
            \label{ThmE60.40}
\V

        Let $f:I \,{\rightarrow}\, {\RR}$ be a $k$-times differentiable function on an open interval $I$, where $k{\in}{\NN}$.
    Suppose that $c$ is a point of $I$, and let $p_{k}$ denote the Taylor polynomial of order $k$ for $f$ about the center point~$c$.
    Then:

        (a) The Taylor polynomial $p_{k}(x)$ is a polynomial, of degree $k$ or less, such that $p_{k}^{(j)}(c) \,=\, f^{(j)}(c)$ for all $j \,=\, 0,1,2, \,{\ldots}\, k$.

\V

        (b) Conversely, suppose that $q(x)$ is a polynomial, of degree $k$ or less, such that $q^{(j)}(c) \,=\, f^{(j)}(c)$ for all $j \,=\, 0,1,2, \,{\ldots}\, k$.
    Then $q(x) \,=\, p_{k}(x)$.

\V

        (c) Suppose that $q(x)$ is a polynomial, of degree $k$ or less, such that ${\displaystyle \lim_{x {\rightarrow} c} \frac{f(x)-q(x)}{(x-c)^{k}} \,=\, 0}$.
    Then $q(x) \,=\, p_{k}(x)$.

\V

        (d) Suppose that $q(x)$ is a polynomial of the form $q(x) \,=\, b_{0}+b_{1}\,(x-c)+ \,{\ldots}\, +b_{k}\,(x-c)^{k}$.
    Then $q(x)$ equals the Taylor polynomial $p_{k}(x)$ of $f(x)$ about the center point $c$
    if, and only if, $b_{j} \,=\, f^{(j)}(c)/n!$ for $j \,=\, 0,1, \,{\ldots}\, k$.

\V

        {\bf Proof}\,

\V

        (a) and (b) These simple proofs are left as exercises.
\V

        (c) Note that
        \begin{displaymath}
        \frac{f(x)-q(x)}{(x-c)^{k}} \,=\, \frac{f(x)-p_{k}(x)}{(x-c)^{k}} +
        \frac{p_{k}(x) - q(x)}{(x-c)^{k}} \h ({\ast})
        \end{displaymath}
    By hypothesis, the fraction on the left side of Equation~$({\ast})$ approaches $0$ as $x$ does.
    By Theorem~\Ref{ThmE60.35}, the first fraction on the right side of this equation also approaches~$0$ with~$x$.
    Thus the same must be true for the second fraction on the right. That is, one has
        \begin{displaymath}
        \lim_{x \,{\rightarrow}\, c} \frac{p_{k}(x)-q(x)}{(x-c)^{k}} \,=\, 0
        \end{displaymath}
    However, $p_{k}(x)-q(x)$ is a polynomial of degree $k$ or less, and it is easy to see that the only way this last limit fact can hold is if $p_{k}(x)-q(x) \,=\, 0$ for all $x$.
    Thus, $q(x) \,=\, p(x)$, as claimed.

\V

        Part~(d) is a special case of the following well-known result from high-school algebra:

        `A necessary and sufficient condition for the polynomials $a_{0} + a_{1}(x-c) +  \,{\ldots}\,  + a_{k}(x-c)^{k}$ and $b_{0} + b_{1}(x-c)+ \,{\ldots}\, +b_{k}(x-c)^{k}$ to be equal is that $a_{j} \,=\, b_{j}$ for $j \,=\, 0,1, \,{\ldots}\, k$.'

\noindent This is usually proved, without using calculus, by induction on~$k$; the details are left as an exercise.

\V

            \subsection{\small{\bf Corollary}}
            \label{CorE60.50}

        Suppose that $f$ is itself a polynomial of degree $k$ or less.
    Let $c$ be any number, and let $p[f;c]_{k}$ denote the Taylor polynomial of order $k$ for $f$ centered at~$c$.
    Then $p[f;c]_{k}(x) \,=\, f(x)$, independent of the choice of center point $c$.

\V

    \underline{Proof} Apply Part~(c) of the preceding theorem with $q \,=\, f$.

\V

        \underline{Warning} The preceding corollary does {\em not} say that $p[f;c]_{j}(x)$ is independent of $c$ when $j\,<\,k$; see Example~(1) below.

\VV


            \subsection{\small{\bf More-Complicated Examples}}
            \label{ExampE60.60}

\hspace*{\parindent} 
        (1) Suppose that $f(x) \,=\, 2x^{2}-7x+5$ for all $x$ in ${\RR}$.
    Let us compute $p[f;c]_{j}(x)$ for orders $j \,=\, 0, 1, 2$ and two different center points, namely $c \,=\, 0$ and $c \,=\, 2$.
    To do this, note that
        \begin{displaymath}
        f(x) \,=\, 2x^{2}-7x+5; \h f'(x) \,=\, 4x-7; \h f''(x) \,=\, 4
        \end{displaymath}
    Thus,
        \begin{displaymath}
        f(0) \,=\, 5; \h f'(0) \,=\, -7; \h \frac{f''(0)}{2} \,=\, 2
        \end{displaymath}
    From this one gets
        \begin{displaymath}
        p[f;0]_{0}(x) \,=\, 5; \h p[f;0]_{1}(x) \,=\, 5 -7(x-0) \,=\, 5-7x;
        \end{displaymath}
        \begin{displaymath}
     p[f;0]_{2}(x) \,=\, 5 -7(x-0) + 2(x-0)^{2} \,=\, 5-7x+2x^{2} \,=\, f(x).
        \end{displaymath}
    Note that the `initial' Taylor polynomials $p[f;0]_{0}$ and $p[f;0]_{1}$ about the center point $c \,=\, 0$
    are given by the same formulas as the constant and linear terms of the of the expression $5-7x+2x^{2}$,
    which is simply the original formula for $f(x)$, but starting with the lowest powers of $x$.

        Similarly, one computes
        \begin{displaymath}
        f(2) \,=\, -1; \h f'(2) \,=\, 1; \h \frac{f''(2)}{2} \,=\, 2.
        \end{displaymath}
    Thus,
        \begin{displaymath}
        p[f;2]_{0}(x) \,=\, -1; \h p[f;2]_{1}(x) \,=\, -1 + 1{\cdot}(x-2) \,=\, x-3;
        \end{displaymath}
        \begin{displaymath}
        p[f;2]_{2}(x) \,=\, -1 + 1{\cdot}(x-2) + 2(x-2)^{2} \,=\, -1 + x -2 +2x^{2}-8x+8 \,=\, 2x^{2} - 7x + 5 \,=\, f(x).
        \end{displaymath}

        The fact that $p[f;0]_{2}(x) \,=\, p[f;2]_{2}(x)$ for all $x$ illustrates the conclusions of Corollary~\Ref{CorE60.50}.
    In contrast, the fact that $5-7x \,\,{\neq}\,\, x-3$, i.e., $p[f;0]_{1}(x) \,\,{\neq}\,\, p[f;2]_{1}(x)$,
    illustrates the warning  which follows the proof of that corollary.

\V

        (2) Here is an alternate way to analyse the quadratic function $f(x) \,=\, 2x^{2}-7x+5$ studied in the previous example.

        First note that if one sets $x \,=\, 2+(x-2)$, then $x^{2} \,=\, (2+(x-2))^{2} \,=\, 4+4(x-2)+(x-2)^{2}$, so that
        \begin{displaymath}
        f(x) \,=\, 5-7x+2x^{2} \,=\, 5-7(2+(x-2))+2(4+4(x-2)+(x-2)^{2}) \,=\, 
        \end{displaymath}
        \begin{displaymath}
    (5-14+8) + (-7+8)(x-2) + 2(x-2)^{2} \,=\, -1 + 1{\cdot}(x-2) + 2(x-2)^{2}.
        \end{displaymath}
    Note that the polynomial on the right is the same as the Taylor polynomial $p[f;2]_{2}(x)$ obtained in Example~(1) above.

        More generally, suppose that $c$ is any choice of center point.
    Note that
        \begin{displaymath}
        x \,=\, c+(x-c) \mbox{ and } x^{2} \,=\, (c+(x-c))^{2} \,=\, c^{2} + 2c(x-c) + (x-c)^{2}.
        \end{displaymath}
    Thus
        \begin{displaymath}
        f(x) \,=\, 5-7(c+(x-c)) + 2(c^{2} + 2c(x-c) + (x-c)^{2}) \,=\, 
    (5-7c+2c^{2}) + (-7+4c)(x-c) + 2(x-c)^{2}
        \end{displaymath}
    Since we already know (from the corollary) that $p_{2}[f;c] \,=\, f$, it follows from the calculation just carried out that
        \begin{displaymath}
        p[f;c]_{2}(x) \,=\, (5-7c+2c^{2}) + (-7+4c)(x-c) + 2(x-c)^{2}.
        \end{displaymath}
    By Part~(d) of Theorem~\Ref{ThmE60.40} it then follows that
        \begin{displaymath}
        5-7c+2c^{2} \,=\, f(c); \h -7+4c \,=\, f'(c); \h
    2 \,=\, \frac{f''(c)}{2}.
        \end{displaymath}
    In other words, we were able to determine the Taylor polynomials of $f$ around any center $c$ using only algebra (i.e., the $x \,=\, c+(x-c)$ trick), 
    and without taking any derivatives. Indeed, we can compute the derivatives of $f$ at the
    center point $c$ without actually computing the derived functions $f'(x)$ or $f''(x)$.

\V
        

        (3) It is shown in Theorem~\Ref{ThmB25.80}  -- long before the concept of `derivative' was introduced -- that if $x$ is any real number such that $x \,\,{\neq}\,\, 1$, then
        \begin{equation}
        \label{EqnE.130}
        1+x+x^{2}+x^{3}+ \,{\ldots}\, +x^{k} \,=\, (1-x^{k+1})/(1-x) \,=\, 
        \frac{1}{1-x} - \frac{x^{k+1}}{1-x}
        \end{equation}
    Now let $f(x) \,=\, 1/(1-x)$ for all $x\,{\neq}\,1$, and let $c \,=\, 0$.
    Then it follows, without any differentiation, that $p_{k}(x) \,=\, 1+x+x^{2}+ \,{\ldots}\, +x^{k}$.
    Indeed, let $q(x) \,=\, 1+x+x^{2}+ \,{\ldots}\, +x^{k}$.
    Clearly $q(x)$ is a polynomial of degree $k$.
    Also, Equation~\Ref{EqnE.130} implies that
        \begin{displaymath}
        q(x) \,=\, f(x) - \frac{x^{k+1}}{1-x},
        \end{displaymath}
    hence
        \begin{displaymath}
        \lim_{x {\rightarrow} 0} \frac{f(x)-q(x)}{x^{k}} \,=\, \lim_{x {\rightarrow} 0} \frac{x^{k+1}}{(1-x)x^{k}} \,=\, \lim_{x {\rightarrow} 0} \frac{x}{1-x} \,=\, 0.
        \end{displaymath}
    Thus, Part~(c) of Theorem~\Ref{ThmE60.40} implies that $q(x) \,=\, p_{k}(x)$.
    In addition, by using Part~(d) of the same theorem, one sees, also without any differentiation, that $f^{(k)}(0)/k! \,=\, 1$, so $f^{(k)}(0) \,=\, k!$ when $f(x) \,=\, 1/(1-x)$.


\V

        (4) Let $g:{\RR} \,{\rightarrow}\, {\RR}$ be the function given by the formula ${\displaystyle g(u) \,=\, \frac{1}{1+u^{2}}}$ for all $u$ in ${\RR}$.
    If one replace $x$ by $-u^{2}$ in Equation~\Ref{EqnE.130}, then one gets
        \begin{equation}
        \label{EqnE.140}
        1-u^{2}+u^{4}-u^{6}+\,{\ldots}\,+ (-1)^{k}u^{2k} \,=\, \frac{1+(-1)^{k+2}u^{2k+2}}{1+u^{2}}
        \end{equation}
    By an analysis similar to that done in the previous example, one sees directly (i.e., without computing any derivatives) that
        \begin{displaymath}
        p[g;0]_{2k}(u) \,=\, p[g;0]_{2k+1}(u) \,=\, 1-u^{2}+u^{4}-u^{6}+\,{\ldots}\,+ (-1)^{k}u^{2k},
        \end{displaymath}
    the polynomial on the left side of Equation~\Ref{EqnE.140}.
    From this one easily reads off the derivatives at $0$ of $g$; note that $g^{(m)}(0) \,=\, 0$ if $m$ is odd.

        In contrast, computing these Taylor polynomials {\em via} the `official' definition would require repeatedly differentiating the function $g$.
    The reader is invited to try computing $g^{(m)}(u)$ for, say, $m \,=\, 10$, and comparing the work required for that to what was needed above.


%----------------  End 2J material ---------


\VV

        The Taylor Approximation is of little value unless one has some useful information about the error in the approximation.
    The next result supplies such information.

\V

            \subsection{\small{\bf Theorem} Taylor's Formula with Remainder -- Lagrange Form}\IndBD{derivatives}{Taylor's formula with remainder -- Lagrange form}
            \label{ThmE60.70}

        Let $f:I \,{\rightarrow}\, {\RR}$ be a function which, for some nonnegative integer~$k$, is $k+1$-times differentiable on an open interval $I$.
    Let $c$ and $x$ be points of $I$ such that $x \,\,{\neq}\,\, c$, and let $J \,=\, \mbox{Seg}\,[x,c]$.
    Suppose that $m$ and $M$ are, respectively, lower and upper bounds of the set $\{f^{(k+1)}(u): u{\in}J\}$;
    the values $m \,=\, -{\infty}$ and $M \,=\, +{\infty}$ are permitted. Then
        \begin{equation}
        \label{IneqE.150A}
        m\,\,{\leq}\,\,\frac{f(x) - p[f;c]_{k}(x)}{(x-c)^{k+1}/(k+1)!}\,\,{\leq}\,\,M.
        \end{equation}
    Furthermore, if $f^{(k+1)}$ is not constant on $J$ then both of the inequalities in~\Ref{IneqE.150A} are strict.

\V

        \underline{Proof} The case $k \,=\, 0$ corresponds to the hypothesis being that $m\,\,{\leq}\,\,f'(u)\,\,{\leq}\,\,M$ for all $u$ in~$J$,
    and the conclusion being that ${\displaystyle m\,\,{\leq}\,\,\frac{f(x) - f(c)}{x-c}\,\,{\leq}\,\,M}$,
    with equality (on either side) if, and only if, the function $f'$ is constant on~$J$. This is Part~(a) of Theorem~\Ref{ThmE50.20}.

        Now suppose that for some natural number $k$ the given theorem fails to hold, and let $n$ be the least such natural number.
    Then, by what was just proved in the case $k \,=\, 0$, it is clear that the theorem holds in the case of the nonnegative integer $k \,=\, n-1$.
    Let $m \,=\, {\inf}\,\{f^{(n+1)}(u): u{\in}J\}$ and $M \,=\, {\sup}\,\{f^{(n+1)}(u): u{\in}J\}$.% FINISH!!

\V

        The left-hand inequality is automatically satisfied -- and is a strict inequality -- if $m \,=\, -{\infty}$.
    Thus, suppose $m$ is finite. Then one has $m\,\,{\leq}\,\,f^{(k+1)}(u)$ for all $u$ in $J$.
    Now apply Equation~\Ref{EqnE.85} to the constant function $m$, and Equation~\Ref{EqnE.125} to $f^{(k)}$, to get
        \begin{displaymath}
        \left(D^{-k}_{c} m\right)(x) \,=\, m\frac{(x-c)^{k}}{k!}
    \mbox{ and }
        \left(D^{-k}_{c} f^{(k)}\right)(x) \,=\, f(x) - p[f;c]_{k-1}(x)
        \end{displaymath}
    Suppose that $x\,>\,c$, and apply Inequality~\Ref{IneqE.88A} to the inequality $m\,\,{\leq}\,\,f^{(k)}$ to get
        \begin{displaymath}
        \frac{m(x-c)^{k}}{k!}\,\,{\leq}\,\,f(x)-p[f;c]_{k-1}(x).
        \end{displaymath}
    Since $(x-c)^{k}\,>\,0$ in this case, division by $(x-c)^{k}/k!$ preserves the inequality. That is
        \begin{displaymath}
        m\,\,{\leq}\,\,\frac{f(x)-p[f;c]_{k}(x)}{(x-c)^{k}/k!}.
        \end{displaymath}
    Likewise, if $x\,<\,c$ then Inequality~\Ref{IneqE.88B} implies
        \begin{displaymath}
        (-1)^{k}\frac{m(x-c)^{k}}{k!}\,\,{\leq}\,\,(-1)^{k}(f(x)-p[f;c]_{k-1}).
        \end{displaymath}
    In this case one has $(-1)^{k}(x-c)^{k} \,=\, (c-x)^{k}\,>\,0$, so division by $(-1)^{k}(x-c)^{k}/k!$ also preserves the inequality. That is,
        \begin{displaymath}
        m\,\,{\leq}\,\,\frac{(-1)^{k}(f(x)-p[f;c]_{k-1})}{(-1)^{k}(x-c)^{k}/k!} \,=\, \frac{f(x)-p[f;c]_{k-1}}{(x-c)^{k}/k!}.
        \end{displaymath}
    If $f^{(k)}$ is not constant on $J$, then it follows from Part~(b) of Theorem~\Ref{ThmE45.110} that ${\displaystyle m\,<\,\frac{f(x) - p[f;c]_{k-1}(x)}{(x-c)^{k}/k!}}$.

    A similar argument works for the right half of Inequality~\Ref{IneqE.150A}.

\V

            \subsection{\small{\bf Corollary} (Taylor's Formula with Remainder -- Derivative Form)}
            \label{CorE60.80}

         Let $f:I \,{\rightarrow}\, {\RR}$ be a function which, for some positive integer $k$, is $k$-times differentiable on an open interval $I$.
    Let $c$ and $x$ be points of $I$ such that $x \,\,{\neq}\,\, c$, and let $E_{k-1}$ denote the error in the Taylor approximation $f(x) \,{\approx}\, p[f;c]_{k-1}(x)$;
    that is, $E_{k-1} \,=\, f(x)-p[f;c]_{k-1}(x)$.
    Then there exists a number $r$ between $c$ and $x$ such that
        \begin{equation}
        \label{EqnE.160}
        E_{k-1} \,=\, \frac{f^{(k)}(r)}{k!}(x-c)^{k}
        \end{equation}
    This last relation is often written in the form
        \begin{equation}
        \label{EqnE.170}
        f(x) \,=\, f(c) + f'(c)(x-c) + \frac{f''(c)}{2!}(x-c)^{2} + \,{\ldots}\, + \frac{f^{(k-1)}(c)}{(k-1)!}(x-c)^{k-1} + \frac{f^{(k)}(r)}{k!}(x-c)^{k}
        \end{equation}

\V

        \underline{Proof} Apply the Intermediate-Value Theorem for Derivatives to the results of the previous theorem.

\V

        {\bf Remark} This is a sharpening of Theorem~\Ref{ThmE60.35}.


\V
\V

        The Taylor formula allows one to obtain `formulas' for functions such as $\exp$, $\sin$ and $\cos$.

\V

            \subsection{\small{\bf Theorem}}
            \label{ThmE60.85}

\V

\hspace*{\parindent}(a) For each $k \,=\, 0,1,2,\,{\ldots}\,$ let $p_{k}$ denote the Taylor polynomial of order $k$ of the function ${\exp}$ about the center point $c \,=\, 0$ (i.e., the Maclaurin polynomial of order $k$).
    Then for all $x$ in ${\RR}$ one has ${\exp}\,x \,=\, \lim_{k \,{\rightarrow}\, {\infty}} p_{k}(x)$.

\V

        (b) For each $k \,=\, 0,1,2,\,{\ldots}\,$ let $p_{k}$ denote the Taylor polynomial of order $k$ of the function ${\sin}$ about the center point $c \,=\, 0$ (i.e., the Maclaurin polynomial of order $k$).
    Then for all $x$ in ${\RR}$ one has ${\sin}\,x \,=\, \lim_{k \,{\rightarrow}\, {\infty}} p_{k}(x)$.

\V

        (c) For each $k \,=\, 0,1,2,\,{\ldots}\,$ let $p_{k}$ denote the Taylor polynomial of order $k$ of the function ${\cos}$ about the center point $c \,=\, 0$ (i.e., the Maclaurin polynomial of order $k$).
    Then for all $x$ in ${\RR}$ one has ${\cos}\,x \,=\, \lim_{k \,{\rightarrow}\, {\infty}} p_{k}(x)$.

\V

        \underline{Proof} (a) Note that $p_{k}(0) \,=\, 1 \,=\, {\exp}\,(0)$ for all $k$, so the result is true when $x \,=\, 0$.
    Thus suppose that $x \,\,{\neq}\,\, 0$. Then by Taylor's Formula with (Derivative) Remainder one has, for some $r_{k}$ such that $|r_{k}|\,<\,|x|$,
        \begin{displaymath}
        |{\exp}\, x - p_{k}(x)| \,=\, \frac{{\exp}(r_{k})}{(k+1)!}|x|^{k+1}\,\,{\leq}\,\,{\exp}\,(|x|)\frac{|x|^{k+1}}{(k+1)!}.
        \end{displaymath}
    By Example~\Ref{ExampC20.20}~(3) it is clear that the right side of this last inequality approaches~$0$ as $k$ approaches ${\infty}$,
    and thus $\lim_{k \,{\rightarrow}\, {\infty}} p_{k}(x) \,=\, {\exp}\,x$, as claimed.

\V

        (b) and (c): The proofs of these parts are even simpler, and are left as exercises.

\V
\V
}%\EndSkip
%--------------------------------------------- J

%---------------------------- K
\StartSkip{

        The preceding examples may lead one to believe that if a function is of class $C^{{\infty}}$ on an open interval $I$,
    so that all its Taylor polynomials at a point $c$ of $I$ exist, then these polynomials lead to a useful `formula' for $f(x)$.
    The next example, which is due to Cauchy, shows that this is not the case.

\V

            \subsection{\small{\bf Example}}
            \label{ExampE60.87}

\V

        Define $f:{\RR} \,{\rightarrow}\, {\RR}$ by the rule
        \begin{displaymath}
        f(x) \,=\, \left\{
        \begin{array}{cl}
        e^{-1/x^{2}} & \mbox{if $x \,\,{\neq}\,\, 0$} \\
            0        & \mbox{if $x \,=\, 0$}
        \end{array}
                                \right.
        \end{displaymath}
    It follows easily from Part~(b) of Lemma~\Ref{LemmaE45.127H} that this function is of class $C^{{\infty}}$ at each point of ${\RR}$,
    and that $f^{(k)}(0) \,=\, 0$ for every $k \,=\, 0,1,2,\,{\ldots}\,$.
    In particular, each of the Taylor polynomials $p_{k}(x)$ of $f$ about the center point $c \,=\, 0$ is the $0$-polynomial.
    Since $f(x)\,>\,0$ for all $x \,\,{\neq}\,\, 0$, it follows that one cannot have $\lim_{k \,{\rightarrow}\, {\infty}} p_{k}(x) \,=\, f(x)$ unless $x \,=\, 0$.
}%\EndSkip


%--------------------- K
\StartSkip{
                \section{{\bf Antiderivatives}} 
                \label{SectE45}\IndB{ZZ Sections}{\Ref{SectE45} Antiderivatives}

\V

        In elementary calculus the first important procedure is that of `differentiation': given a function~$F$, determine its derivative~$f \,=\, F'$.
    Most students have relatively little difficulty becoming proficient at carrying out this process.

        Much more difficult for students is the reverse procedure: given a function $f$, determine a function $F$ of which $f$ is the derivative.
    The present section studies this reverse procedure in some depth.

\V

             \subsection{\small{\bf Definition} (Antiderivatives)}
            \label{DefE45.30}\IndB{antiderivatives}{definition}

        Suppose that $f:I \,{\rightarrow}\, {\RR}$ is function defined on an open interval $I$ in ${\RR}$.

\V

        (1) A function $F:I \,{\rightarrow}\, {\RR}$ is said to be {\bf an antiderivative of $f$ on $I$} provided $F'(x) \,=\, f(x)$ for all $x$ in $I$.

\V

        (2) More generally, let $k$ be a positive integer. A function $G:I \,{\rightarrow}\, {\RR}$ is said to be a {\bf \IndBB{antiderivatives}{$k$-th order antiderivative} of $f$ on $I$} provided $G^{(k)}(x) \,=\, f(x)$ for all $x$ in $I$.

\V

        (3) The process of calculating an antiderivative of a given function $f$ is called {\bf antidifferentiation} of~$f$.


\VV

             \subsection{\small{\bf Examples}}
            \label{ExampE45.40}

\V%%---

\hspace*{\parindent}(1) Let $f:{\RR} \,{\rightarrow}\, {\RR}$ be given by $f(x) \,=\, 3\,x^{2}$ for all~$x$.
    Then is is clear that $F$, given by $F(x) \,=\, x^{3}$, is an antiderivative of $f$, as is $F+C$ for any constant~$C$.
    Why is it so clear? Becuae we have differentiated $x^{3}$ in claculus and found the result to be~$3\,x^{2}$.

\V

        (2) (Build-It-Yourself Example) Let $f$ be any standard function from calculus. Differentiate $f$, and let $g$ be the resulting function.
    Then $f$ is an antiderivative of $g$, as is $f+C$ for any constant~$C$.

\V

        (3) Let $f:{\RR} \,{\rightarrow}\, {\RR}$ be given by the rule
        \begin{displaymath}
        f(x) \,=\, \frac{1}{(1+u^{2})^{3/2}} \mbox{ for all $x$ in ${\RR}$}
        \end{displaymath}


        \underline{Claim} The function $F:{\RR} \,{\rightarrow}\, {\RR}$ given by the rule
        \begin{displaymath}
        F(x) \,=\, \frac{x}{\sqrt{1+x^{2}}} \mbox{ for all $x$ in~${\RR}$},
        \end{displaymath}
    is an antiderivative of~$f$.

        To {\em prove} that this claim is correct is not hard: differentiate the function $F$, using the rules of differentiation, and simpify algebraically.
    The harder question to answer is this: Where did the formula for $F$ come from?

\VV

        (3) Let $f(x) \,=\, |x|$ for all $x$ in ${\RR}$. Then $f(x) \,=\, x$ if $x\,\,{\geq}\,\,0$, and $f(x) \,=\, -x$ if $x\,<\,0$.
    On the open interval $(0,+{\infty})$ the function $f$ has many antiderivatives;
    namely, any function on $(0,+{\infty})$ of the form $G(x) \,=\, {\displaystyle \frac{x^{2}}{2}+C_{1}}$, where $C_{1}$ is constant.
    Likewise $f$ has infinitely many antiderivatives on the interval $(-{\infty},0)$,
    namely functions of the form $H(x) \,=\, {\displaystyle -\frac{x^{2}}{2}+C_{2}}$.
    To get an antiderivative defined on all of ${\RR}$, choose the constants $C_{1}$ and $C_{2}$ so that $\lim_{x{\nearrow}0} H(x) \,=\, \lim_{x{\searrow}0} G(x)$.
    This simply requires $C_{1} \,=\, C_{2}$, so let us make the simplest choice, namely $C_{1} \,=\, C_{2} \,=\, 0$.
    Then define $F:{\RR} \,{\rightarrow}\, {\RR}$ by the rule
        \begin{displaymath}
        F(x) \,=\, \left\{
        \begin{array}{ccrl}
        H(x) & \,=\, & -x^{2}/2 & \mbox{if $x\,<\,0$} \\
          0  & \,=\, &          & \mbox{if $x\,=\,0$} \\
        G(x) & \,=\, &  x^{2}/2 & \mbox{if $x\,>\,0$}
        \end{array}
                        \right.
        \end{displaymath}
    This is the same function that appears in Example~\Ref{ExampE20.85} above, where it is shown that $F'$ is the absolute-value function.

\VV

        As is indicated in Example~(1) above, the use of the indefinite article `an' in the definition of `an antiderivative' is needed:
    If $F$ is an antiderivative of $f$ on an interval $I$, then for every constant function $C$ the function $F+C$ is also an antiderivative of $F$ on $I$.
    The next result shows that this is the only ambiguity possible for first-order antiderivatives.

\V

             \subsection{\small{\bf Theorem}}
            \label{ThmE45.50}

\V

        Suppose that $F_{1}$ and $F_{2}$ are antiderivatives of $f$ on an open interval $I$.
    Then there exists a constant function $C$ such that $F_{2}(x) \,=\, F_{1}(x) + C$ for all $x$ in $I$.

    Equivalently: The function $F_{2}-F_{1}$ is constant on $I$.

\V

        \underline{Proof} Note that, by Theorem~\Ref{ThmE30.20}, the function $F_{2}-F_{1}$ is differentiable on $I$, and
        \begin{displaymath}
        (F_{1}-F_{2})'(x) \,=\, F_{1}'(x)-F_{2}'(x) \,=\, f(x)-f(x) \,=\, 0
        \end{displaymath}
    for all $x$ in $I$.
    It then follows from Corollary~\Ref{CorE40.50} that $F_{2}-F_{1}$ is constant on $I$, as required. \Q

\V

        \underline{Notes} (1) Because of the preceding result, if $F$ is a particular antiderivative of a given function $f$ on an open interval $I$,
    then it is common to refer to the expression $F+C$ in which $C$ is an `arbitrary constant', 
    as {\bf the \IndBB{antiderivatives}{general} antiderivative of $f$ on $I$}.

\V

        (2) The corresponding ambiguity for higher-order antiderivatives is more complicated, and is considered later; see Theorem~\Ref{ThmE45.110}.

\V
\V

             \subsection{\small{\bf Corollary}}
            \label{CorE45.60}

\V

        \hspace*{\parindent}(a) Suppose that $f:I \,{\rightarrow}\, {\RR}$ is a function which has an antiderivative on an open interval $I$.
    Let $c$ be any point in $I$, and let $A$ be any real number.
    Then there exists a unique antiderivative $F$ of $f$ on $I$ such that $F(c) \,=\, A$.

        More precisely, if $G:I \,{\rightarrow}\, {\RR}$ is any antiderivative of $f$ on $I$, then the unique $F$ with this property is given by
        \begin{displaymath}
        F(x) \,=\, A + G(x) - G(c) \h ({\ast})
        \end{displaymath}
    In particular, if $A \,=\, 0$ then the formula reduces to
        \begin{displaymath}
        F(x) \,=\, G(x)-G(c).
        \end{displaymath}

\V

        (b) Suppose that $f:I \,{\rightarrow}\, {\RR}$ is a function defined on an open interval $I$ and that $a$ and $b$ are elements of $I$ with $a\,<\,b$.
    Suppose that there are numbers $x_{1}$, $x_{2}$,\,{\ldots}\,$x_{k}$ with $a\,<\,x_{1}\,<\,x_{2}\,<\,\,{\ldots}\,\,<\,x_{k}\,<\,b$ such that $f$ has an antiderivative on each of the subintervals $[a,x_{1}]$, $[x_{1},x_{2}]$,\,{\ldots}\,$[x_{k-1},x_{k}]$, $[x_{k},b]$.
    Then for each $A$ in ${\RR}$ there exists a unique antiderivative $F:[a,b] \,{\rightarrow}\, {\RR}$ of $f$ on $[a,b]$ such that $F(a) \,=\, A$.
\V

        \underline{Proof}

        (a) \underline{Existence} If $G$ is any antiderivative of $F$ on $I$, then clearly the function $F$ given by Equation~$({\ast})$ is also an antiderivative of $f$, since it differs from $G$ by the constant $A-G(c)$.
    Furthermore, one computes that $F(c) \,=\, A+G(c)-G(c) \,=\, A$, as required.

        \underline{Uniqueness} Suppose that $F_{1}$ and $F_{2}$ are both antiderivatives of $f$ on $I$ such that $F_{1}(c) \,=\, A$ and $F_{2}(c) \,=\, A$.
    By the preceding theorem there exists a constant function $C$ such that $F_{2}(x) - F_{1}(x) \,=\, C$ for all $x$ in $I$.
    In particular, this condition must hold when $x \,=\, c$; that is,
        \begin{displaymath}
        0 \,=\, A-A \,=\, F_{1}(c)-F_{1}(c) \,=\, C,
        \end{displaymath}
    so $C \,=\, 0$ and $F_{2} \,=\, F_{1}$ on $I$, as claimed.

\V

        (b) For notational convenience, set $x_{0} \,=\, a$ and $x_{k+1} \,=\, b$.
    By Part~(a), $f$ has a unique antiderivative $F_{1}:[x_{0},x_{1}] \,{\rightarrow}\, {\RR}$ on $[x_{0},x_{1}]$ such that $F_{1}(x_{0}) \,=\, A$.
     Then by Part~(a) again $f$ has a unique antiderivative $F_{2}:[x_{1},x_{2}] \,{\rightarrow}\, {\RR}$ on $[x_{1},x_{2}]$ such that $F_{2}(x_{1}) \,=\, F_{1}(x_{1})$.
    Continuing on this way, one obtains functions $F_{1}$, $F_{2}$,\,{\ldots}\,$F_{k+1}$ such that

        \h (i)\, for each $j \,=\, 1,2,\,{\ldots}\,k+1$, $F_{j}$ is an antiderivative of $f$ on the subinterval $[x_{j-1},x_{j}]$.

        \h (ii) $F_{1}(x_{0}) \,=\, A$; and for each $j \,=\, 1,2,\,{\ldots}\,k$, $F_{j+1}(x_{j}) \,=\, F_{j}(x_{j})$.

\noindent Now define $F:[a,b] \,{\rightarrow}\, {\RR}$ by the rule
        \begin{displaymath}
        F(x) \,=\, F_{j}(x) \mbox{ if $x{\in}[x_{j-1},x_{j}]$}.
        \end{displaymath}
    It is easy to show by Property~(ii) above that $F(x)$ is well-defined at all $x$ in $[a,b]$, even at $x \,=\, x_{j}$.
    It is also easy to show by using one-sided limits at the points $x_{j}$ that $F'(x) \,=\, f(x)$ for all $x$ in $[a,b]$; the details are left to the reader.
    The desired result now follows. \Q

\VV

             \subsection{\small{\bf Examples}}
            \label{ExampE45.65}
\V

\hspace*{\parindent}(1) Suppose that $f:{\RR} \,{\rightarrow}\, {\RR}$ is a monomial, in the sense of high-school algebra;
    that is, there is a nonzero constant $a$ and a nonnegative integer $k$ such that $f(x) \,=\, a\,x^{k}$ for all $x$ in~${\RR}$.
    (Note that if $k$ is even, then $f(-x) \,=\, f(x)$ for all $x$, while if $k$ is odd then $f(-x) \,=\, -f(x)$ for all~$x$.)
    The function $f$ has a unique antiderivative $F$ on ${\RR}$ which is also a monomial, namely the antiderivatives $F$ such that $F(0) \,=\, 0$.
    Indeed, this antiderivative is given by $F(x) \,=\, b\,x^{k+1}$, where $b \,=\, a/(k+1)$.
    Note that if $f$ is a monomial of even degree, then $F$ is a monomial of odd  degree. Similarly, if $f$ is of odd degree, then $F$ is of even degree.
   In the former case one has $f(-x) \,=\, f(x)$ and $F(-x) \,=\, -F(x)$, while in the latter case one has $f(-x) \,=\, -f(x)$ and $F(-x) \,=\, F(x)$.

\V

        (2) More generally, suppose that $I$ is an open interval of the form $(-b,b)$, where $b\,>\,0$; the case $b \,=\, +{\infty}$ is allowed.
    Recall, from high-school algebra, that a function $g:I \,{\rightarrow}\, {\RR}$ is said to be an
    {\bf even function on $\Bfm{I}$}\IndBD{functions}{even, odd functions} provided $g(-x) \,=\, g(-x)$ for all $x$ in~$I$.
    Likewise, a function $h:I \,{\rightarrow}\, {\RR}$ is said to be an {\bf odd function on $\Bfm{I}$} provided $h(-x) \,=\, -h(x)$ for all $x$ in~$I$.

        Suppose that $f:I \,{\rightarrow}\, {\RR}$ is a real-valued function with domain~$I$, and that $f$ has an antiderivative on~$I$.
    Let $F:I \,{\rightarrow}\, {\RR}$ be the unique antiderivative of $f$ on~$I$ such that $F(0) \,=\, 0$.

\VA

        \h (i)\, If $f$ is an even function on the interval~$I$, then $F$ is an odd function on~$I$.

        \h (ii) If $f$ is an odd function on~$I$, then $F$ is an even function on~$I$.

\VA

\noindent Indeed, consider the functiom $G:I \,{\rightarrow}\, {\RR}$ given by the rule $G(x) \,=\, F(-x)$ for all $x$ in~$I$.
    By the Chain Rule one sees that $G'(x) \,=\, -F'(-x) \,=\, -f(-x)$, so that $(-G)'(x) \,=\, f(-x)$.

        In Case~(i) one has $f(-x) \,=\, f(x)$, so that $(-G)'(x) \,=\, f(x)$. Since clearly $-G(0) \,=\, 0$,
    it follows that in this case one has $-G(x) \,=\, F(x)$; that is, $-F(-x) \,=\, F(x)$, which implies that $F$ is an odd function, as claimed.
    Likewise, in Case~(ii) one gets $(-G)'(x) \,=\, -f(x)$, which implies that $F(-x) \,=\, F(x)$, so that $F$ is an even function, as claimed.

\VV

             \subsection{\small{\bf Remarks}}
            \label{RemrkE45.70}
\V

\hspace*{\parindent}(1) The use of the word `antidifferentiation', to indicate the process opposite to the process of `differentiation', seems reasonable;
    likewise for naming the result of that process an `antiderivative'. Indeed, the `antidifferentiation/antiderivative' terminology
    seems to be universal in the modern calculus texts.

    However, for most of the centuries since calculus was first developed the words used instead of `antidifferentiation' and `antiderivative' were 
    `integration' and `integral'; some authors used the word `primitive' instead `integral', and sometimes the word `indefinite' is used before `integral'.

        Why is it important for you, the modern reader, to know this? Because the `integration/integral' terminology for these concepts is still widely used;
    thus if you encounter it, you need to know what it means. Furthermore, although the use of the `antiderivative' {\em terminology} ,
    the {\em notation} used in connection with this concept remains stuck in the eighteenth century.
    More precisely, the way one writes the statement `$F+C$ is the general antiderivative of $f$' in mathematical symbols is this:
        \begin{displaymath}
        F(x)+C \,=\, \int\, f(x)\,dx
        \end{displaymath}
    In the expression ${\displaystyle \int\, f(x)\,dx}$ the symbol ${\displaystyle \int}$ is called the {\bf integral sign},
    the function $f(x)$ is called the {\bf integrand}, and the `arbitrary constant' $C$ is called the {\bf constant of integration}.
    Even in those elementary-calculus textbooks which consistently use the `antiderivative' terminology,
     the section in which one learns how to compute antiderivatives is normally titled something like `Techniques of Integration',
    not `Techniques of Antidifferentiation'.

\V

        (2) There is good reason to assert that the fundamental goal of elementary single-variable calculus is this:

\VA

    \h {\em Given a function $f$, to find its derivative;
    and given a function $g$, to find its (general) antiderivative.}

\VA

\noindent Indeed, this is the significance of Chapter Quote~{1} at the start of this chapter, namely 

\VA

        \h `{\em 6accd{\ae}13eff7i3l9n4o4qrr4s8t12vx}'.

\VA

\noindent Isaac Newton, one of the cofounders of calculus in the seventeenth century, used these letters as an anagram to disguise the following statement:

\begin{quotation}
{\footnotesize
      {\em Data {\ae}quatione quotcunque fluentes quantitates involvente, fluxiones invenire: et vice vers\^{a}}.
}%EndFootNoteSize
\end{quotation}

    That is,
\begin{quotation}
{\footnotesize 
    \em Given an equation involving any number of fluent quantities, to find the fluxions: and vice versa.
}%EndFootNoteSize
\end{quotation}

\noindent In Newton's terminology, a `fluxion' is the rate of change (with respect to time) of a quantity; that is, a derivative;
    and a `fluent' is the quantity whose change produces a give fluxion; that is, an antiderivative.
    Thus, according to Newton the purpose of calculus is to take derivatives and find antiderivatives.



\VV

       The concepts of derivative and antiderivative are so intimately related that it should not be a surprise that for each fact realting to the one concept there is a corresponding fact relating to the other.
    The next result restates Corollary~\Ref{CorE40.65} in a format which handles the existence of more than one antiderivative in a simple manner.

\V

             \subsection{\small{\bf Corollary (of Corollary~\Ref{CorE40.65})}}
            \label{CorE45.75}

\V

        Suppose that $f$ and $g$ are functions which have antiderivatives on an open interval $I$ in~${\RR}$,
    and assume that $f(u)\,\,{\leq}\,\,g(u)$ for all $u$ in~$I$. Let $a$ be a point of $I$,
    and let $F$ and $G$ denote the antiderivatives of $f$ and $g$ on $I$ such that $F(a) \,=\, G(a) \,=\, 0$.


\V

      (a) If $a\,\,{\leq}\,\,x$, then      
        \begin{displaymath}
        F(x)\,\,{\leq}\,\,G(x),
        \end{displaymath}
    with equality if, and only if, $F \,=\, G$ on the set $\mbox{Seg}\,[a,x]$.

\V

        (b) If $x\,\,{\leq}\,\,a$, then
        \begin{displaymath}
        G(x)\,\,{\leq}\,\,F(x),
        \end{displaymath}
    with equality if, and only if, $F \,=\, G$ on {\mbox{Seg}\,[a,x]}.

\V

{\bf Proof} The result is simply a restatement of Corollary~\Ref{CorE40.65} \Q

\VV

             \subsection{\small{\bf Examples}}
            \label{ExampE45.75A}

\V

        Throughout these examples $I$ is an open interval in ${\RR}$, $:I \,{\rightarrow}\, {\RR}$ is a function with domain~$I$,
    $a$ and~$x$ are numbers in the interval~$I$ such that $a\,<\,x$. (The restriction to $x\,>\,a$ is to simplify the example;
    the case $x\,\,{\leq}\,\,a$ will be included later.)

\V

        (1) Suppose that $f'$ is defined on $I$ and that $M_{1}$ is a number such that $f'(u)\,\,{\leq}\,\,M_{1}$ for all $u$ in~$[a,x]$;
    let $g_{1}$ be the constant function such that $g_{1}(u) \,=\, M_{1}$ for all $u$ in~$I$. (The reason for the subscript will be clear soon.)
    Note that the antiderivative of $f$ on $I$ with value $0$ at $a$ is clearly $F_{1}:I \,{\rightarrow}\, {\RR}$ given by the formula $F_{1}(u) \,=\, f(u)-f(a)$.
    Likewise, the antiderivative of $g_{1}$ with value $0$ at $a$ is given by $G_{1}(u) \,=\, M_{1}\,(u-a)$.
    It follows from the preceding corollary that
        \begin{displaymath}
        f(x) - f(a)\,\,{\leq}\,\,M_{1}\,(x-a),
        \end{displaymath}
    with equality if, and only if, $f(u)-f(a) \,=\, M_{1}\,(u-a)$ for all $u$ in~$[a,x]$.
    A similar argument shows that if $m_{1}$ is a number such that $m_{1}\,\,{\leq}\,\,f'(u)$ for all $u$ in~$[a,x]$, then
        \begin{displaymath}
        m_{1}\,(x-a)\,\,{\leq}\,\,f(x) - f(a),
        \end{displaymath}
    with equality if, and only if, $f(u)-f(a) \,=\, m_{1}\,(u-a)$ for all $u$ in $[a,x]$.

\V

        (2) Now suppose that $f''$ is defined on $I$, and that $f''(u)\,\,{\leq}\,\,M_{2}$ for all $u$ in~$[a,x]$.
    Apply the results of the preceding example to get
        \begin{displaymath}
        f'(u)
        \end{displaymath}



%%----------------------- L
%\StartSkip{
        The following question is natural:

\VA

        \h `Under what circumstances does a given function have an antiderivative on a given interval?'.

\VA

\noindent One knows from elementary calculus many examples of functions whose antiderivatives can be computed.
    In contrast, it is easy to provide simple examples of functions $f:I \,{\rightarrow}\, {\RR}$
    which fail to have an antiderivative on an open interval~$I$. For example, if $f:{\RR} \,{\rightarrow}\, {\RR}$ is the Dirichlet function,
    then there is no function $F:{\RR} \,{\rightarrow}\, {\RR}$ such that $F'(x) \,=\, f(x)$ for all~$x$.
    This is obvious because the Dirichlet function clearly fails to possess the Intermediate-Value Property on any subinterval of~${\RR}$.
    In other words, a complete answer to this question is likely to be complicated.

\V

        The next result generalizes the method used in the `Absolute Value' example above.

\V

             \subsection{\small{\bf Theorem}}
            \label{ThmE45.125A}

\V

        Let $[a,b]$ be a closed bounded interval in~${\RR}$, and let ${\cal D} \,=\, \{(x_{0},y_{0}), (x_{1},y_{1}),\,{\ldots}\,(x_{k},y_{k})\}$
    be a set of `data points' in ${\RR}^{2}$ such that $a \,=\, x_{0}\,<\,x_{1}\,<\,\,{\ldots}\,\,<\,x_{k} \,=\, b$,
    and let $g:{\RR} \,{\rightarrow}\, {\RR}$ be the corresponding continuous piecewise-linear interpolating function through these points;
    see Example~\Ref{ExampA30.29}~(3) and Example~\Ref{ExampD20.53}~(5). Then for each real number $c$ in the interval $[a,b]$
    the function $g$ has a unique antiderivative $G$ on ${\RR}$ such that $G(c) \,=\, 0$.

\V

        \underline{Proof} It suffices to show that $g$ has an antiderivative $G$ which takes on the value $0$ at $c \,=\, a \,=\, x_{0}$;
    the case for general choice of $c$ follows easily.

        Note that for each index $j \,=\, 1,2,\,{\ldots}\,k$, if $x$ satisfies the condition $x_{j-1}\,\,{\leq}\,\,x\,\,{\leq}\,\,x_{j}$, then by definition
        \begin{displaymath}
        g(x) \,=\, y_{j-1} + a_{j}\,(x-x_{j-1}) \mbox{ where } a_{j} \,=\, \frac{y_{j} - y_{j-1}}{x_{j} - x_{j-1}}.
        \end{displaymath}
    It is clear that, on the inverval $[x_{j-1},x_{j}]$,
    every function of the form $G_{j}(x) \,=\, a_{j}\,(x-x_{j-1})^{2}/2 + y_{j}\,(x-x_{j-1}) + c_{j}$
    is an antiderivative of $g$ on the interval~$[x_{j-1},x_{j}]$ such that $G_{j}(x_{j-1}) \,=\, c_{j}$.
    The remainder of the construction of the desired function~$G$ is to choose the constants
    $c_{1}$, $c_{2}$,\,{\ldots}\,$c_{k}$ so that the antiderivatives $G_{1}$, $G_{2}$,\,{\ldots}\, `fit' together properly at the points $x_{j}$.

        \underline{The case $j \,=\, 1$} Since, as is observed above, one has $G_{1}(x_{0}) \,=\, c_{1}$, one must choose $c_{1} \,=\, 0$.

        \underline{The Case $j \,=\, 2$} Choose $c_{2}$ so that $G_{1}(x_{1}) \,=\, G_{2}(x_{1})$.
    Since $G_{2}(x_{1}) \,=\, c_{2}$, this requires that one should choose $c_{2} \,=\, G_{1}(x_{1})$.

        The case of general $j$ is carried out similarly, so that $c_{j} \,=\, G_{j-1}(x_{j})$ for each~$j$.

        Finally, define $G:[a,b] \,{\rightarrow}\, {\RR}$ by the rule that if $x_{j-1}\,\,{\leq}\,\,x\,\,{\leq}\,\,x_{j}$, then $G(x) \,=\, G_{j}(x)$.
    One can apply Theorem~\Ref{ThmE20.25A} to conclude that $G'(x) \,=\, g(x)$ for all $x$ in~$[a,b]$. \Q

\V

        {\bf Remark}\,It is easy to show, using only simple Euclidean geometry, that if $g$ is as above and $G$ is any anitderivative of $g$ on~$[a,b]$,
    then $G(b) - G(a)$ equals the signed area between the graph $y \,=\, g(x)$ and the horizontal axis for $a\,\,{\leq}\,\,x\,\,{\leq}\,\,b$.
    (`Signed area' refers to the idea that area above the horizontal axis is positive, while area below that axis is negative.)

\VV

        The next theorem is one of the most important in mathematics. 

\VV

             \subsection{\small{\bf Theorem} (Cauchy's Antiderivative Theorem)}
                         \label{ThmE45.125B}\IndBD{antiderivatives}{Cauchy's antiderivative theorem}

\V

        Let $f:I \,{\rightarrow}\, {\RR}$ be a function which is continuous on an open interval $I$ in ${\RR}$.
    Then the function $f$ has an antiderivative on $I$. More precisely, if $c$ is any point of $I$,
    then $f$ has a unique antiderivative on $I$ whose value at $c$ is~$0$.

\V

        {\bf Proof}\,Let $a$ and $b$ be points of $I$ such that $a\,<\,c\,<\,b$.
    Since $f$ is continuous on $I$, for each positive integer $k$ there is a continuous function $g_{k}:{\RR} \,{\rightarrow}\, {\RR}$ such that

\VA

        \h Condition (i)\, $|g_{k}(x) - f(x)|\,<\,1/k$ for all $x$ in the interval $[a,b]$.

        \h Condition (ii) $g_{k}$ has an antiderivative on ${\RR}$;



\VA

\hspace*{\parindent}Indeed, Theorem~\Ref{ThmD25.60} implies that there is a continuous piecewise-linear function on ${\RR}$ which satisfies Condition~(i),
    while Theorem~\Ref{ThmE45.125A} implies that every such function satisfies Condition~(ii).
    Note also that Condition~(i) implies that for each $x$ in $[a,b]$ one has ${\displaystyle \lim_{k \,{\rightarrow}\, {\infty}} g_{k}(x) \,=\, f(x)}$.

        Let $G_{k}:[a,b] \,{\rightarrow}\, {\RR}$ denote the unique antiderivative of $g_{k}$ on $[a,b]$ such that $G_{k}(c) \,=\, 0$.

\V

        \underline{Claim 1} For each $x$ in $[a,b]$ the sequence ${\Gamma}_{x} \,=\, \{G_{1}(x), G_{2}(x),\,{\ldots}\,\}$ is a Cauchy sequence.

        \underline{Proof of Claim 1} Let $k$ and $m$ be positive integers. Note that if $x{\in}[a,b]$ then, by the `segment' form of the Mean-Value Theorem applied to the function $G_{k+m} - G_{k}$, there exists a number $\hat{x}$ in ${\mbox{Seg}\,[x,c]}$ such that the following holds:
        \begin{displaymath}
        G_{k+m}(x) - G_{k}(x) \,=\, (G_{k+m} - G_{k})(x) - (G_{k+m} - G_{k})(c)
     \,=\, 
        (G_{k+m} - G_{k})'(\hat{x})(x-c) \,=\,
        \end{displaymath}
        \begin{displaymath}
         (G'_{k+m}(\hat{x}) - G_{k}'(\hat{x}))(x-c)
     \,=\, 
       (g_{k+m}(\hat{x}) - g_{k}(\hat{x}))(x-c).
        \end{displaymath}
    It follows, after doing a judicious `add-and-substract' trick and using the Triangle Inequality, that
        \begin{displaymath}
        |G_{k+m}(x) - G_{k}(x)|\,\,{\leq}\,\,\left(|g_{k+m}(x) - f(x)| + |f(x) - g_{k}(x)|\right)|x-c|\,\,{\leq}\,\,\left(\frac{1}{k+m} + \frac{1}{k}\right)|b-a|\,\,{\leq}\,\,\frac{|b-a|}{2k}.
        \end{displaymath}
    In particular, if ${\varepsilon}\,>\,0$ is given, then one has
        \begin{equation}
        \label{IneqE.83A}
        |G_{k+m}(x) - G_{k}(x)|\,\,{\leq}\,\,\frac{|b-a|}{2k}\,<\,{\varepsilon}
        \end{equation}
    for all $k$ in ${\NN}$ such that $k\,>\,|b-a|/(2{\varepsilon})$ and all $m$ in ${\NN}$. In particular, the sequence ${\Gamma}(x)$ is Cauchy, as claimed.

        The preceding result suggests a natural candidate for the desired antiderivative of the given function~$f$; namely,
    define $F:[a,b] \,{\rightarrow}\, {\RR}$ by the rule $F(x) \,=\, \lim_{k \,{\rightarrow}\, {\infty}} G_{k}(x)$ for all $x$ in the interval $[a,b]$.
    (The fact that for each $x$ in $[a,b]$ this limit exists and is finite follows from Claim~1.)

\V

    \underline{Claim 2} The function $F$ just described is the desired antiderivative of $f$.
    More precisely, for each $x_{0}$ in the open interval $(a,b)$ and for each ${\varepsilon}\,>\,0$,
    there exists ${\delta}\,>\,0$ such that if $h$ satisfies $0\,<\,|h|\,<\,{\delta}$ then
        \begin{displaymath}
        \left|\frac{F(x_{0} + h) - F(x_{0})}{h} - f(x_{0})\right|\,\,{\leq}\,\,{\varepsilon} \h ({\ast})
        \end{displaymath}

\V

        \underline{Proof of Claim 2} Let $x_{0}$ be any point of $(a,b)$, and let $h$ be any nonzero real number small enough that $x_{0}+h$ is also in $(a,b)$; more precisely, $|h|\,<\,{\min}\,\{|x_{0}-a|,|b-x_{0}|\}$.
    Then for each $k$ in ${\NN}$ one has, for some $\hat{x}$ in $\mbox{Seg}\,[x_{0}, x_{0}+h]$,
        \begin{displaymath}
        G_{k}(x_{0}+h) - G_{k}(x_{0}) - g_{k}(x_{0})h \,=\, G_{k}'(\hat{x})h - g_{k}(x_{0})h \,=\, (g_{k}(\hat{x}) - g_{k}(x_{0}))h.
        \end{displaymath}
    It then follows from the Extended Triangle Inequality that
        \begin{displaymath}
        \left|G_{k}(x_{0}+h) - G_{k}(x_{0}) - g_{k}(x_{0})h\right| 
    \,\,{\leq}\,\,\left(\left|g_{k}(\hat{x}) - f(\hat{x})\right| +|f(\hat{x}) - f(x_{0})| + \left|f(x_{0}) - g_{k}(x_{0})\right|\right){\cdot}|h| \h ({\ast}{\ast})
        \end{displaymath}
    Now let ${\varepsilon}\,>\,0$ be given. Using the uniform continuity of $f$ on the interval $[a,b]$, choose ${\delta}\,>\,0$ small enough that if $x$ and $y$ are any points of $[a,b]$ such that 
    $|y-x|\,<\,{\delta}$, then one has $|f(y)-f(x)|\,<\,{\varepsilon}/3$.
    It then follows from Condition~(ii) above that if $0\,<\,|h|\,<\,{\delta}$ then~$({\ast}{\ast})$ implies
        \begin{displaymath}
        \left|G_{k}(x_{0}+h) - G_{k}(x_{0}) - g_{k}(x_{0})h\right| \,<\,\left(\frac{{\varepsilon}}{3} + \frac{{\varepsilon}}{3} + \frac{{\varepsilon}}{3}\right)|h| \,=\, {\varepsilon}|h|
        \end{displaymath}
    Divide by $|h|$ and do some simple algebraic simplification to get
        \begin{displaymath}
        \left|\frac{G_{k}(x_{0}+h) - G_{k}(x_{0})}{h} - g_{k}(x_{0})\right|\,<\,{\varepsilon}
        \end{displaymath}
    This last inequality is true for {\em all} sufficiently large $k$. In particular,
    by letting $k$ approach ${\infty}$ one gets the desired inequality~$({\ast})$ for all $h$ such that $0\,<\,|h|\,<\,{\delta}$.

    Since this is true for all ${\varepsilon}\,>\,0$, it follows that $F'(x_{0})$ exists and equals $f(x_{0})$.
    Since $x_{0}$ can be any point of $(a,b)$, it follows that $F$ is an antiderivative of $f$ on $(a,b)$.
    The fact that $F(c) \,=\, 0$ follows from the fact that, by definition, $G_{k}(c) \,=\, 0$ for each~$k$.

\V

        \underline{Summary} For each $a$ and $b$ in $I$ such that $a\,<\,c\,<\,b$,
    there exists a unique antiderivative $F$ of $f$ on $(a,b)$ such that $F(c) \,=\, 0$.

        The desired result, namely that $f$ has a unique antiderivative, whose value at $c$ is $0$, on the {\em full} open interval $I$,
    now follows by letting $a$ approach ${\inf}\,I$ and $b$ approach ${\sup}\,I$; the details are left to the reader. \Q

\VV

                \section{{\bf Numerical Methods for Antiderivatives}} 
                \label{SectE45C}\IndB{ZZ Sections}{\Ref{SectE45C} Numerical Methods for Antiderivatives}


\VV

        If $f:I \,{\rightarrow}\, {\RR}$ is a continuous function on the open interval $I$,
    then the existence of an antiderivative $F:I \,{\rightarrow}\, {\RR}$ of $f$ on $I$ is guaranteed by the preceding results.
    The numbers of greatest interest for such an antiderivative are differences of the form $F(b) - F(a)$, with $a$ and $b$ in $I$ and $a\,<\,b$.
    In elementary calculus one normally computes such differences using a method that is independent of the specific choice of $a$ and~$b$:
    from the formula for $f(x)$ on $I$, obtain an explicit formula for $F(x)$; then substitute $x \,=\, a$ and $x \,=\, b$ into the latter formula, and subtract.

        Unfortunately, in many cases determining an explicit formula for $F$ from the formula for $f$ can be difficult or even impossible.
    In such cases one may need to use methods which depend on the specific choice of $a$ and~$b$ and which provide only estimates of $F(b)-F(a)$.
    Most of these methods fall under the heading of so-called `numerical methods'; there is a vast literature of such methods.
    We consider several of the most familar here.

\V

        Perhaps the most straight-forward way to estimate the difference $F(b) - F(a)$, where $F' \,=\, f$, is to use the Mean-Value Inequality, 
    Corollary~\Ref{CorE40.69A}. In the present context this Inequality takes the more precise form
        \begin{displaymath}
        m\,(b-a)\,\,{\leq}\,\,F(b)-F(a)\,\,{\leq}\,\,M\,(b-a),
        \end{displaymath}
    where $m$ is the minimum value of $f$ on $[a,b]$ and $M$ is the corrsponding maximum value; these extreme values exist because $f$ is continuous on~$[a,b]$.
    Of course the quantities $m$ and $M$ need not be close to each other, so this estimate may be of little value.
    To improve it, divide the interval $[a,b]$ into short subtintervals and apply the analogous inequalities on each of them.
    More precisely, let ${\cal P} \,=\, \{a \,=\, x_{0}\,<\,x_{1}\,<\,\,{\ldots}\,\,<\,x_{n-1}\,<\,x_{n}\,=\,b\}$
    be a partition of $[a,b]$ into $n$ subintervals $[x_{j-1},x_{j}]$, $1\,\,{\leq}\,\,j\,\,{\leq}\,\,n$.
    For each such index $j$ let $m_{j}$ be the minimum value of $f$ on the subinterval $[x_{j-1},x_{j}]$, and let $M_{j}$ be the corresponding maximum.
    Then from the equation
        \begin{displaymath}
        F(b) - F(a) \,=\, (F(x_{n})-F(x_{n-1})) + \,{\ldots}\, + (F(x_{1})-F(0)), \mbox{ that is, }
        F(b) - F(a) \,=\, \sum_{j=1}^{n} {\Delta}F_{j}(x), \mbox{ where ${\Delta}F_{j}(x) \,=\, F(x_{j})-F(x_{j-1})$},
        \end{displaymath}
    one obtains the estimate
        \begin{displaymath}
        m_{1}\,{\Delta}x_{1} + m_{2}\,{\Delta}x_{2} + \,{\ldots}\,+ m_{n}\,{\Delta}x_{n}
        \,\,{\leq}\,\,F(b) - F(a)\,\,{\leq}\,\,
        M_{1}\,{\Delta}x_{1} + M_{2}\,{\Delta}x_{2} + \,{\ldots}\,+ M_{n}\,{\Delta}x_{n};
        \end{displaymath}
    as usual, ${\Delta}x_{j} \,=\, x_{j}-x_{j-1}$. For brevity, denote the quantities
    $m_{1}\,{\Delta}x_{1} + m_{2}\,{\Delta}x_{2} + \,{\ldots}\,+ m_{n}\,{\Delta}x_{n}$ and
    $M_{1}\,{\Delta}x_{1} + M_{2}\,{\Delta}x_{2} + \,{\ldots}\,+ M_{n}\,{\Delta}x_{n}$ by $L(f;{\cal P})$ and $U(f;{\cal P})$, respectively.
    In particular, the numbers $L(f;{\cal P})$ and $U(f;{\cal P})$ determine a full range of reasonable estimates $F(b)-F(a) \,{\approx}\, A$:
    namely, let $A$ be any number such that $L(f;{\cal P})\,\,{\leq}\,\,A\,\,{\leq}\,\,U(f;{\cal P})$.
    Here are a few of the commonly used ways to choose the estimating number~$A$:

\V

        \h (1) Let ${\zeta} \,=\, (z_{1}, z_{2},\,{\ldots}\,z_{n})$ be an ordered list of `sample points'
    such that for each index $j$ one has $z_{j}{\in}[x_{j-1},x_{j}]$. It is clear that $m\,\,{\leq}\,\,f(z_{j})\,\,{\leq}\,\,M_{j}$ for each~$j$, so that
    $F(b) - F(a) \,{\approx}\, A_{{\cal P};{\zeta}}$, where $A_{{\cal P}; {\zeta}} \,=\, f(z_{1})\,{\Delta}x_{1} + \,{\ldots}\, + f(z_{n})\,{\Delta}\,x_{n}$.
    Here are several popular `Rules' for choosing the sample points $z_{j}$:

\VA

        \h \h (Left-hand Rule) $z_{j} \,=\, x_{j-1}$; that is,
        \begin{displaymath}
        A_{{\cal P}; {\lambda}} \,=\, \sum_{j=1}^{n} f(x_{j-1})\,{\Delta}x_{j}.
        \end{displaymath}

        \h \h (Right-hand Rule) $z_{j} \,=\, x_{j}$; that is,
        \begin{displaymath}
        A_{{\cal P}; {\rho}} \,=\, \sum_{j=1}^{n} f(x_{j})\,{\Delta}x_{j}.
        \end{displaymath}

        \h \h (Midpoint Rule) $z_{j} \,=\, (x_{j-1}+x_{j})/2$; that is,
        \begin{displaymath}
        A_{{\cal P}; {\mu}} \,=\, \sum_{j=1}^{n} f\left(\frac{x_{j-1}+x_{j}}{2}\right)\,{\Delta}x_{j}.
        \end{displaymath}

        \h \h (Minimum-Value Rule) $z_{j}$ is a point in $[x_{j-1},x_{j}]$ at which $f(z_{j}) \,=\, m_{j}$; that is,
        \begin{displaymath}
        A_{{\cal P}; \mbox{min}} \,=\, L(f;{\cal P})
        \end{displaymath}

        \h \h (Maximum-Value Rule) $z_{j}$ is a point in $[x_{j-1},x_{j}]$ at which $f(z_{j}) \,=\, M_{j}$; that is,
        \begin{displaymath}
        A_{{\cal P}; \mbox{max}} \,=\, U(f;{\cal P})
        \end{displaymath}

        \h \h (Mean-Value-Theorem Rule) $z_{j}$ is a point in the interval $[x_{j-1},x_{j}]$ at which
    $f(z_{j}) \,=\, {\displaystyle \left(\frac{F(x_{j})-F(x_{j-1})}{x_{j}-x_{j-1}}\right)}$; that is,
        \begin{displaymath}
        A_{{\cal P}; \mbox{MVT}} \,=\, \sum_{j=1}^{n} f(z_{j})\,{\Delta}x_{j} \,=\, 
        \sum_{j=1}^{n} (F(x_{j})-F(x_{j-1})) \,=\, F(b) - F(a).
        \end{displaymath}

        {\bf Remark} The final rule provides an approximation, $A_{{\cal P}; \mbox{MVT}} \,{\approx}\, F(b)-F(a)$, which in fact is exactly correct.
    Unfortunately, it is almost always impossible to determine the corresponding values of the points $z_{j}$
    without already knowing the exact value of $F(b)-F(a)$ in advance, making this rule of little practical value.
    Similarly, the difficulty of finding extrema for the function $f$ on the $n$ subintervals makes the Minimum/Maximum-Value Rules of little value.

% EXERCISE GAUSSIAN INTEGRATION

\V

        \h (2) Since each of the estimating values $A_{{\cal P}; {\lambda}}$, $A_{{\cal P}; {\rho}}$ and $A_{{\cal P}; {\mu}}$ satisfies the condition
    $L(f;{\cal P})\,\,{\leq}\,\,A\,\,{\leq}\,\,U(f;{\cal P})$, it follows that any linear combination
    $p\,A_{{\cal P}; {\lambda}} + q\,A_{{\cal P}; {\rho}} + r\,A_{{\cal P}; {\mu}}$,
    with $0\,\,{\leq}\,\,p, q, r\,\,{\leq}\,\,1$ and $p+q+r \,=\, 1$, also satisfies this condition. The two most familiar examples are these:
pppp
\VA

        \h \h (Trapezoid Rule: $p \,=\, q \,=\, 1/2, q \,=\, 0$)
    ${\displaystyle A_{{\cal P}; {\tau}} \,=\, \frac{A_{{\lambda}} + A_{{\rho}}}{2}}$.

        \h \h (Simpson's Rule: $p \,=\, q \,=\, 1/6, r \,=\, 1/3$)
    ${\displaystyle A_{{\cal P}; {\sigma}} \,=\, \frac{A_{{\cal P}; {\tau}} + 2\,A_{{\cal P}; {\mu}}}{3}}$.

\V

        {\bf Remarks} (1) For simplicity, many calculus texts formulate these rules only in the special case the partition ${\cal P}$ has constant spacing;
    that is, all $n$ of the subintervals $[x_{j-1},x_{j}]$ are of equal length; equivalently, ${\Delta}x_{j} \,=\, (b-a)/n$ for each~$j$.

\V

        (2) Lebesgue's proof of Cauchy's Antiderivative Theorem, given above, involves repeated use of the Trapezoid Rule.

\VV

        The next result shows that these estimates can be guaranteed to be as accurate as one wishes by choosing the partition ${\cal P}$ appropriately.
    The key to this result is the observation that from the inequalities 
        \begin{displaymath}
        L(f;{\cal P})\,\,{\leq}\,\,F(b)-F(a)\,\,{\leq}\,\,U(f;{\cal P})
    \mbox{ and }
        L(f;{\cal P})\,\,{\leq}\,\,A_{{\cal P}; {\zeta}}\,\,{\leq}\,\,U(f;{\cal P})
        \end{displaymath}
    obtained above, it follows that
        \begin{displaymath}
        0\,\,{\leq}\,\,|(F(b)-F(a)) - A_{{\cal P}; {\zeta}}|\,\,{\leq}\,\,U(f;{\cal P})-L(f;{\cal P}) \h ({\ast})
        \end{displaymath}

\VV

             \subsection{\small{\bf Theorem}}
             \label{TheoremE45.130}

\V

        Suppose that $f:I \,{\rightarrow}\, {\RR}$ is continuous on an open interval~$I$,
    and that $F:I \,{\rightarrow}\, {\RR}$ is an antiderivative of $f$ on~$I$. Let $a$ and $b$ be points of $I$ such that $a\,<\,b$. Then:

\V

        (a) For every ${\varepsilon}\,>\,0$ there exists ${\delta}\,>\,0$ such that if
    ${\cal P} \,=\, \{a\,<\,x_{1}\,<\,\,{\ldots}\,\,<\,x_{n-1}\,<\,x_{n} \,=\, b\}$ is any partition of $[a,b]$ whose mesh $||{\cal P}||$
    satisfies $||{\cal P}||\,<\,{\delta}$, then for every list ${\zeta} \,=\, (z_{1}, z_{2},\,{\ldots}\,z_{n})$ of sample points associated with ${\cal P}$
    one has
        \begin{displaymath}
        \left|(F(b)-F(a)) - A_{{\cal P};{\zeta}}\right|\,<\,{\varepsilon}.
        \end{displaymath}
    (As usual, $||{\cal P}||$ is defined to be the the largest of the numbers ${\Delta}x_{j}$, $1\,\,{\leq}\,\,j\,\,{\leq}\,\,n$.)

\V

        (b) The quantity $F(b) - F(a)$ is the unique number $B$ such that $L(f;{\cal P})\,\,{\leq}\,\,B\,\,{\leq}\,\,U(f;{\cal P})$
    for every partition ${\cal P}$ of the interval~$ \,{\subseteq}\, a,b]$.

\V

        {\bf Proof}\, (a)\,By Theorem~\Ref{ThmD25.70}, the function $f$ is uniformly continuous on the closed bounded interval~$[a,b]$.
    Thus, given ${\varepsilon}\,>\,0$ let ${\delta}\,>\,0$ be small enough that if $c$ and $d$
    are any points in $[a,b]$ such that $|d-c|\,<\,{\delta}$, then $|f(d)-f(c)|\,<\,{\varepsilon}/(b-a)$.
    Let ${\cal P} \,=\, \{a\,<\,x_{1}\,<\,\,{\ldots}\,\,<\,x_{n-1}\,<\,x_{n} \,=\, b\}$ be a partition as above such that $||{\cal P}||\,<\,{\delta}$.
    Then for each index $j$ one has $|d-c|\,\,{\leq}\,\,(x_{j}-x_{j-1})\,<\,{\delta}$ for each $c$ and $d$ in the subinterval~$[x_{j-1},x_{j}]$.
    In particular, choose $c$ so that $f(c) \,=\, m_{j}$ and choose $d$ so that $f(d) \,=\, M_{j}$. Then $0\,\,{\leq}\,\,M_{j} - m_{j}\,<\,{\varepsilon}/(b-a)$.
    Multiply by the positive quantity ${\Delta}x_{j}$ and sum over $j$ to get
        \begin{displaymath}
        0\,\,{\leq}\,\,\sum_{j=1}^{n} (M_{j}-m_{j})\,{\Delta}x_{j}\,<\,\frac{{\varepsilon}}{b-a}\,\sum_{j=1}^{n} {\Delta}x_{j} \,=\, {\varepsilon}. 
        \end{displaymath}
    It follows from Inequality~$({\ast})$ above that $|(F(b)-F(a)) - A_{{\cal P}; {\zeta}}|\,<\,{\varepsilon}$, as required.

\V

        (b) This follows easily from the preceding. \Q

\VV

            \subsection{\small{\bf Remark}}
            \label{RemrkE45.140}

\V

        The preceding discussion provides a natural motivation for one of the most characteristic notations found in classical analysis.
    Indeed, recall the equation $F(b)-F(a) \,=\, \sum_{j=1}^{n} {\Delta}F_{j}(x)$ mentioned above,
    which expresses the `whole difference' $F(b)-F(a)$ as the sum of `partial differences' ${\Delta}F_{j}(x)$ as $j$ runs from $j \,=\, 1$ to $j \,=\, n$. 
    Leibniz' treatment of calculus often replaces such discrete sums of ordinary quantities by `continuous' sums of infinitely small quantities.
    Thus, he expresses $F(b)-F(a)$ as the `continuous sum' of infinitely small differences $dF(x)$ as $x$ runs from $x \,=\, a$ to~$x \,=\, b$.
    Instead of the upper case Greek letter $\sum$ as a reminder of the first letter of the Latin word `summus' (sum),
    he uses the symbol ${\displaystyle \int}$, an elongated version of the letter `S', for the same purpose when summing infinitely small quantities.
    Thus, when combined with the formula $dF(x) \,=\, F'(x)\,dx$ obtained above, one gets, using the hypothesis $F'(x) \,=\, f(x)$, that
        \begin{displaymath}
        F(b) - F(a) \,=\, \int\,dF(x) \,=\, \int\,f(x)\,dx.
        \end{displaymath}
    It is assumed here that one knows that $x$ runs from $a$ to $b$ either from the context or by stating it in words;
    the standard calculus notation ${\displaystyle \int_{a}^{b} f(x)\,dx}$ arose only at the beginning of the nineteenth century.

        The process in calculus of obtaining `whole' quantities by summing infinitely small `partial quantities', as above,
    is called `integration', from the Latin for `whole' or `complete'.
    Likewise, the symbol ${\displaystyle \int}$ is called the `integral sign', and the quantity
    ${\displaystyle \int f(x)\,dx}$ is the `integral of the function~$f$'. From the time of Leibniz until the nineteenth century,
    the concept of `integrals' was identified with what we now call `antiderivatives'.\IndB{integrals}{Leibniz' antiderivative viewpoint}
    Even now, most calculus texts refer to the methods of computing antiderivatives as `Techniques of Integration'.
    The reason for the modern practice of separating the concept of `integral' from that of `antiderivative'
    grew from profound studies of Fourier, published in~$1822$, on the theory of heat flow. This transition is discussed at the begining of the next chapter.


%--------------- M EXERCISE??
\StartSkip{
             \subsection{\small{\bf Corollary}}
            \label{CorE45.126B}

\V

        Suppose that $f:I \,{\rightarrow}\, {\RR}$ is a $C^{1}$ function on an open interval $I$ in ${\RR}$.
    Let $c$ be a point in $I$, and let $F:I \,{\rightarrow}\, {\RR}$ be the unique antiderivative of $f$ on $I$ such that $F(c) \,=\, 0$.
    Let $a$ and $b$ be elements of $I$ such that $a\,<\,c\,<\,b$, and let $M$ be an upper bound of $|f'|$ on the closed bounded interval $[a,b]$.
    For each $n$ in ${\NN}$ let $g_{n}$ denote the piecewise-linear function 


described in the proof above,
    and let $G_{n}$ be the antiderivative of $g_{n}$ such that $G_{n}(c) \,=\, 0$. Then for every $n$ in ${\NN}$ one has
        \begin{displaymath}
        |F(x)-G_{n}(x)|\,\,{\leq}\,\,\frac{2\,M\,(b-a)^{2}}{n} \mbox{ for all $x$ in $[a,b]$}.
        \end{displaymath}

\V

        {\bf Proof}\, From Inequality~\Ref{IneqE.83A} one has
        \begin{displaymath}
        \left|\left(G_{n+k}-G_{n}\right)(x)\right|\,\,{\leq}\,\,\frac{2M(b-a)^{2}}{n}
        \end{displaymath}
    for each $x$ in $[a,b]$ and each $k$ in ${\NN}$.
    The desired result now follows by letting $k$ approach ${\infty}$ and recalling that $F(x) \,=\, \lim_{j \,{\rightarrow}\, {\infty}} G_{j}(x)$.

}%\EndSkip
%---------------- M

\VV
%}%\EndSkip
%%-------------------- L

}%\EndSkip
%---------------------------- K


\newpage

\input{Exercises_M140AB_E_2017} %% NOTE: Automatically starts on a new page
